{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement des librairies nécessaires\n",
    "library(ggplot2)\n",
    "library(tidyverse)\n",
    "library(gridExtra)\n",
    "library(GGally)\n",
    "library(plotly)\n",
    "library(corrplot)\n",
    "library(reshape2)\n",
    "library(FactoMineR) \n",
    "library(factoextra)\n",
    "library(glmnet) \n",
    "library(ggfortify)\n",
    "library(pROC)\n",
    "library(ROCR)\n",
    "library(repr)\n",
    "library(caret)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "set.seed(1234)\n",
    "\n",
    "## GRAPH SETTINGS ##\n",
    "# Save original parameters (optional)\n",
    "original_par <- par(no.readonly = TRUE)\n",
    "\n",
    "# Set global scaling factors (1.5x default size)\n",
    "par(\n",
    "  cex.lab = 1.5,   # Axis labels\n",
    "  cex.axis = 1.5,  # Axis text (tick labels)\n",
    "  cex.main = 1.5,  # Main title\n",
    "  cex.sub = 1.5    # Subtitle\n",
    ")\n",
    "\n",
    "# Define a custom theme with larger fonts\n",
    "custom_theme <- theme(\n",
    "  text = element_text(size = 16),            # Global text size\n",
    "  axis.title = element_text(size = 18),      # Axis labels\n",
    "  axis.text = element_text(size = 14),       # Axis tick labels\n",
    "  plot.title = element_text(size = 20),      # Main title\n",
    "  plot.subtitle = element_text(size = 16)    # Subtitle\n",
    ")\n",
    "\n",
    "# Apply the theme to all future plots\n",
    "theme_set(custom_theme)\n",
    "\n",
    "\n",
    "## DATA LOADING & PROCESSING ##\n",
    "# Load data\n",
    "path <- \"../../\" # modifier le nombre de ../ si nécessaire\n",
    "gym <- read.table(paste(path, \"gym_members_exercise_tracking.csv\", sep = \"\"),\n",
    "                    sep = \",\", header = TRUE)\n",
    "\n",
    "gym[,'Gender'] <- as.factor(gym[,'Gender'])\n",
    "gym[,'Workout_Type'] <- as.factor(gym[,'Workout_Type'])\n",
    "gym[,'Experience_Level'] <- as.factor(gym[,'Experience_Level'])\n",
    "gym[,'Workout_Frequency..days.week.'] <- as.factor(gym[,'Workout_Frequency..days.week.'])\n",
    "\n",
    "gym[, \"Weight..kg.\"] <- log(gym[,\"Weight..kg.\"])\n",
    "\n",
    "max_fat <- max(gym[,\"Fat_Percentage\"])\n",
    "gym[, \"Fat_Percentage\"] <- sqrt((max_fat + 1) - gym[,\"Fat_Percentage\"])\n",
    "\n",
    "# renome les variables Weight..kg. et BMI en LWeight et LBMI\n",
    "names(gym)[names(gym) == \"Weight..kg.\"] <- \"LWeight\"\n",
    "names(gym)[names(gym) == \"Fat_Percentage\"] <- \"SFat_Percentage\"\n",
    "\n",
    "gym <- gym %>% select(-c(BMI))\n",
    "\n",
    "# divide data into training and testing sets for experience level\n",
    "trainIndex <- createDataPartition(gym$Experience_Level, p = .8, \n",
    "                                  list = FALSE, \n",
    "                                  times = 1)\n",
    "gym_train <- gym[ trainIndex,]\n",
    "gym_test  <- gym[-trainIndex,]\n",
    "\n",
    "# Normalize the data\n",
    "gym_train_scaled = gym_train\n",
    "scaler <- scale(gym_train[,-c(2,10,13,14)])\n",
    "\n",
    "# Extract the center and scale attributes\n",
    "center <- attr(scaler, \"scaled:center\")\n",
    "scale <- attr(scaler, \"scaled:scale\")\n",
    "\n",
    "gym_train_scaled[,-c(2,10,13,14)] <- scale(gym_train[,-c(2,10,13,14)], center = center, scale = scale)\n",
    "\n",
    "gym_test_scaled = gym_test\n",
    "gym_test_scaled[,-c(2,10,13,14)] <- scale(gym_test[,-c(2,10,13,14)], center = center, scale = scale)\n",
    "\n",
    "\n",
    "cat(\"Data loaded and preprocessed\")\n",
    "\n",
    "\n",
    "## FUNCTION DEFINITIONS ##\n",
    "\n",
    "# Function to plot residuals\n",
    "# x: predicted values\n",
    "# y: residuals\n",
    "gplot.res <- function(x, y, titre = \"titre\"){\n",
    "    ggplot(data.frame(x=x, y=y),aes(x,y))+\n",
    "    geom_point(col = \"blue\")+#xlim(0, 250)+ylim(-155, 155)+\n",
    "    ylab(\"Résidus\")+ xlab(\"Valeurs prédites\")+\n",
    "    ggtitle(titre)+\n",
    "    geom_hline(yintercept = 0,col=\"green\")\n",
    "}\n",
    "\n",
    "# Function to plot ROC curve\n",
    "# model: model to evaluate\n",
    "# data: data to evaluate\n",
    "# title: title of the plot\n",
    "plot_roc <- function(model, data, title = \"ROC curve\"){\n",
    "    pred <- predict(model, data, type = \"response\")\n",
    "    roc <- roc(data$Experience_Level, pred)\n",
    "    auc <- round(auc(roc), 2)\n",
    "    plot(roc, main = title)\n",
    "    text(0.8, 0.2, paste(\"AUC = \", auc), cex = 1.5)\n",
    "}\n",
    "\n",
    "# Function that compute R2\n",
    "\n",
    "# compute_R2 <- function(model, newdata = NA){\n",
    "#   if newdata == NA{\n",
    "#     residuals <- predict(model)\n",
    "#       rss <- sum()\n",
    "#       tss <- \n",
    "# }\n",
    "#   res.tree.cal.cp_high.test <- predict(tree.reg.cal.cp_high, newdata = gym_test)\n",
    "#   mse_test_cal_cp_high <- mean((res.tree.cal.cp_high.test - gym_test[,\"Calories_Burned\"])^2)\n",
    "#   rss_cal_cp_high <- sum((res.tree.cal.cp_high.test - gym_test[,\"Calories_Burned\"])^2)\n",
    "#   tss_cal_cp_high <- sum((gym_test[,\"Calories_Burned\"] - mean(gym_test[,\"Calories_Burned\"]))^2)\n",
    "#   r2_test_cal_cp_high <- 1 - rss_cal_cp_high / tss_cal_cp_high\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summary(gym_train_scaled)\n",
    "summary(gym_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sans selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "reg.lm <- lm(Calories_Burned ~ ., data = gym_train_scaled)\n",
    "\n",
    "# Summary of the regression\n",
    "summary(reg.lm)\n",
    "\n",
    "# Extract the residuals\n",
    "sel.lm <- reg.lm$residuals\n",
    "fit.lm <- reg.lm$fitted.values\n",
    "\n",
    "# Plot the residuals\n",
    "gplot.res(fit.lm, sel.lm, \"Régression linéaire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# MSE of the model on the training and test set\n",
    "mse_train <- mean(sel.lm^2)\n",
    "mse_test <- mean((predict(reg.lm, gym_test_scaled) - gym_test_scaled$Calories_Burned)^2)\n",
    "\n",
    "cat(\"MSE on training set: \", mse_train, \"\\n\")\n",
    "cat(\"MSE on test set: \", mse_test, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : Giga trompette, on va rajouter des termes quadratiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x.mat <- model.matrix(Calories_Burned ~ . -1, data = gym_train_scaled)\n",
    "y.vec <- gym_train_scaled$Calories_Burned\n",
    "\n",
    "head(x.mat)\n",
    "\n",
    "# Fit the lasso model\n",
    "reg.lasso <- glmnet(x.mat, y.vec, alpha = 1, nfolds = 10) # alpha = 1 for lasso\n",
    "\n",
    "# Plot the coefficients\n",
    "options(repr.plot.width=12, repr.plot.height=10)\n",
    "plot(reg.lasso, xvar = \"lambda\", label = TRUE)\n",
    "legend(\"topright\", \n",
    "       legend = paste(1:ncol(x.mat), \" - \", colnames(x.mat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "reg.lasso.cv <- cv.glmnet(x.mat, y.vec, alpha = 1, nfolds = 10)\n",
    "reg.lasso.cv\n",
    "autoplot(reg.lasso.cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cat(\"Best lambda: \", round(reg.lasso.cv$lambda.min,5), \"\\t \\t\")\n",
    "cat(\"MSE for best lambda: \", round(reg.lasso.cv$cvm[which.min(reg.lasso.cv$cvm)],5), \"\\n\")\n",
    "cat(\"Best lambda 1se: \", round(reg.lasso.cv$lambda.1se,5), \"\\t\")\n",
    "cat(\"MSE for best lambda 1se: \", round(reg.lasso.cv$cvm[which.min(reg.lasso.cv$cvm)],5), \"\\n\")\n",
    "\n",
    "\n",
    "# Extract the best model\n",
    "coef(reg.lasso.cv, s = \"lambda.1se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(reg.lasso, xvar = \"lambda\", label = TRUE)\n",
    "abline(v=log(reg.lasso.cv$lambda.1se),col=\"red\")\n",
    "abline(v=log(reg.lasso.cv$lambda.min),col=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract fitted values and residuals\n",
    "fit.lasso.min <- predict(reg.lasso.cv, s = \"lambda.min\", newx = x.mat)\n",
    "res.lasso.min <- y.vec - fit.lasso.min\n",
    "\n",
    "fit.lasso.1se <- predict(reg.lasso.cv, s = \"lambda.1se\", newx = x.mat)\n",
    "res.lasso.1se <- y.vec - fit.lasso.1se\n",
    "\n",
    "# Plot the residuals\n",
    "options(repr.plot.width=12, repr.plot.height=12)\n",
    "p0 <- gplot.res(fit.lm, sel.lm, \"Régression linéaire\")\n",
    "p1 <- gplot.res(fit.lasso.min, res.lasso.min, \"Lasso - Best lambda\")\n",
    "p2 <- gplot.res(fit.lasso.1se, res.lasso.1se, \"Lasso - Best lambda 1se\")\n",
    "\n",
    "grid.arrange(p0, p1, p2, ncol = 1)\n",
    "rm(p0, p1, p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle Quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Quadratic model with lasso\n",
    "x.mat.quad <- model.matrix(Calories_Burned ~ .^2 -1, data = gym_train_scaled)\n",
    "reg.lasso.quad.cv <- cv.glmnet(x.mat.quad, y.vec, alpha = 1, nfolds = 10)\n",
    "reg.lasso.quad.cv \n",
    "# coef(reg.lasso.quad.cv, s = \"lambda.1se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fit.lasso.quad.1se <- predict(reg.lasso.quad.cv, s = \"lambda.1se\", newx = x.mat.quad)\n",
    "res.lasso.quad.1se <- y.vec - fit.lasso.quad.1se\n",
    "\n",
    "# Plot the residuals\n",
    "options(repr.plot.width=12, repr.plot.height=12)\n",
    "p0 <- gplot.res(fit.lm, sel.lm, \"Régression linéaire\")\n",
    "p1 <- gplot.res(fit.lasso.1se, res.lasso.1se, \"Lasso - Best lambda 1se\")\n",
    "p2 <- gplot.res(fit.lasso.quad.1se, res.lasso.quad.1se, \"Lasso - Best lambda 1se - Quadratic\")\n",
    "\n",
    "grid.arrange(p0, p1, p2, ncol = 1)\n",
    "rm(p0, p1, p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(e1071)\n",
    "\n",
    "# SVM for regression\n",
    "svm.reg0 <- svm(Calories_Burned ~ ., data = gym_train_scaled, cross = 5)\n",
    "summary(svm.reg0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "svm.reg.tune = tune.svm(Calories_Burned ~ ., data = gym_train_scaled, cost = c(1, 25, 50, 75, 100, 150, 200), \n",
    "    gamma = 10^seq(-4, -2, by = 0.5))\n",
    "plot(svm.reg.tune)\n",
    "svm.reg.tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "svm.reg.tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Best model\n",
    "svm.reg <- svm.reg.tune$best.model\n",
    "\n",
    "# Predictions\n",
    "pred.svm.reg <- predict(svm.reg, newdata=gym_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "cat(\"SVM Regression Model\\n\")\n",
    "cat(\"MSE on test set: \", mean((gym_test_scaled$Calories_Burned - pred.svm.reg)^2), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graphe des résidus\n",
    "fit.svm.reg = svm.reg$fitted\n",
    "res.svm.reg = fit.svm.reg - gym_train_scaled$Calories_Burned\n",
    "gplot.res(fit.svm.reg, res.svm.reg, \"SVM Regression Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART & Agregation\n",
    "### Regression Trees\n",
    "\n",
    "- Fit a decision tree regressor.\n",
    "- Prune the tree using cross-validation.\n",
    "- Plot: Decision tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a regression tree model\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "\n",
    "# Fit a regression tree model for Calories_Burned\n",
    "tree.reg.cal <- rpart(Calories_Burned ~ ., data = gym_train, control=rpart.control(cp=0.0001))\n",
    "\n",
    "options(repr.plot.width=18, repr.plot.height=12)\n",
    "# Plot the tree\n",
    "rpart.plot(tree.reg.cal, extra = 101, type = 3, under = TRUE, cex = 0.8, tweak = 1)\n",
    "\n",
    "# summary(tree.reg.cal)\n",
    "\n",
    "# compute MSE and R2 on training and test sets\n",
    "mse_train <- mean((gym_train$Calories_Burned - predict(tree.reg.cal, gym_train))^2)\n",
    "mse_test <- mean((gym_test$Calories_Burned - predict(tree.reg.cal, gym_test))^2)\n",
    "r2_train <- 1 - mse_train / var(gym_train$Calories_Burned)\n",
    "r2_test <- 1 - mse_test / var(gym_test$Calories_Burned)\n",
    "cat(\"MSE on training set: \", mse_train, \"\\n\")\n",
    "cat(\"MSE on test set: \", mse_test, \"\\n\")\n",
    "cat(\"R2 on training set: \", r2_train, \"\\n\")\n",
    "cat(\"R2 on test set: \", r2_test, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : Nous avons initialement construit un arbre de régression avec un paramètre de complexité extrêmement faible (`cp = 0.0001`). Comme attendu, ce modèle présente une structure profondément ramifiée (58 feuilles), caractéristique d'un sur-apprentissage. Ce modèle présente une performance relativement bonne sur le jeu d’entraînement (R² = 0.966, MSE = 2613), mais un écart significatif entre l’erreur d’entraînement et de test (MSE_test = 4521) révèle un sur-apprentissage. Toutefois, le R² sur le test reste élevé (0.934), indiquant que le modèle capture une part substantielle de la variance explicative, malgré sa complexité excessive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "\n",
    "xmat = xpred.rpart(tree.reg.cal, xval = 10)\n",
    "\n",
    "CVerr<-apply((xmat-gym_train[,\"Calories_Burned\"])^2,2,sum)\n",
    "\n",
    "plotcp(tree.reg.cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(as.numeric(names(CVerr)), CVerr, type = \"b\", xlab = \"cp\", ylab = \"CV Error\", main = \"CV Error vs cp\", log = \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=18, repr.plot.height=12)\n",
    "as.numeric(attributes(which.min(CVerr))$names)\n",
    "tree.reg.cal <- rpart(Calories_Burned ~ ., data = gym_train, control=rpart.control(cp=as.numeric(attributes(which.min(CVerr))$names)))\n",
    "\n",
    "# Plot the tree\n",
    "rpart.plot(tree.reg.cal, extra = 101, type = 3, under = TRUE, cex = 0.8, tweak = 1)\n",
    "\n",
    "# display the number of nodes of the treee\n",
    "cat(\"Number of nodes: \", length(tree.reg.cal$frame$var), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : La validation croisée 10-fold a identifié une pénalité optimale inattendue (`cp ≈ 0.00015`), conduisant à une erreur de validation (MSE ≈ 4521) inférieure aux modèles moins complexes. Ce résultat paradoxal – où réduire `cp` améliore la performance en validation – pourrait s'expliquer par:\n",
    "- La présence d'interactions complexes dans les données, nécessitant une structure arborescente fine pour être capturées,\n",
    "- Un biais de sélection lié à l'échantillon, où le sur-apprentissage partiel reste généralisable.\n",
    "\n",
    "Le premier point est peu probable car le modèle de régression linéaire avec régularisation LASSO a déjà capturé la plupart des interactions significatives. Le second point est plus plausible, suggérant que le modèle a appris des motifs spécifiques à l'échantillon d'entraînement qui se généralisent bien à la validation croisée.\n",
    "\n",
    "Cependant, l'arbre résultant reste difficilement interprétable (115 nœuds), soulignant un compromis problématique entre performance et explicabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# library(partykit)\n",
    "# plot(as.party(tree.reg.cal), type=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=18, repr.plot.height=15)\n",
    "\n",
    "fit.tree.cal=predict(tree.reg.cal)\n",
    "res.tree.cal=fit.tree.cal-gym_train[,\"Calories_Burned\"]\n",
    "p1 <- gplot.res(fit.tree.cal,res.tree.cal,\"residus de tree.reg.cal (cp~0.00015)\")\n",
    "\n",
    "fit.tree.cal.test=predict(tree.reg.cal, newdata=gym_test)\n",
    "res.tree.cal.test=fit.tree.cal.test-gym_test[,\"Calories_Burned\"]\n",
    "p2 <- gplot.res(fit.tree.cal.test,res.tree.cal.test,\"residus de tree.reg.cal.test (cp~0.00015)\")\n",
    "\n",
    "# Create a tree with lower complexity parameter (cp)\n",
    "tree.reg.cal.cp_high <- rpart(Calories_Burned ~ ., data = gym_train, control=rpart.control(cp=0.01))\n",
    "\n",
    "fit.tree.cal.cp_high=predict(tree.reg.cal.cp_high)\n",
    "res.tree.cal.cp_high=fit.tree.cal.cp_high-gym_train[,\"Calories_Burned\"]\n",
    "p3 <- gplot.res(fit.tree.cal.cp_high,res.tree.cal.cp_high,\"residus de tree.reg.cal.cp_high (cp=0.01)\")\n",
    "fit.tree.cal.cp_high.test=predict(tree.reg.cal.cp_high, newdata=gym_test)\n",
    "res.tree.cal.cp_high.test=fit.tree.cal.cp_high.test-gym_test[,\"Calories_Burned\"]\n",
    "p4 <- gplot.res(fit.tree.cal.cp_high.test,res.tree.cal.cp_high.test,\"residus de tree.reg.cal.cp_high.test (cp=0.01)\")\n",
    "\n",
    "grid.arrange(p1, p2, p3, p4, ncol = 2)\n",
    "rm(p1, p2, p3, p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate metrics for tree.reg.cal\n",
    "mse_train_cal <- mean(res.tree.cal^2)\n",
    "r2_train_cal <- 1 - mean(res.tree.cal^2) / var(gym_train[,\"Calories_Burned\"])\n",
    "\n",
    "res.tree.cal.test <- predict(tree.reg.cal, newdata = gym_test)\n",
    "mse_test_cal <- mean((res.tree.cal.test - gym_test[,\"Calories_Burned\"])^2)\n",
    "rss_cal <- sum((res.tree.cal.test - gym_test[,\"Calories_Burned\"])^2)\n",
    "tss_cal <- sum((gym_test[,\"Calories_Burned\"] - mean(gym_test[,\"Calories_Burned\"]))^2)\n",
    "r2_test_cal <- 1 - rss_cal / tss_cal\n",
    "\n",
    "# Calculate metrics for tree.reg.cal.cp_high\n",
    "mse_train_cal_cp_high <- mean(res.tree.cal.cp_high^2)\n",
    "r2_train_cal_cp_high <- 1 - mean(res.tree.cal.cp_high^2) / var(gym_train[,\"Calories_Burned\"])\n",
    "\n",
    "res.tree.cal.cp_high.test <- predict(tree.reg.cal.cp_high, newdata = gym_test)\n",
    "mse_test_cal_cp_high <- mean((res.tree.cal.cp_high.test - gym_test[,\"Calories_Burned\"])^2)\n",
    "rss_cal_cp_high <- sum((res.tree.cal.cp_high.test - gym_test[,\"Calories_Burned\"])^2)\n",
    "r2_test_cal_cp_high <- 1 - rss_cal_cp_high / tss_cal\n",
    "\n",
    "# Create a summary table\n",
    "results <- data.frame(\n",
    "    Model = c(\"tree.reg.cal\", \"tree.reg.cal.cp_high\"),\n",
    "    MSE_Train = c(mse_train_cal, mse_train_cal_cp_high),\n",
    "    MSE_Test = c(mse_test_cal, mse_test_cal_cp_high),\n",
    "    R2_Train = c(r2_train_cal, r2_train_cal_cp_high),\n",
    "    R2_Test = c(r2_test_cal, r2_test_cal_cp_high)\n",
    ")\n",
    "\n",
    "# Print the table\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : Le modèle complexe (`cp = 0.00015`) montre des résidus mieux centrés et moins dispersés que le modèle élagué (`cp = 0.01`), avec des métriques favorables (R²_test = 0.934 vs 0.853). Toutefois, ces résultats masquent deux risques critiques :\n",
    "1. Strucutre instable : Une légère perturbation des données pourrait altérer radicalement la structure et la hierarchie des nœuds,\n",
    "2. Robustesse incertaine : La performance pourrait se dégrader sur des jeux de données déséquilibrés ou non stationnaires.\n",
    "\n",
    "Pour lever ces doutes, une validation complémentaire par bootstrap (échantillonnage Monte Carlo) serait nécessaire afin d'étudier la variabilité des partitions de l'arbre.\n",
    "\n",
    "**Bilan** :\n",
    "Ces résultats paradoxaux – un modèle clairement surappris mais conservant un pouvoir prédictif élevé – suggère deux hypothèses : \n",
    "1. Signal fort dans les données : Les variables explicatives contiennent des relations structurelles robustes (linéaires ou non-linéaires), qui seraient généralisables même avec un arbre très complexe.  \n",
    "2. Limites du sur-apprentissage arborescent : Contrairement à d’autres méthodes (ex : réseaux de neurones), les arbres surappris peuvent rester partiellement interprétables et éviter un effondrement complet en généralisation. \n",
    "\n",
    "Néanmoins, la supériorité du modèle linéaire (R²_test = 0.98) remet en question la pertinence de la complexité de l'arbre. Si la relation sous-jacente est majoritairement linéaire, l’arbre introduit un biais de variance inutile. Cette observation plaide pour une analyse comparative approfondie entre modèles linéaires et non linéaires.\n",
    "\n",
    "Pour conclure, bien que l’arbre complexe ne soit pas optimal (sur-apprentissage avéré et performance inférieure au linéaire), sa robustesse relative en généralisation (R²_test = 0.93) souligne la présence de motifs prédictifs stables dans les données. Ce résultat justifie une exploration des méthodes hybrides (ex : forêts aléaires avec régularisation, XGBoost) pour concilier flexibilité non linéaire et stabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests and Boosting\n",
    "- Random forests with `mtry` and Brieman criterion\n",
    "- Regularization with Boosting\n",
    "- Using Bootsratp\n",
    "- **plot** feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rf.reg.cal <- randomForest(Calories_Burned ~ ., data = gym_train,\n",
    " xtest = gym_test[, -9], ytest = gym_test[, \"Calories_Burned\"],\n",
    " ntree=500,do.trace=50,importance=TRUE)\n",
    "\n",
    "attributes(rf.reg.cal)\n",
    "\n",
    "cat(\"mtry = \", rf.reg.cal$mtry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mean(rf.reg.cal$test$mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Plot MSE OOB as a function of the number of trees\n",
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "plot(rf.reg.cal$mse, type = \"l\", col = \"blue\", lwd = 2,\n",
    "    xlab = \"Number of Trees\", ylab = \"MSE (OOB)\",\n",
    "    main = \"MSE (OOB) vs Number of Trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif des forets aléatoires est de réduire la variance des arbres tout en conservant leur pouvoir prédictif via le bagging, qui est une technique combinant bootstraping et agrégation d'arbres.\n",
    "\n",
    "**Paramètres à optimiser** : \n",
    "- `mtry`, le nombre de variables tirées aléatoirement à chaque split.\n",
    "    - **Empiriquement** il est choisi par `mtry ≈ p/3` en régression ou `p` est le nombre total de variables. Ici `p = 14` donc `mtry` vaut logiquement 4.\n",
    "    - L'optimisation du `mtry` va être réalisé avec la fonction tuneRF qui cherche en partant du `mtry = p/3 = 4` et va essayer avec un mtry plus petit ou plus grand selon comment varie l'erreur de généralisation Out-Of-Bag (OOB). Il s'arrête dès qu'une amélioration de cette erreur OOB est inférieure à 5% (par défaut).\n",
    "- `ntree`, le nombre d'arbre dans la forêt. Il varie de 100 à 500 et les gains sont marginaux au-delà."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimisation du `mtry`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "\n",
    "rf.reg.cal.tune <- tuneRF(gym_train[,-9], gym_train[,9], ntreeTry = 100,\n",
    " improve = 0.01, trace = 50, doBest = TRUE, xtest = gym_test[, -9], ytest = gym_test[, \"Calories_Burned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cat(\"mtry optimal = \", rf.reg.cal.tune$mtry, \"\\n\")\n",
    "\n",
    "res.rf.cal <- rf.reg.cal.tune$predicted\n",
    "r2.rf.cal.train <- 1 - mean(res.rf.cal^2) / tss_cal\n",
    "\n",
    "res.rf.cal.test <- rf.reg.cal.tune$test$predicted\n",
    "rss.rf.cal.test <- sum((res.rf.cal.test - gym_test[,\"Calories_Burned\"])^2)\n",
    "r2.rf.cal.test <- 1 - rss.rf.cal.test / tss_cal\n",
    "\n",
    "cat(\"R2 train :\", r2.rf.cal.train, \"\\n\")\n",
    "cat(\"R2 test :\", r2.rf.cal.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mean(rf.reg.cal.tune$mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `mtry` optimal trouvé par l'algorithme est 13. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "attributes(rf.reg.cal.tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fit.rf.cal.tune <- rf.reg.cal.tune$predicted\n",
    "res.rf.cal.tune <- fit.rf.cal.tune - gym_train[,\"Calories_Burned\"]\n",
    "gplot.res(fit.rf.cal.tune, res.rf.cal.tune, titre=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(ggRandomForests)\n",
    "\n",
    "options(repr.plot.width=18, repr.plot.height=8)\n",
    "\n",
    "p1 <- plot(gg_vimp(rf.reg.cal), main = \"Importance des variables (mtry = 3)\")\n",
    "p2 <- plot(gg_vimp(rf.reg.cal.tune), main = \"Importance des variables (mtry = 13)\")\n",
    "\n",
    "grid.arrange(p1, p2, ncol = 2)\n",
    "rm(p1, p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "##### Avec la librairie `gbm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "attributes(boost.reg.cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(gbm)\n",
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "boost.reg.cal = gbm(Calories_Burned ~ ., data = gym_train, distribution = \"gaussian\", n.trees = 1000, \n",
    "    cv.folds = 10, n.minobsinnode = 5, shrinkage = 0.03, verbose = FALSE)\n",
    "# fixer verbose à FALSE pour éviter trop de sorties\n",
    "plot(boost.reg.cal$cv.error, type = \"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "best.iter <- gbm.perf(boost.reg.cal, method=\"cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fit.boostr.cal <- boost.reg.cal$fit\n",
    "res.boostr.cal <- fit.boostr.cal - gym_train[,\"Calories_Burned\"]\n",
    "gplot.res(fit.boostr.cal, res.boostr.cal, titre=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# plot(gg_vimp(importance(boost.reg.cal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Avec XGBoost\n",
    "- Optimize number of trees and learning rate.\n",
    "- Use early stopping to prevent overfitting.\n",
    "- Plot: SHAP values for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "\n",
    "xgb.mat.cal = xgb.DMatrix(\n",
    "    data = model.matrix(Calories_Burned ~ . -1, data = gym_train),\n",
    "    label = gym_train$Calories_Burned\n",
    ")\n",
    "\n",
    "params <- list(\n",
    "  booster = \"gbtree\",\n",
    "  objective = \"reg:squarederror\",\n",
    "  eval_metric = \"rmse\",\n",
    "  eta = 0.1,\n",
    "  max_depth = 4,\n",
    "  gamma = 0,\n",
    "  subsample = 0.8,\n",
    "  colsample_bytree = 0.8,\n",
    "  lambda = 0, # L2 Reg\n",
    "  alpha = 1 # L1 Reg\n",
    ")\n",
    "xgb.reg.cal <- xgb.cv(params = params, data = xgb.mat, \n",
    "    nround = 1000, verbose = FALSE, nfold = 10, nthread=6)\n",
    "\n",
    "# xgb.reg.cal <- xgboost(data = xgb.mat,\n",
    "#     nrounds = 1000, eta = 0.1, max_depth = 3, gamma = 0, \n",
    "#     min_child_weight = 1, subsample = 0.8, colsample_bytree = 0.8, \n",
    "#     lambda = 1, alpha = 0, scale_pos_weight = 1, \n",
    "#     objective = \"reg:squarederror\", eval_metric = \"rmse\", \n",
    "#     verbose = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "attributes(xgb.reg.cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract evaluation log\n",
    "eval_log <- xgb.reg.cal$evaluation_log\n",
    "\n",
    "# Plot RMSE as a function of the number of iterations\n",
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "ggplot(eval_log, aes(x = iter)) +\n",
    "    geom_line(aes(y = train_rmse_mean, color = \"Train RMSE\")) +\n",
    "    geom_line(aes(y = test_rmse_mean, color = \"Test RMSE\")) +\n",
    "    labs(\n",
    "        title = \"Performance as a Function of Iterations\",\n",
    "        x = \"Number of Iterations\",\n",
    "        y = \"RMSE\"\n",
    "    ) +\n",
    "    scale_color_manual(values = c(\"Train RMSE\" = \"blue\", \"Test RMSE\" = \"red\")) +\n",
    "    theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "\n",
    "# Define the parameter grid\n",
    "tune_grid <- expand.grid(\n",
    "  nrounds = 100,\n",
    "  max_depth = c(3, 6, 10),\n",
    "  eta = c(0.01, 0.1, 0.3),\n",
    "  gamma = 0,\n",
    "  colsample_bytree = c(0.7, 0.8, 0.9),\n",
    "  min_child_weight = 1,\n",
    "  subsample = c(0.7, 0.8, 0.9)\n",
    ")\n",
    "\n",
    "# Train control for cross-validation\n",
    "train_control <- trainControl(\n",
    "  method = \"cv\",\n",
    "  number = 5,\n",
    "  verboseIter = FALSE,\n",
    "  allowParallel = TRUE\n",
    ")\n",
    "\n",
    "# Train the model using caret\n",
    "xgb_model <- train(\n",
    "  Calories_Burned ~ .,\n",
    "  data = gym_train_scaled,\n",
    "  method = \"xgbTree\",\n",
    "  trControl = train_control,\n",
    "  tuneGrid = tune_grid,\n",
    "  metric = \"RMSE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(xgb_model$bestTune)\n",
    "print(xgb_model$finalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "res.xgb.cal <- predict(xgb_model$finalModel, newdata = xgb.mat.cal)\n",
    "1 - mean(res.xgb.cal^2) / tss_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "attributes(xgb_model$finalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "xgb.ggplot.deepness(xgb.reg.cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "xgb.ggplot.importance(\n",
    "    xgb.importance(model = xgb.reg.cal)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
