{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "randomseed = 1234\n",
    "\n",
    "## DATA LOADING AND PREPROCESSING\n",
    "# Load the data\n",
    "gym = pd.read_csv('../../gym_members_exercise_tracking.csv')\n",
    "\n",
    "# set 'Gender', 'Workout_Type', 'Workout_Frequency (days/week)' and 'Experience_Level' as categorical\n",
    "for col in ['Gender', 'Workout_Type', 'Workout_Frequency (days/week)', 'Experience_Level']:\n",
    "    gym[col] = gym[col].astype('category')\n",
    "\n",
    "# log transform Weight and BMI\n",
    "gym['Weight (kg)'] = np.log1p(gym['Weight (kg)'])\n",
    "\n",
    "# transform 'Fat_Percentage'\n",
    "max_fat = gym['Fat_Percentage'].max()\n",
    "gym['Fat_Percentage'] = gym['Fat_Percentage'].apply(lambda x: np.sqrt(max_fat+1)-x)\n",
    "\n",
    "# rename transformed columns\n",
    "gym.rename(columns={'Weight (kg)': 'LWeight', 'Fat_Percentage': 'SFat_Percentage'}, inplace=True)\n",
    "\n",
    "gym.drop(columns=['BMI'], inplace=True)\n",
    "\n",
    "# divide into train and test set\n",
    "gym_train, gym_test = train_test_split(gym, test_size=0.2, random_state=randomseed)\n",
    "\n",
    "# Create gym_train_scale, gym_test_scale\n",
    "gym_train_scale = gym_train.copy()\n",
    "gym_test_scale = gym_test.copy()\n",
    "\n",
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "gym_train_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']] = scaler.fit_transform(gym_train_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']])\n",
    "\n",
    "gym_test_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']] = scaler.transform(gym_test_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']])\n",
    "\n",
    "\n",
    "# Create X_train_exp_level, X_test_exp_level, y_train_exp_level, y_test_exp_level\n",
    "X_train_exp_level = gym_train.drop(columns=['Experience_Level'])\n",
    "X_train_exp_level_scale = gym_train_scale.drop(columns=['Experience_Level'])\n",
    "y_train_exp_level = gym_train['Experience_Level']\n",
    "X_test_exp_level = gym_test.drop(columns=['Experience_Level'])\n",
    "X_test_exp_level_scale = gym_test_scale.drop(columns=['Experience_Level'])\n",
    "y_test_exp_level = gym_test['Experience_Level']\n",
    "\n",
    "# Create X_train_calories, X_test_calories, y_train_calories, y_test_calories\n",
    "X_train_calories = gym_train.drop(columns=['Calories_Burned'])\n",
    "X_train_calories_scale = gym_train_scale.drop(columns=['Calories_Burned'])\n",
    "y_train_calories = gym_train['Calories_Burned']\n",
    "X_test_calories = gym_test.drop(columns=['Calories_Burned'])\n",
    "X_test_calories_scale = gym_test_scale.drop(columns=['Calories_Burned'])\n",
    "y_test_calories = gym_test['Calories_Burned']\n",
    "\n",
    "print(\"Data loaded and preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©diction de Calories Burned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_calories_dummy = pd.get_dummies(X_train_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "X_test_calories_dummy = pd.get_dummies(X_test_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "\n",
    "X_train_calories_dummy1 = pd.get_dummies(X_train_calories, drop_first=True)\n",
    "X_test_calories_dummy1 = pd.get_dummies(X_test_calories, drop_first=True)\n",
    "# Normalisation des donn√©es - scaled = Scale + Dummies alors que scale = just scale\n",
    "X_train_calories_scaled = scaler.fit_transform(X_train_calories_dummy1)\n",
    "X_test_calories_scaled = scaler.transform(X_test_calories_dummy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gym_train.head().style.background_gradient(cmap='YlGnBu', low=0, high=0, axis=0))\n",
    "# display unique values of categorical columns\n",
    "display(gym_train.info())\n",
    "for col in gym_train.select_dtypes(include='category').columns:\n",
    "    print(col, gym_train[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gym_train.head().style.background_gradient(cmap='YlGnBu', low=0, high=0, axis=0))\n",
    "# display unique values of categorical columns\n",
    "display(gym_train.info())\n",
    "for col in gym_train.select_dtypes(include='category').columns:\n",
    "    print(col, gym_train[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mod√®le Lin√©aire : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importer les biblioth√®ques n√©cessaires\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 2. Cr√©er un mod√®le de r√©gression lin√©aire\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "\n",
    "#calcul du temps d'entra√Ænement\n",
    "import time\n",
    "start_time = time.time()\n",
    "# Entra√Ænement du mod√®le\n",
    "# 3. Entra√Æner le mod√®le sur les donn√©es d'entra√Ænement\n",
    "linear_model.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "# 4. Faire des pr√©dictions sur l'√©chantillon de test (X_test_scaled)\n",
    "#y_pred_calories = linear_model.predict(X_test_calories_scaled)\n",
    "y_pred_train_calories = linear_model.predict(X_train_calories_scaled)\n",
    "y_pred_test_calories = linear_model.predict(X_test_calories_scaled)\n",
    "# 5. √âvaluer la performance du mod√®le\n",
    "# Coefficient de d√©termination R¬≤\n",
    "r2_test = r2_score(y_test_calories, y_pred_test_calories)\n",
    "r2_train = r2_score(y_train_calories, y_pred_train_calories)\n",
    "print(f\"R¬≤ test: {r2_test}\")\n",
    "print(f\"R¬≤ train: {r2_train}\")\n",
    "\n",
    "# Erreur quadratique moyenne (MSE)\n",
    "mse = mean_squared_error(y_test_calories, y_pred_test_calories)\n",
    "print(f\"MSE: {mse}\")\n",
    "\n",
    "# 6. Afficher les coefficients du mod√®le\n",
    "print(\"Coefficients du mod√®le : \", linear_model.coef_)\n",
    "print(\"Intercept du mod√®le : \", linear_model.intercept_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train_calories, y_pred_train_calories, color='green', alpha=0.5, label='scikit-learn Regression Predictions')\n",
    "plt.plot([y_train_calories.min(), y_pred_train_calories.max()], [y_train_calories.min(), y_pred_train_calories.max()], 'k--', lw=2)\n",
    "plt.scatter(y_test_calories, y_pred_test_calories, color='blue', alpha=0.6)\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], [y_test_calories.min(), y_test_calories.max()], color='red', lw=2)  # Ligne id√©ale\n",
    "plt.xlabel(\"Calories r√©elles\")\n",
    "plt.ylabel(\"Calories pr√©dites\")\n",
    "plt.title(\"Pr√©dictions vs Valeurs r√©elles\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances du mod√®le\n",
    "\n",
    "R¬≤ = 0.978 :\n",
    "Le mod√®le explique 97.8% de la variance des calories br√ªl√©es. Cette valeur exceptionnellement √©lev√©e pourrait indiquer un surapprentissage (overfitting), surtout si le mod√®le a beaucoup de variables (18 coefficients ici).\n",
    "On remarque √©galement que les points verts (entra√Ænement) et bleus (test) semblent bien align√©s, ce qui sugg√®re une bonne performance globale du mod√®le. Toutefois, pour obtenir une analyse compl√®te, il faudrait tracer r√©sidus vs pr√©dictions pour v√©rifier la r√©partition uniforme des r√©sidus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calcul des r√©sidus\n",
    "residuals_train = y_train_calories - y_pred_train_calories\n",
    "residuals_test = y_test_calories - y_pred_test_calories\n",
    "\n",
    "# Cr√©ation d'une seule figure\n",
    "plt.figure(figsize=(8, 6))  # Ajuste la taille selon tes besoins\n",
    "\n",
    "# 1. R√©sidus vs Valeurs ajust√©es\n",
    "sns.residplot(x=y_pred_train_calories, y=residuals_train, lowess=True, \n",
    "              line_kws={'color': 'red', 'lw': 1})\n",
    "\n",
    "# Ajout de la ligne horizontale √† z√©ro\n",
    "plt.axhline(0, color='black', linestyle='dotted', alpha=0.6)\n",
    "\n",
    "# Ajout des labels et du titre\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted\")\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forme en banane dans le graphique des r√©sidus (Residuals vs Fitted) r√©v√®le une non-lin√©arit√© non captur√©e par le mod√®le.Ce qui nous indique que la valeur du score R¬≤ est trompeuse. En effet, le R¬≤ mesure la variance expliqu√©e, pas la justesse des pr√©dictions. Un mod√®le peut, donc, avoir un R¬≤ √©lev√© tout en ayant des erreurs syst√©matiques. Le mod√®le lin√©aire est inad√©quat pour capturer la vraie relation dans les donn√©es, malgr√© un R¬≤ √©lev√©. Ainsi, pour am√©liorer la g√©n√©ralisation du mod√®le et identifier les variables r√©ellement influentes, une approche de r√©gularisation s‚Äôimpose. C‚Äôest ici que la r√©gression Lasso (Least Absolute Shrinkage and Selection Operator) entre en jeu. \n",
    "\n",
    "Donc, maintenant, nous allons passer √† l‚Äôimpl√©mentation de Lasso pour voir comment il am√©liore (ou non) la robustesse du mod√®le, malgr√© les limites structurelles de la lin√©arit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord avec un lambda quelconque puis avec un lambda choisi par validation crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso\n",
    "lasso1= Lasso(alpha=10)\n",
    "# calcul du temps d'entra√Ænement\n",
    "\n",
    "start_time = time.time()\n",
    "# Entra√Ænement du mod√®le\n",
    "lasso1.fit(X_train_calories_scaled, y_train_calories)\n",
    "train_score_lasso1=lasso1.score(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "test_score_lasso1=lasso1.score(X_test_calories_scaled, y_test_calories)\n",
    "\n",
    "print(\"The train score for ls model is {}\".format(train_score_lasso1))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso1))\n",
    "# print the lasso coefficient with the name of the variable next to it\n",
    "\n",
    "\n",
    "coef_calories_lasso1 = pd.Series(lasso1.coef_, index=X_train_calories_dummy1.columns)\n",
    "coef_calories_lasso1.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du mod√®le Lasso pour Calories Burned')\n",
    "plt.show()\n",
    "# Afficher le nombre de variables conserv√©es et √©limin√©es\n",
    "print(f\"Lasso conserve {sum(coef_calories_lasso1 != 0)} variables et en supprime {sum(coef_calories_lasso1 == 0)}\")\n",
    "# print the coefficients of lasso1\n",
    "#print(\"Coefficients du mod√®le Lasso : \", coef_calories_lasso1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Appliquer Lasso avec validation crois√©e pour trouver le meilleur alpha\n",
    "#lasso = LassoCV(cv=5, random_state=1234, max_iter=10000)  # 5-fold cross-validation\n",
    "start_time = time.time()\n",
    "lasso = LassoCV(cv=5, alphas=np.array(range(1, 50, 1)) / 20., n_jobs=-1, random_state=13).fit(X_train_calories_scaled, y_train_calories)\n",
    "lasso.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "\n",
    "# Coefficient optimal alpha s√©lectionn√© par LassoCV\n",
    "optimal_alpha = lasso.alpha_\n",
    "print(f\"Optimal alpha: {optimal_alpha}\")\n",
    "\n",
    "# Coefficients du mod√®le Lasso\n",
    "coef_calories_lasso = pd.Series(lasso.coef_, index=X_train_calories_dummy1.columns)\n",
    "\n",
    "# Afficher les coefficients du mod√®le Lasso\n",
    "print(\"Coefficients du mod√®le Lasso pour Calories Burned:\")\n",
    "print(coef_calories_lasso)\n",
    "\n",
    "# Afficher le nombre de variables conserv√©es et √©limin√©es\n",
    "print(f\"Lasso conserve {sum(coef_calories_lasso != 0)} variables et en supprime {sum(coef_calories_lasso == 0)}\")\n",
    "\n",
    "# Tracer les coefficients\n",
    "coef_calories_lasso.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du mod√®le Lasso pour Calories Burned')\n",
    "plt.show()\n",
    "\n",
    "# Pr√©dictions avec le mod√®le Lasso\n",
    "y_pred_lasso = lasso.predict(X_test_calories_scaled)\n",
    "\n",
    "# Calcul de l'erreur quadratique moyenne pour √©valuer les performances du mod√®le\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_lasso_test = mean_squared_error(y_test_calories, y_pred_lasso)\n",
    "\n",
    "print(f\"Test Mean Squared Error (MSE) pour Lasso pour l'√©chantillon de test: {mse_lasso_test}\")\n",
    "\n",
    "train_score_lasso= lasso.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_lasso= lasso.score(X_test_calories_scaled, y_test_calories)\n",
    "print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances du mod√®le\n",
    "\n",
    "- On obtient un MSE = 1638.14. On a donc une l√©g√®re am√©lioration par rapport au mod√®le lin√©aire non r√©gularis√© (MSE=1679.54). Cependant, cette diff√©rence minime sugg√®re que la r√©gularisation Lasso r√©duit l√©g√®rement le surapprentissage.Toutefois, Le probl√®me fondamental de non-lin√©arit√© (forme en banane des r√©sidus) persiste, limitant les gains de performance. Ceci est visible dans le graphe des r√©sidus ci-dessous, o√π l'on observe un profil en banane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trac√© des r√©sidus\n",
    "residuals_lasso = y_test_calories - y_pred_lasso\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, residuals_lasso, color='blue', alpha=0.6)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Valeurs r√©elles\")\n",
    "plt.ylabel(\"R√©sidus\")\n",
    "plt.title(\"R√©sidus du mod√®le Lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- On a un alpha optimal = 0.8 :Une p√©nalit√© L1 relativement forte, ce qui explique pourquoi 11 variables sur 18 ont √©t√© √©limin√©es (coefficients √† z√©ro).\n",
    "\n",
    "### Interpretation des r√©sultats: \n",
    "\n",
    "#### Relation Session_Duration - Calories Burned\n",
    "- On remarque, d'apr√®s le graphe, que la variable Session_Duration domine clairement, c'est √† dire qu'une augmentation d‚Äô1 heure de la dur√©e de la s√©ance entra√Æne une augmentation pr√©dite de 243 calories br√ªl√©e. Donc, plus la s√©ance est longue, plus le corps puise dans ses r√©serves √©nerg√©tiques (glycog√®ne et lipides).\n",
    "\n",
    "Les activit√©s prolong√©es (ex : cardio, endurance) sollicitent le m√©tabolisme a√©robie, favorisant une d√©pense calorique cumulative.\n",
    "\n",
    "- Remarque: Ce coefficient √©lev√© pourrait aussi refl√©ter une corr√©lation indirecte (ex : les s√©ances longues incluent souvent des exercices intenses).\n",
    "\n",
    "#### Diff√©rence homme femme \n",
    "-  Les hommes br√ªlent 40.9 calories de plus que les femmes √† caract√©ristiques √©gales.\n",
    "    Ceci pourrait √™tre d√ª au fait que les hommes ont g√©n√©ralement une masse musculaire plus √©lev√©e, qui consomme plus de calories au repos et √† l‚Äôeffort.Les diff√©rences hormonales (testost√©rone) favorisent un m√©tabolisme √©nerg√©tique plus actif.\n",
    "\n",
    "- Remarque: Ce coefficient pourrait aussi refl√©ter des biais comportementaux (ex : les hommes choisissent des entra√Ænements plus intenses non mesur√©s dans les donn√©es)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution du MSE en fonction de lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "model = LassoCV(cv=5, alphas=np.array(range(1,200,1))/10.,n_jobs=-1,random_state=13).fit(X_train_calories_scaled, y_train_calories)\n",
    "m_log_alphas = np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "# ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='MSE moyen', linewidth=2)\n",
    "plt.axvline(np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: optimal par VC')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('log(alpha)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE de chaque validation: coordinate descent ')\n",
    "plt.show()\n",
    "#le courbe noire correspond √† la moyennes des 5 autres\n",
    "# on decoupe en 5 √©chantillons d'apprentissage d'ou les 5 courbes \n",
    "# Plot the coefficients as a function of -log(alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque une zone o√π la MSE est relativement basse et stable autour d‚Äôun certain intervalle de alpha. Puis, quand alpha devient trop grand (r√©gularisation trop forte), la MSE monte en fl√®che (le mod√®le est trop contraint, sous-apprentissage).\n",
    "\n",
    "√Ä l‚Äôoppos√©, quand alpha est trop petit, la r√©gularisation est quasi nulle : on risque un sur-apprentissage (m√™me si, parfois, la MSE peut rester relativement stable dans cette zone si le dataset n‚Äôest pas trop bruyant).\n",
    "\n",
    "Le point choisi par la validation crois√©e est un compromis : il vise √† r√©duire le nombre de coefficients non nuls (pour la parcimonie) tout en conservant une bonne performance (basse MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# Calculer le chemin du Lasso\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_train_calories_scaled, y_train_calories, alphas=np.array(range(1, 400, 1)))\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "# Styles pour les lignes\n",
    "styles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "# Log des alphas\n",
    "log_alphas_lasso = np.log10(alphas_lasso)\n",
    "\n",
    "# Tracer les coefficients\n",
    "for coef_l, s in zip(coefs_lasso, styles):\n",
    "    plt.plot(log_alphas_lasso, coef_l, linestyle=s, c='b')\n",
    "\n",
    "# Ajouter une ligne verticale pour l'alpha optimal\n",
    "plt.axvline(optimal_alpha, color='red', linestyle='--', label=f'Optimal alpha: {optimal_alpha}')\n",
    "\n",
    "# Ajouter des labels et une l√©gende\n",
    "plt.xlabel('Log(alpha)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.legend()\n",
    "plt.title('Lasso Path with Optimal Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique illustre le m√©canisme de r√©gularisation L1 propre √† la r√©gression Lasso : lorsque le param√®tre de r√©gularisation *alpha* augmente, la contrainte de parcimonie s'intensifie, conduisant progressivement les coefficients les moins informatifs vers z√©ro. Ce comportement est intrins√®que √† l'algorithme, qui privil√©gie un **mod√®le simplifi√©** (moins de variables) au d√©triment d'une l√©g√®re d√©gradation de la pr√©cision. En d'autres termes, un *alpha* √©lev√© renforce la p√©nalisation des coefficients, favorisant ainsi un **√©quilibre optimal entre simplicit√© interpr√©tative et g√©n√©ralisation**, au prix d'un biais accru. Cela traduit directement le compromis biais-variance au c≈ìur de l'optimisation du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes et std des scores pour chaque alpha\n",
    "mse_path = model.mse_path_.mean(axis=1)\n",
    "stds = model.mse_path_.std(axis=1)\n",
    "alphas = model.alphas_\n",
    "\n",
    "# Index de l'erreur minimale\n",
    "min_idx = np.argmin(mse_path)\n",
    "\n",
    "# lambda_min\n",
    "alpha_min = alphas[min_idx]\n",
    "\n",
    "# lambda_1se = plus grand alpha avec erreur ‚â§ (erreur min + 1 std)\n",
    "threshold = mse_path[min_idx] + stds[min_idx]\n",
    "alpha_1se = max(alphas[mse_path <= threshold])\n",
    "\n",
    "# Refit pour alpha_min\n",
    "lasso_min = Lasso(alpha=alpha_min, max_iter=10000)\n",
    "start_time = time.time()\n",
    "lasso_min.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement pour alpha_min : {end_time - start_time} secondes\")\n",
    "\n",
    "non_zero_min = (lasso_min.coef_ != 0).sum()\n",
    "r2_min = lasso_min.score(X_test_calories_scaled, y_test_calories)\n",
    "# Refit pour alpha_1se\n",
    "lasso_1se = Lasso(alpha=alpha_1se, max_iter=10000)\n",
    "start_time = time.time()\n",
    "lasso_1se.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement pour alpha_1se : {end_time - start_time} secondes\")\n",
    "non_zero_1se = (lasso_1se.coef_ != 0).sum()\n",
    "r2_1se = lasso_1se.score(X_test_calories_scaled, y_test_calories)\n",
    "# Affichage des r√©sultats\n",
    "print(f\"alpha_min (Œª_min) = {alpha_min:.6f}\")\n",
    "print(f\"  -> MSE moyen : {mse_path[min_idx]:.6f}\")\n",
    "print(f\"  -> √âcart-type : {stds[min_idx]:.6f}\")\n",
    "print(f\"  -> Nb variables non nulles : {non_zero_min}\")\n",
    "print(f\"  -> R¬≤ : {r2_min:.6f}\")\n",
    "# Trouver l'indice correspondant √† alpha_1se\n",
    "idx_1se = list(alphas).index(alpha_1se)\n",
    "\n",
    "print(f\"\\nalpha_1se (Œª_1se) = {alpha_1se:.6f}\")\n",
    "print(f\"  -> MSE moyen : {mse_path[idx_1se]:.6f}\")\n",
    "print(f\"  -> √âcart-type : {stds[idx_1se]:.6f}\")\n",
    "print(f\"  -> Nb variables non nulles : {non_zero_1se}\")\n",
    "print(f\"  -> R¬≤ : {r2_1se:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons \n",
    "\n",
    "a. Alpha_min (Œª_min = 0.8)\n",
    "- Pour Œª_min = 0.8 et MSE moyen = 1629.17 :\n",
    "C'est l'erreur minimale moyenne obtenue par validation crois√©e.\n",
    "C'est la valeur de  Œª qui donne les meilleures performances pr√©dictives (mod√®le le plus pr√©cis).\n",
    "\n",
    "- √âcart-type = 139.26 :\n",
    "Indique la variabilit√© des erreurs entre les folds de validation crois√©e.\n",
    "Une valeur √©lev√©e sugg√®re que le mod√®le est instable (sensible aux variations des donn√©es d'entra√Ænement).\n",
    "\n",
    "- 12 variables non nulles :\n",
    "Le mod√®le garde 12 coefficients non nuls ‚Üí mod√®le complexe mais pr√©cis.\n",
    "\n",
    "b. Alpha_1se (Œª_1se = 5.7)\n",
    "- Œª_1se = 5.7 et MSE moyen = 1764.87 (+8.3% vs Œª_min) :\n",
    "L'erreur est dans l'intervalle [Œª_min - 1SE, Œª_min + 1SE] ‚Üí consid√©r√©e comme statistiquement √©quivalente √† l'erreur minimale.\n",
    "\n",
    "- √âcart-type = 241.99 :\n",
    "Variabilit√© accrue ‚Üí le mod√®le simplifi√© est plus sensible aux fluctuations des donn√©es.\n",
    "\n",
    "- 5 variables non nulles :\n",
    "Le mod√®le √©limine 7 variables ‚Üí mod√®le interpr√©table mais potentiellement moins pr√©cis.\n",
    "\n",
    "Ces valeurs sont logiques vu que Alpha_1se represente la plus grande valeur de lambda dont l‚Äôerreur est √† moins d‚Äôun √©cart-type de l‚Äôerreur minimale(favorise un mod√®le plus simple) et que Alpha_min minimise l‚Äôerreur de validation crois√©e (MSE) (donne le meilleur ajustement possible sur les donn√©es de validation). Toutefois, nous pr√©f√©rons g√©n√©ralement afficher ces coefficients en R et pas en python. Ceci est d√ª √† l'absence de Œª_1se natif en python\n",
    "(Scikit-learn ne calcule pas automatiquement Œª_1se, contrairement √† glmnet en R)\n",
    "‚Üí Calcul manuel sujet √† des erreurs (ex: gestion des intervalles de confiance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mod√®le quadratique et ordre √©lev√©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipeline pour le Lasso avec les interactions\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(interaction_only=True, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),  # Good practice before Lasso\n",
    "    ('lasso', Lasso(max_iter=10000))\n",
    "])\n",
    "\n",
    "scoring = {\n",
    "    'r2': 'r2',\n",
    "    'neg_mse': 'neg_mean_squared_error'\n",
    "}\n",
    "\n",
    "# grille de param√®tres pour le Lasso\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3],      # Tune the interaction degree\n",
    "    'lasso__alpha': np.logspace(-2, 1, 10)  # Tune the Lasso strength\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='r2',  # On choisit sur quelle m√©trique choisir le best_estimator_\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    n_jobs=6 # run en parall√®le\n",
    ")\n",
    "\n",
    "grid.fit(X_train_calories_scaled, y_train_calories)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul du temps d'entrainement pour une configuration sp√©cifique\n",
    "\n",
    "# Cr√©ation du pipeline avec les hyperparam√®tres sp√©cifiques\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(interaction_only=True, include_bias=False, degree=2)), # Degr√© fix√© √† 2\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso', Lasso(alpha=0.1, max_iter=10000)) # Alpha fix√© √† 0.1\n",
    "])\n",
    "\n",
    "# Mesure du temps pour UNE configuration\n",
    "start_time = time.time()\n",
    "pipeline.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Temps d'entra√Ænement pour degree=2 et alpha=0.1 : {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "filtered_row = results[results['params'] == {'lasso__alpha': 1.0, 'poly__degree': 3}]\n",
    "filtered_row[['mean_test_neg_mse']]\n",
    "print(\"Best model R¬≤ (Cross Validation):\", grid.best_score_)\n",
    "print(\"Best model MSE (Cross Validation):\", -filtered_row['mean_test_neg_mse'].values[0] , \"\\n\")\n",
    "\n",
    "print(\"Best model test R¬≤:\", grid.score(X_test_calories_scaled, y_test_calories))\n",
    "print(\"Best model test MSE:\", mean_squared_error(y_test_calories, best_model.predict(X_test_calories_scaled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. R√©cup√©rer le PolynomialFeatures entra√Æn√©\n",
    "poly = best_model.named_steps['poly']\n",
    "\n",
    "# 2. R√©cup√©rer le mod√®le Lasso entra√Æn√©\n",
    "lasso = best_model.named_steps['lasso']\n",
    "\n",
    "# 3. Construire les noms des features\n",
    "feature_names = poly.get_feature_names_out(input_features=X_train_calories_dummy1.columns)\n",
    "\n",
    "# 4. Associer chaque feature √† son coefficient\n",
    "coefs = pd.Series(lasso.coef_, index=feature_names)\n",
    "\n",
    "# 5. Afficher ou trier les coefficients\n",
    "pd.set_option('display.max_rows', None)\n",
    "coefs = coefs.sort_values()\n",
    "print(coefs)\n",
    "coefs = coefs[coefs != 0]  # Garder uniquement les coefficients non nuls\n",
    "\n",
    "# 6. Plot\n",
    "coefs.plot(kind='barh', figsize=(10, 12))\n",
    "plt.title('Coefficients Lasso avec interactions')\n",
    "plt.show()\n",
    "#plot the residuals for the lasso model\n",
    "y_fitted_lasso = best_model.predict(X_train_calories_scaled)\n",
    "y_fitted_lasso_test= best_model.predict(X_test_calories_scaled)\n",
    "print(y_fitted_lasso.shape)\n",
    "print(y_test_calories.shape)\n",
    "residuals_lasso = y_train_calories  - y_fitted_lasso\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=y_fitted_lasso, y=residuals_lasso, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Valeurs r√©elles')\n",
    "plt.ylabel('R√©sidus')\n",
    "plt.title('R√©sidus du mod√®le Lasso avec interactions')\n",
    "plt.show()\n",
    "\n",
    "#print(\"Best model test R¬≤:\", grid.score(X_test_calories_scaled, y_test_calories))\n",
    "mse_lasso_quadratic_test = mean_squared_error(y_test_calories, y_fitted_lasso_test)\n",
    "\n",
    "print(f\"Test Mean Squared Error (MSE) pour Lasso pour l'√©chantillon de test: {mse_lasso_quadratic_test}\")\n",
    "#compute the score for the lasso model from the previous\n",
    "\n",
    "train_score_lasso= best_model.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_lasso= best_model.score(X_test_calories_scaled, y_test_calories)\n",
    "#print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\n",
    "\n",
    "\n",
    "#print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\"\"\"\n",
    "residuals_train = y_train_calories - y_pred_train_calories\n",
    "residuals_test = y_test_calories - y_pred_test_calories\n",
    "\n",
    "# Cr√©ation d'une seule figure\n",
    "plt.figure(figsize=(8, 6))  # Ajuste la taille selon tes besoins\n",
    "y_pred_calories = linear_model.predict(X_test_calories_scaled)\n",
    "y_pred_train_calories = linear_model.predict(X_train_calories_scaled)\n",
    "y_pred_test_calories = linear_model.predict(X_test_calories_scaled)\n",
    "# 1. R√©sidus vs Valeurs ajust√©es\n",
    "sns.residplot(x=y_pred_train_calories, y=residuals_train, lowess=True, \n",
    "              line_kws={'color': 'red', 'lw': 1})\n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîé **Interpr√©tation du Mod√®le Lasso avec Interactions (Polynomial + Lasso)**\n",
    "\n",
    "Ce mod√®le repose sur un encodage polynomial avec interactions uniquement (`interaction_only=True`, degr√© 3), suivi d‚Äôune r√©gularisation L1 (`Lasso`). Cela permet de capturer des **effets combin√©s non lin√©aires** tout en **√©liminant automatiquement les interactions inutiles**.\n",
    "\n",
    "#####  Performances\n",
    "- **R¬≤ test** = **0.993**, **MSE test** ‚âà **542**\n",
    "- Gain substantiel par rapport au Lasso simple (R¬≤ ‚âà 0.979, MSE ‚âà 1638)\n",
    "- Ce mod√®le capture donc beaucoup mieux la complexit√© des relations entre variables.\n",
    "\n",
    "#### Temps de Calcul (1 minute)\n",
    "Complexit√© justifi√©e : Bien que plus lent qu‚Äôun mod√®le lin√©aire (quelques secondes), le gain en performance valide l‚Äôutilisation d‚Äôun mod√®le quadratique.\n",
    "\n",
    "Optimisation : Le Lasso r√©duit la complexit√© en √©liminant les termes non pertinents, √©quilibrant pr√©cision et parcimonie.\n",
    "#####  Interpr√©tation des principales interactions retenues\n",
    "\n",
    "Les **coefficients positifs** indiquent des interactions qui **augmentent** la pr√©diction de `Calories_Burned`, et les **n√©gatifs** celles qui la **diminuent** :\n",
    "\n",
    "---\n",
    "\n",
    "##### Quelques interactions dominantes positives :\n",
    "\n",
    "- **`Avg_BPM √ó Session_Duration`** ‚Üí **+21.44**\n",
    "  > Synergie intensit√©/dur√©e : les longues s√©ances √† haut BPM amplifient la d√©pense calorique (effet non-lin√©aire critique).\n",
    "\n",
    "- **`Session_Duration (hours) Gender_Male`** ‚Üí **+11.42**\n",
    "  > Les hommes tirent un b√©n√©fice calorique suppl√©mentaire des sessions longues, possiblement gr√¢ce √† une endurance musculaire sup√©rieure.\n",
    "\n",
    "##### Quelques interactions dominantes n√©gatives :\n",
    "\n",
    "- **`Age √ó Session_Duration`** ‚Üí **‚àí10.6**\n",
    "  > √Ä dur√©e d'entrainement √©quivalents, l'√¢ge **r√©duit fortement** la d√©pense calorique. Cela confirme et approfondit l‚Äôeffet observ√© dans les PDP, en le liant au BPM et √† la dur√©e. Un marqueur indirect tr√®s probable du **d√©clin m√©tabolique d√ª au vieillissement**.\n",
    "\n",
    "- **`Age √ó Avg_BPM`** ‚Üí **‚àí2.35**\n",
    "  >  √Ä fr√©quence cardiaque √©quivalente, les seniors br√ªlent moins, possiblement d√ª √† une VO‚ÇÇ max (d√©bit maximum d'oxyg√®ne) r√©duite.\n",
    "\n",
    "\n",
    "#### Conclusion \n",
    "\n",
    "> *Le mod√®le polynomial r√©gularis√© par Lasso am√©liore significativement la pr√©diction (R¬≤ ‚âà 0.993, MSE ‚âà 571), en capturant des effets d‚Äôinteractions complexes entre l‚Äô√¢ge, l‚Äôintensit√© de l‚Äôeffort, la dur√©e des s√©ances et certaines caract√©ristiques morphologiques (poids, sexe). Contrairement au Lasso simple ou au mod√®le lin√©aire, cette approche met en √©vidence des synergies physiologiques r√©alistes, comme la chute d‚Äôefficacit√© m√©tabolique li√©e √† l‚Äô√¢ge ou l‚Äôimpact combin√© du sexe et de la charge cardiaque. Cette complexit√© justifie le recours √† un mod√®le non lin√©aire, √† la fois performant et interpr√©table.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous √©tudierons bri√®vement l'effet d'une p√©nalisation plus stricte sur le mod√®le via Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1) Instanciation sans n_jobs ni random_state\n",
    "start_time = time.time()\n",
    "ridgereg = RidgeCV(alphas=np.arange(1, 50) / 20., cv=5)\n",
    "\n",
    "# 2) Entra√Ænement\n",
    "\n",
    "ridgereg.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "# 3) Alpha optimal\n",
    "optimal_alpha = ridgereg.alpha_\n",
    "print(f\"Optimal alpha: {optimal_alpha}\")\n",
    "\n",
    "# 4) Coefficients\n",
    "coef_calories_ridge = pd.Series(ridgereg.coef_, index=X_train_calories_dummy1.columns)\n",
    "print(\"Coefficients du mod√®le Ridge pour Calories Burned:\")\n",
    "print(coef_calories_ridge)\n",
    "\n",
    "# 5) Comme Ridge ne met quasiment jamais un coefficient strictement √† 0, \n",
    "#    le comptage ¬´ conserv√© / supprim√© ¬ª n‚Äôest pas tr√®s significatif, mais :\n",
    "print(f\"Nombre de coefficients non nuls : {sum(coef_calories_ridge != 0)}\")\n",
    "\n",
    "# 6) Trac√©\n",
    "coef_calories_ridge.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du mod√®le Ridge pour Calories Burned')\n",
    "plt.show()\n",
    "\n",
    "# 7) Pr√©diction et MSE\n",
    "y_pred_ridge = ridgereg.predict(X_test_calories_scaled)\n",
    "mse_ridge = mean_squared_error(y_test_calories, y_pred_ridge)\n",
    "print(f\"MSE pour Ridge : {mse_ridge:.4f}\")\n",
    "\n",
    "# 8) R¬≤ (score) entra√Ænement et test\n",
    "train_score_ridge = ridgereg.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_ridge  = ridgereg.score(X_test_calories_scaled,  y_test_calories)\n",
    "print(f\"Train R¬≤ pour Ridge : {train_score_ridge:.4f}\")\n",
    "print(f\"Test  R¬≤ pour Ridge : {test_score_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mod√®le Ridge obtient un MSE de 1 661,23, un R¬≤ entra√Ænement de 0,9791 et un R¬≤ test de 0,9787. Ce MSE l√©g√®rement plus √©lev√© que celui du Lasso s‚Äôexplique par une p√©nalisation Œª* plus forte : Ridge r√©partit son effet de r√©gularisation sur toutes les variables (biais mod√©r√© mais constant), alors que le Lasso, avec un Œª optimal plus faible, parvient √† conserver un ajustement un peu plus pr√©cis.\n",
    "\n",
    "Cependant, les performances des deux mod√®les lin√©aires restent tr√®s proches :\n",
    "\n",
    "Lasso (Œª_min) : MSE test ‚âÉ 1 638,14, R¬≤ test ‚âÉ 0,9790\n",
    "\n",
    "Ridge : MSE test ‚âÉ 1 661,23, R¬≤ test ‚âÉ 0,9787\n",
    "\n",
    "Enfin, le Lasso quadratique (avec interactions) surpasse nettement ces deux approches lin√©aires, avec un MSE test ‚âÉ 570,61 et un R¬≤ test ‚âÉ 0,9927, gr√¢ce √† sa capacit√© √† capturer des relations non lin√©aires entre les variables mais reste un peu plus lent niveau temps d'entrainement \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apr√®s avoir analys√© les performances du mod√®le Lasso et de Ridge et identifi√© l'alpha optimal pour r√©gulariser notre r√©gression, nous allons maintenant explorer une approche alternative en utilisant la r√©gression par vecteurs de support (SVR) afin de comparer ses performances et sa capacit√© √† capturer des relations potentiellement non lin√©aires dans les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR SUR CALORIES BURNED : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#calibrage des param√®tres c et gamma\n",
    "\n",
    "param = [{'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']}]\n",
    "param_lin_opt= GridSearchCV(SVR(),param,refit=True,verbose=3)\n",
    "start_time = time.time()\n",
    "param_lin_opt.fit(X_train_calories_scaled,y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "print(param_lin_opt.best_params_)\n",
    "start_time = time.time()\n",
    "y_pred_svr_lin = param_lin_opt.predict(X_test_calories_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"Temps de pr√©diction : {end_time - start_time} secondes\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_lin, color='darkorange', label='Pr√©dictions vs R√©elles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite pr√©diction')\n",
    "plt.xlabel('Valeurs R√©elles (Calories_Burned)')\n",
    "plt.ylabel('Pr√©dictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR Lin√©aire - Comparaison Pr√©dictions/R√©elles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_score_lin= r2_score(y_test_calories,y_pred_svr_lin)\n",
    "print(f\"R¬≤ pour SVR lin: {R2_score_lin}\")\n",
    "mse_svr_lin = mean_squared_error(y_test_calories, y_pred_svr_lin)\n",
    "print(f\"MSE pour SVR poly: {mse_svr_lin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rbf=[{'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}]\n",
    "parmopt_rbf = GridSearchCV(SVR(), param_rbf, refit = True, verbose = 3)\n",
    "parmopt_rbf.fit(X_train_calories_scaled, y_train_calories)\n",
    "print(parmopt_rbf.best_params_)\n",
    "start_time = time.time()\n",
    "y_pred_svr_rbf = parmopt_rbf.predict(X_test_calories_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"Temps de pr√©diction : {end_time - start_time} secondes\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_rbf, color='darkorange', label='Pr√©dictions vs R√©elles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite pr√©diction')\n",
    "plt.xlabel('Valeurs R√©elles (Calories_Burned)')\n",
    "plt.ylabel('Pr√©dictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR rbf - Comparaison Pr√©dictions/R√©elles')\n",
    "plt.show()\n",
    "\n",
    "R2_score_rbf= r2_score(y_test_calories,y_pred_svr_rbf)\n",
    "print(f\"R¬≤ pour SVR rbf: {R2_score_rbf}\")\n",
    "mse_svr_rbf = mean_squared_error(y_test_calories, y_pred_svr_rbf)\n",
    "print(f\"MSE pour SVR rbf: {mse_svr_rbf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_poly=[{'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['poly']}]\n",
    "parmopt_poly = GridSearchCV(SVR(), param_poly, refit = True, verbose = 3)\n",
    "time_start = time.time()\n",
    "parmopt_poly.fit(X_train_calories_scaled, y_train_calories)\n",
    "time_end = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {time_end - time_start} secondes\")\n",
    "print(parmopt_poly.best_params_)\n",
    "\n",
    "y_pred_svr_poly = parmopt_poly.predict(X_test_calories_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_poly, color='darkorange', label='Pr√©dictions vs R√©elles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite pr√©diction')\n",
    "plt.xlabel('Valeurs R√©elles (Calories_Burned)')\n",
    "plt.ylabel('Pr√©dictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR poly - Comparaison Pr√©dictions/R√©elles')\n",
    "plt.show()\n",
    "\n",
    "R2_score_poly= r2_score(y_test_calories,y_pred_svr_poly)\n",
    "print(f\"R¬≤ pour SVR poly: {R2_score_poly}\")\n",
    "mse_svr_poly = mean_squared_error(y_test_calories, y_pred_svr_poly)\n",
    "print(f\"MSE pour SVR poly: {mse_svr_poly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Performances des diff√©rents noyaux SVR**\n",
    "\n",
    "a. SVR avec noyau RBF (Radial Basis Function)\n",
    "R¬≤ = 0,992 et MSE = 636,57\n",
    "\n",
    "=> Le mod√®le RBF parvient √† expliquer 99,2 % de la variance des calories br√ªl√©es, avec une erreur quadratique moyenne extr√™mement basse.\n",
    "Ceci est d√ª au fait que le noyau RBF est capable de capturer des relations non lin√©aires complexes (par exemple, l‚Äôinteraction entre la dur√©e de s√©ance et la fr√©quence cardiaque moyenne). Les hyperparam√®tres C (r√©gularisation) et gamma (√©tendue d‚Äôinfluence) ont √©t√© optimis√©s via GridSearchCV, garantissant un compromis id√©al entre biais et variance.\n",
    "\n",
    "b. SVR avec noyau lin√©aire\n",
    "R¬≤ = 0,977 et MSE = 1 790,89\n",
    "\n",
    "=> Le SVR lin√©aire offre √©galement de bonnes performances , mais nettement inf√©rieures au noyau RBF (erreur MSE beaucoup plus √©lev√©e).\n",
    "Ceci pourrait √™tre d√ª au fait que\n",
    "\n",
    "c. SVR avec noyau polynomial\n",
    "R¬≤ = 0,949 et MSE = 3 952,21\n",
    "\n",
    "=> Les r√©sultats sont bien plus faibles, avec une erreur environ 6 fois sup√©rieure √† celle du RBF.\n",
    "\n",
    "\n",
    "2. **Comparaison des Performances : Lasso Quadratique vs SVR RBF**\n",
    "\n",
    "| Crit√®re               | Lasso Quadratique (Interactions) | SVR RBF              |\n",
    "|-----------------------|-----------------------------------|----------------------|\n",
    "| **MSE (Test)**        | **570.61**                        | 636.57              |\n",
    "| **R¬≤ (Test)**         | **0.9927**                        | 0.992               |\n",
    "| **Complexit√©**        | Mod√®le lin√©aire avec interactions | Mod√®le non lin√©aire |\n",
    "| **Interpr√©tabilit√©**  | Coefficients explicables          | \"Bo√Æte noire\"       |\n",
    "| **Flexibilit√©**       | Capte interactions sp√©cifiques    | Adapt√© aux relations complexes/g√©n√©riques |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Points Cl√©s :**\n",
    "1. **Performance Pr√©dictive** :  \n",
    "   - Le **Lasso Quadratique** est l√©g√®rement meilleur en MSE (+10% d'erreur pour SVR RBF).  \n",
    "   - Les deux mod√®les ont un R¬≤ quasi identique (> 0.99), indiquant une explication quasi parfaite de la variance.\n",
    "\n",
    "2. **Equilibre Complexit√©/Interpr√©tabilit√©** :  \n",
    "   - **Lasso Quadratique** : Moins flexible mais interpr√©table (coefficients des interactions analysables).  \n",
    "   - **SVR RBF** : Plus flexible mais difficile √† expliquer (d√©pend de la fonction noyau).\n",
    "\n",
    "3. **Choix du mod√®le** :  \n",
    "   - **Lasso Quadratique** : Si l‚Äôon privil√©gie l‚Äôerreur quadratique minimale et la parcimonie  \n",
    "   - **SVR RBF** : Si l‚Äôon recherche avant tout la flexibilit√© pour capter des structures non-lin√©aires plus subtiles\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbres et Forest al√©atoires\n",
    "### Arbre de d√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_calories_dummy = pd.get_dummies(X_train_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "X_test_calories_dummy = pd.get_dummies(X_test_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "# Normalisation des donn√©es - scaled = Scale + Dummies alors que scale = just scale\n",
    "X_train_calories_scaled = scaler.fit_transform(X_train_calories_dummy)\n",
    "X_test_calories_scaled = scaler.transform(X_test_calories_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit a regression tree model for Calories_Burned using dummy variables\n",
    "tree_reg_cal = DecisionTreeRegressor(random_state=randomseed, ccp_alpha=0.001)\n",
    "start_time = time.time()\n",
    "tree_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(18, 12))\n",
    "plot_tree(tree_reg_cal, feature_names=X_train_calories_dummy.columns, filled=True, rounded=True, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Compute MSE and R2 on training and test sets\n",
    "y_train_pred = tree_reg_cal.predict(X_train_calories_dummy)\n",
    "y_test_pred = tree_reg_cal.predict(X_test_calories_dummy)\n",
    "\n",
    "mse_train = mean_squared_error(y_train_calories, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test_calories, y_test_pred)\n",
    "r2_train = r2_score(y_train_calories, y_train_pred)\n",
    "r2_test = r2_score(y_test_calories, y_test_pred)\n",
    "\n",
    "print(\"MSE on training set: \", mse_train)\n",
    "print(\"MSE on test set: \", mse_test)\n",
    "print(\"R2 on training set: \", r2_train)\n",
    "print(\"R2 on test set: \", r2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Nous avons initialement construit un arbre de r√©gression avec un param√®tre de complexit√© extr√™mement faible (`cp = 0.01`). Comme attendu, ce mod√®le pr√©sente une structure profond√©ment ramifi√©e, caract√©ristique d'un sur-apprentissage. Ce mod√®le pr√©sente queasiment aucun biais sur le jeu d‚Äôentra√Ænement (R¬≤ = 0.999, MSE = 0.027), mais un √©cart significatif entre l‚Äôerreur d‚Äôentra√Ænement et de test (MSE_test = 4484) r√©v√®le un sur-apprentissage. Toutefois, le R¬≤ sur le test reste √©lev√© (0.934), indiquant que le mod√®le capture une part substantielle de la variance explicative, malgr√© sa complexit√© excessive. Le mod√®le d'arbre en Python est plus complexe qu'en R alors que nous utilisons un cp plus √©lev√© (`cp=0.01`), tandis que R utilise un `cp=0.001`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for best cp \n",
    "\n",
    "scoring = {\n",
    "    'r2': 'r2',\n",
    "    'neg_mse': 'neg_mean_squared_error'\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'ccp_alpha': np.logspace(-4, 2, 15)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(tree_reg_cal, params, scoring=scoring, cv=5, refit='r2', n_jobs=-1)\n",
    "grid.fit(X_train_calories_dummy, y_train_calories)\n",
    "grid_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# plot the results as a function of ccp_alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(grid_results['param_ccp_alpha'], grid_results['mean_test_neg_mse'] * -1, label='MSE', marker='o')\n",
    "plt.xlabel('Complexity Parameter (alpha)')\n",
    "plt.ylabel('Cross-Validation Error')\n",
    "plt.title('Cross-Validation Error vs Complexity Parameter')\n",
    "plt.grid(True, which=\"both\", ls=\"-\")\n",
    "\n",
    "optimal_alpha = np.argmin(grid_results['mean_test_neg_mse'] * -1)\n",
    "plt.axvline(grid_results['param_ccp_alpha'][optimal_alpha], color='red', linestyle='--', label='Optimal alpha')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(18, 12))\n",
    "tree_reg_cal_optimal = grid.best_estimator_\n",
    "plot_tree(tree_reg_cal_optimal, feature_names=X_train_calories_dummy.columns, filled=True, rounded=True, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# display the dataframe with top 5 results from mean_test_neg_mse\n",
    "display(grid_results[['param_ccp_alpha', 'mean_test_neg_mse', 'std_test_neg_mse', 'rank_test_neg_mse']].sort_values(by='mean_test_neg_mse', ascending=False).head(5))\n",
    "# same for r2\n",
    "\n",
    "display(grid_results[['param_ccp_alpha', 'mean_test_r2', 'std_test_r2', 'rank_test_r2']].sort_values(by='mean_test_r2', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Par validation crois√©e, nous avons d√©termin√© que le meilleur param√®tre d'√©lagage est `ccp ‚âà 5.18`. L'arbre de regression r√©sultant est moins complexe que le pr√©c√©dent, mais est encore trop ramifi√©, comme celui de R. Ce mod√®le est un peu moins performant que celui de R, avec un MSE calcul√© par cross-validation 5-fold de 5017 ici contre 4521 pour le mod√®le de R. Le R¬≤ est similaire dans les deux langages (~0.93) en revanche. Cela souligne que le mod√®le de r√©gression est tout de m√™me robuste, malgr√© la complexit√© de l'arbre.\n",
    "\n",
    "Nous allons pouvoir explorer d'autres m√©thodes d'arbres de d√©cision, comme les for√™ts al√©atoires et le boosting, qui sont souvent plus performantes que les arbres de d√©cision simples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For√™ts al√©atoires\n",
    "\n",
    "#### Simple random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Cr√©er et entra√Æner une for√™t al√©atoire\n",
    "rf_reg_cal = RandomForestRegressor(random_state=randomseed, oob_score=True)\n",
    "rf_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "\n",
    "# Pr√©dictions sur les ensembles d'entra√Ænement et de test\n",
    "y_train_pred_rf = rf_reg_cal.predict(X_train_calories_dummy)\n",
    "y_test_pred_rf = rf_reg_cal.predict(X_test_calories_dummy)\n",
    "\n",
    "# Calculer le MSE et le R2\n",
    "mse_train_rf = mean_squared_error(y_train_calories, y_train_pred_rf)\n",
    "mse_test_rf = mean_squared_error(y_test_calories, y_test_pred_rf)\n",
    "r2_train_rf = r2_score(y_train_calories, y_train_pred_rf)\n",
    "r2_test_rf = r2_score(y_test_calories, y_test_pred_rf)\n",
    "\n",
    "print(\"Random Forest - OOB score :\", rf_reg_cal.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Le mod√®le de base basique al√©atoire de `scikit-learn` est construit avec 100 arbres, avec les param√®tres `min_samples_split = 2` (nombre minimum d'√©lements pour consid√©rer une d√©cision) et `min_samples_leaf = 1` (nombre minimum d'√©lement dans une feuille). Ces param√®tres sont les valeurs par d√©faut de `scikit-learn`, mais nous allons les optimiser par la suite. \n",
    "\n",
    "Le mod√®le est construit avec un √©chantillonnage bootstrap, ce qui signifie que chaque arbre est construit sur un sous-ensemble al√©atoire des donn√©es d'entra√Ænement. Cela nous permet d'extraire l'erreur OOB qui est calcul√© par d√©faut avec le score R¬≤ dans `scikit-learn`, alors qu'en R, elle est traditionnellement √©valu√©e via la somme des r√©sidus au carr√© (RSS, Residual Sum of Squares).\n",
    "\n",
    "Contrairement √† R ou le param√®tre √† optimiser est `mtry` (nombre de variables consid√©r√©es √† chaque split), `scikit-learn` nous permet d'optimiser plusieurs hyperparam√®tres essentiels :\n",
    "- **`max_depth`** : la profondeur maximale de chaque arbre (plus un arbre est profond, plus il peut mod√©liser des interactions complexes, mais aussi surapprendre). \n",
    "- **`min_samples_split`** : le nombre minimum d'√©chantillons requis pour diviser un noeud. Plus il est grand, plus l‚Äôarbre est contraint et moins il risque de surapprendre.\n",
    "- **`min_samples_leaf`** : le nombre minimum d'√©chantillons n√©cessaires dans une feuille terminale. Cela permet d‚Äô√©viter des feuilles trop petites, ce qui am√©liore la robustesse.\n",
    "- **`max_features`** : le nombre maximal de variables consid√©r√©es pour chercher le meilleur split √† chaque division (√©quivalent au `mtry` de R). Peut √™tre fix√© √† un nombre entier, √† une proportion de la taille du sample (`float` entre 0 et 1), ou aux valeurs pr√©d√©finies `'sqrt'` : $\\sqrt{n_\\text{variables}}$ ou `'log2'` : $\\log_2(n_\\text{variables})$.\n",
    "- **`max_leaf_nodes`** : limite le nombre total de feuilles de l‚Äôarbre, for√ßant une structure plus simple.\n",
    "- **`ccp_alpha`** : le param√®tre de co√ªt-complexit√© pour l'√©lagage (post-pruning) ; plus `ccp_alpha` est grand, plus l'√©lagage sera fort.\n",
    "\n",
    "Enfin, il nous est √©galement permis de choisir le **crit√®re d‚Äô√©valuation** de la qualit√© du split (`criterion`).   \n",
    "Alors qu‚Äôen R, la performance est √©valu√©e via le **RSS** (Residual Sum of Squares), l‚Äôoption la plus proche disponible dans `scikit-learn` est `friedman_mse`, con√ßue pour optimiser la variance r√©siduelle de mani√®re similaire au RSS.  \n",
    "Ici, nous avons l'occasion de comparer l'impact du choix du crit√®re (`friedman_mse` vs `squared_error`) sur la construction des arbres.  \n",
    "\n",
    "Nous observerons notamment l'effet sur la performance de g√©n√©ralisation (via le score OOB R¬≤) ainsi que sur le temps d'apprentissage et d'√©lagage.\n",
    "Le score OOB √©tant uniquement calcul√© sur la m√©trique R¬≤ sous `scikit-learn`, le mod√®le optimal ne sera pas directement comparable aux mesures obtenues en R (RSS).\n",
    "\n",
    "Par ailleurs, ces hyperparam√®tres **sont interd√©pendants** : en pratique, optimiser l'hyperparam√®tre `max_leaf_nodes` peut r√©duire la n√©cessit√© d'√©laguer l'arbre, ou la n√©c√©ssit√© de d√©finir `max_depth`. \n",
    "\n",
    "Nous avons d√©cid√© de construire un mod√®le de for√™t al√©atoire avec les param√®tres par d√©faut et optimiser les hyperparam√®tres `n_estimators` et `max_features` ainsi que le param√®tre `ccp_alpha` pour l'√©lagage, que nous avons vu en cours, mais que nous avons pas appliqu√© dans le mod√®le de R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest avec √©lagage\n",
    "\n",
    "Comme les for√™ts al√©atoires sont construits avec un √©chantillonnage bootstrap, nous pouvons estimer l'**erreur OOB (Out-Of-Bag) pour √©valuer la performance du mod√®le**. Ainsi nous n'avons pas besoin d'utiliser la validation crois√©e pour √©valuer le mod√®le et d√©terminer les meilleurs hyperparam√®tres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# D√©finir le grille de param√®tres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': np.linspace(0.1, 1.0, 10),  # proportion du nombre total de variables\n",
    "    'ccp_alpha': [0.01, 0.1, 1.0, 5.0, 10.0],\n",
    "    'criterion': ['friedman_mse', 'squared_error'],  # Comparer plusieurs crit√®res !\n",
    "    'oob_score': [True],\n",
    "}\n",
    "\n",
    "# G√©n√©rer toutes les combinaisons possibles\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# Fonction pour entra√Æner et √©valuer\n",
    "def train_and_evaluate(params):\n",
    "    model = RandomForestRegressor(random_state=randomseed, **params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_calories_dummy, y_train_calories)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'n_estimators': params['n_estimators'],\n",
    "        'max_features': params['max_features'],\n",
    "        'ccp_alpha': params['ccp_alpha'],\n",
    "        'criterion': params['criterion'],\n",
    "        'oob_score': model.oob_score_,\n",
    "        'training_time_sec': elapsed_time,\n",
    "    }\n",
    "\n",
    "# Parall√©liser\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_and_evaluate)(params) for params in param_combinations\n",
    ")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Trier par oob_score d√©croissant\n",
    "results_df = results_df.sort_values(by='oob_score', ascending=False)\n",
    "\n",
    "# Afficher\n",
    "# display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_max_features = [0.9, 0.6, 0.4, 0.1]\n",
    "selected_ccp_alpha = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# Cr√©er 4 sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))  # 2x2 grid\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, alpha in enumerate(selected_ccp_alpha):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Sous-ensemble des r√©sultats pour ce ccp_alpha\n",
    "    subset = results_df[results_df['ccp_alpha'] == alpha]\n",
    "    \n",
    "    for max_feat in selected_max_features:\n",
    "        # Prendre uniquement les lignes correspondant √† un max_features donn√©\n",
    "        curve = subset[np.isclose(subset['max_features'], max_feat)]\n",
    "        # Trier par n_estimators pour des courbes bien propres\n",
    "        curve = curve.sort_values('n_estimators')\n",
    "        \n",
    "        ax.plot(curve['n_estimators'], curve['oob_score'], marker='o', label=f'max_features = {max_feat}')\n",
    "    \n",
    "    ax.set_title(f'OOB Score vs n_estimators (ccp_alpha = {alpha})')\n",
    "    ax.set_xlabel('n_estimators')\n",
    "    ax.set_ylabel('OOB R¬≤ Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parmi les 100 meilleures combinaisons, sortir les 10 plus longues √† fitter et les 10 plus courtes\n",
    "best_results_df = results_df[results_df['oob_score'] > 0.974].sort_values(by='training_time_sec', ascending=False).copy()\n",
    "display(best_results_df.head(10))\n",
    "\n",
    "display(best_results_df.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Interpr√©tation des r√©sulats de la for√™t al√©atoire** :\n",
    "\n",
    "Nous avons r√©alis√© une analyse fine de la performance de la for√™t al√©atoire en fonction de plusieurs hyperparam√®tres (`n_estimators`, `max_features`, `ccp_alpha`), en nous concentrant sur l'estimation de l'erreur de g√©n√©ralisation via l'**OOB score**.\n",
    "\n",
    "$\\rightarrow$ **Influence du crit√®re de split (`criterion`)**\n",
    "\n",
    "En observant le tableau des r√©sultats, nous constatons que **le choix du crit√®re `friedman_mse` ou `squared_error` n‚Äôimpacte pratiquement pas la performance du mod√®le**.  \n",
    "Que ce soit en termes de **score OOB** ou de **temps d'entra√Ænement**, les deux crit√®res m√®nent aux **m√™mes choix optimaux d'hyperparam√®tres**, avec des performances quasi-identiques.  \n",
    "Cela montre que, dans le cas de la for√™t al√©atoire, **le crit√®re de construction locale des arbres influence peu la qualit√© globale du mod√®le**.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Influence de `max_features`**\n",
    "\n",
    "Comme nous l'avions observ√© lors de la mod√©lisation sous R, **plus la proportion de variables s√©lectionn√©es √† chaque split est √©lev√©e, meilleure est la performance de la for√™t**.  \n",
    "Ici, c'est avec `max_features = 0.9` que nous obtenons les meilleurs scores OOB.\n",
    "\n",
    "En proposant davantage de variables au moment de cr√©er les divisions, chaque arbre a acc√®s √† plus d'information pour produire des splits efficaces, ce qui am√©liore la qualit√© globale de la for√™t.\n",
    "\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Influence de `ccp_alpha` (√©lagage)**\n",
    "\n",
    "L'√©lagage, contr√¥l√© via le param√®tre `ccp_alpha`, **semble avoir un effet n√©gligeable sur la performance OOB**.\n",
    "\n",
    "Quelle que soit la valeur choisie (0.01, 0.1, 1.0, 10.0), l'OOB score reste quasiment stable.  \n",
    "Cela indique que **le mod√®le est naturellement robuste** et peu sensible au surapprentissage, m√™me sans √©lagage agressif.\n",
    "\n",
    "Cela confirme l'intuition classique en for√™t al√©atoire : **l'overfitting n'est pas un probl√®me majeur** gr√¢ce √† l'agr√©gation de nombreux arbres faibles.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Performances extr√™mes (meilleur mod√®le)**\n",
    "\n",
    "- Le **meilleur mod√®le** atteint un **OOB score** de **0.975666** et a n√©cessit√© **5.211 secondes** pour √™tre entra√Æn√©.\n",
    "- Ce mod√®le utilise :\n",
    "  - `n_estimators = 500`\n",
    "  - `max_features = 1.0`\n",
    "  - `ccp_alpha = 1.0`\n",
    "  - `criterion = friedman_mse`\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Trade-off performance/temps**\n",
    "\n",
    "Parmi les mod√®les ayant un OOB score > 0.974, **le plus rapide** a pris seulement **0.897 secondes** pour un OOB score de **0.974343** (`n_estimators=100`, `max_features=0.8`, `ccp_alpha=0.10`).\n",
    "\n",
    "Cela montre que **des mod√®les plus l√©gers peuvent offrir des performances presque √©quivalentes** tout en √©tant **beaucoup plus rapides** √† entra√Æner.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **D√©tail des mod√®les extr√™mes**\n",
    "\n",
    "- **Top 10 mod√®les les plus longs √† entra√Æner** (extraits du tableau) : majoritairement avec `n_estimators = 500`.\n",
    "- **Top 10 mod√®les les plus rapides** : configurations avec `n_estimators = 100` et `max_features` entre 0.8 et 1.0.\n",
    "\n",
    "Cela est coh√©rent avec l'id√©e que **plus le nombre d'arbres est √©lev√©, plus le temps d'entra√Ænement augmente**.\n",
    "\n",
    "--- \n",
    "\n",
    "**Conclusion** :\n",
    "\n",
    "Dans l'ensemble, nous constatons que :\n",
    "- **Un `max_features` √©lev√©** permet d'am√©liorer significativement la performance du mod√®le.\n",
    "- **Le param√®tre `ccp_alpha` (√©lagage) impacte tr√®s peu la qualit√© de la for√™t**.\n",
    "- **R√©duire `n_estimators`** permet **d‚Äôacc√©l√©rer consid√©rablement** l'entra√Ænement sans perte substantielle de performance.\n",
    "- **La for√™t al√©atoire reste robuste** face au surapprentissage, m√™me avec des arbres profonds et peu √©lagu√©s.\n",
    "\n",
    "  \n",
    "Apr√®s avoir valid√© ces r√©sultats, nous allons d√©sormais nous int√©resser √† **l‚Äôimportance des variables**, afin d‚Äôidentifier les facteurs les plus influents dans la pr√©diction des calories, comme nous l'avions fait sous R.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the best random forest model \n",
    "\n",
    "best_rf_reg_cal = RandomForestRegressor(random_state=randomseed, n_estimators=500, max_features=0.9, ccp_alpha=0.01, criterion='friedman_mse', oob_score=True)\n",
    "best_rf_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "\n",
    "# extract variable importance\n",
    "importances = best_rf_reg_cal.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train_calories_dummy.columns[indices]\n",
    "importances = importances[indices]\n",
    "importances_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importances_df['Cumulative Importance'] = importances_df['Importance'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(importances_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_df,\n",
    "    palette='cool',\n",
    ")\n",
    "plt.title(\"Variable Importance from Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : √Ä partir du mod√®le de for√™t al√©atoire optimal entra√Æn√© sous `scikit-learn`, nous avons extrait l'importance des variables bas√©e sur la r√©duction de l'impuret√© cumul√©e (Gini importance). \n",
    "\n",
    "- Le pr√©dicateur `Session_Duration (hours)` domine, expliquant 73.67% de la variance, ce qui est intuitif puisqu'une **session plus longue** implique m√©caniquement **une d√©pense √©nerg√©tique plus √©lev√©e**.\n",
    "- Il est suivi par `Avg_BPM`, qui contribue √† 10.56% de la variance, ce qui est √©galement logique car un rythme cardiaque lors d'une s√©ane de sport plus √©lev√© est souvent associ√© √† une **d√©pense calorique accrue**.\n",
    "- Enfin, `SFat_Percentage`, `Experience_Level` et `Age` ont des contributions faibles, mais permettent de capter des interactions int√©ressantes et am√©liorent la performance globale du mod√®le.\n",
    "\n",
    "On observe ainsi que 5 variables expliquent √† elles seules plus de **97 % de l'importance totale du mod√®le**.\n",
    "\n",
    "En revanche, sous R, les variables `Session_Duration (hours)` et `Avg_BPM` √©taient les seules √† ressortir comme les plus importantes, tandis que toutes les autres variables avaient une importance tr√®s faible. Ainsi, on peut d√©duire que `scikit-learn` ne construit pas les for√™ts al√©atoires de la m√™me mani√®re que `caret` sous R.\n",
    "\n",
    "Nous allons maintenant nous int√©resser √† un autre algorithme d'arbres de d√©cision, le **boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Gradient Boosting & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# D√©finir le nombre de folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=randomseed)\n",
    "\n",
    "# Stocker les scores et temps\n",
    "r2_scores_gb = []\n",
    "mse_scores_gb = []\n",
    "times_gb = []\n",
    "\n",
    "r2_scores_xgb = []\n",
    "mse_scores_xgb = []\n",
    "times_xgb = []\n",
    "\n",
    "# Boucle sur les folds\n",
    "for train_index, val_index in kf.split(X_train_calories):\n",
    "    X_train_fold, X_val_fold = X_train_calories.iloc[train_index], X_train_calories.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_calories.iloc[train_index], y_train_calories.iloc[val_index]\n",
    "    \n",
    "    # Dummifier pour Gradient Boosting (pas pour XGBoost car enable_categorical=True)\n",
    "    X_train_fold_dummies = pd.get_dummies(X_train_fold, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "    X_val_fold_dummies = pd.get_dummies(X_val_fold, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "    \n",
    "    ## 1. Gradient Boosting\n",
    "    start_time = time.time()\n",
    "    gb_reg = GradientBoostingRegressor(random_state=randomseed)\n",
    "    gb_reg.fit(X_train_fold_dummies, y_train_fold)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_gb = gb_reg.predict(X_val_fold_dummies)\n",
    "    \n",
    "    r2_scores_gb.append(r2_score(y_val_fold, y_pred_gb))\n",
    "    mse_scores_gb.append(mean_squared_error(y_val_fold, y_pred_gb))\n",
    "    times_gb.append(elapsed_time)\n",
    "    \n",
    "    ## 2. XGBoost\n",
    "    start_time = time.time()\n",
    "    xgb_reg = XGBRegressor(random_state=randomseed, enable_categorical=True)\n",
    "    xgb_reg.fit(X_train_fold, y_train_fold)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_xgb = xgb_reg.predict(X_val_fold)\n",
    "    \n",
    "    r2_scores_xgb.append(r2_score(y_val_fold, y_pred_xgb))\n",
    "    mse_scores_xgb.append(mean_squared_error(y_val_fold, y_pred_xgb))\n",
    "    times_xgb.append(elapsed_time)\n",
    "\n",
    "# R√©sultats finaux\n",
    "print(f\"Gradient Boosting R¬≤ moyen (CV) : {np.mean(r2_scores_gb):.4f} ¬± {np.std(r2_scores_gb):.4f}\")\n",
    "print(f\"Gradient Boosting MSE moyen (CV) : {np.mean(mse_scores_gb):.2f} ¬± {np.std(mse_scores_gb):.2f}\")\n",
    "print(f\"Gradient Boosting Temps moyen d'entra√Ænement (par fold) : {np.mean(times_gb):.2f} sec\")\n",
    "\n",
    "print(f\"\\nXGBoost R¬≤ moyen (CV) : {np.mean(r2_scores_xgb):.4f} ¬± {np.std(r2_scores_xgb):.4f}\")\n",
    "print(f\"XGBoost MSE moyen (CV) : {np.mean(mse_scores_xgb):.2f} ¬± {np.std(mse_scores_xgb):.2f}\")\n",
    "print(f\"XGBoost Temps moyen d'entra√Ænement (par fold) : {np.mean(times_xgb):.2f} sec\")\n",
    "\n",
    "# Performance sur le test final\n",
    "print(f\"\\nGradient Boosting R¬≤ sur l'ensemble de test : {r2_score(y_test_calories, gb_reg.predict(X_test_calories_dummy)):.4f}\")\n",
    "print(f\"Gradient Boosting MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, gb_reg.predict(X_test_calories_dummy)):.2f}\")\n",
    "\n",
    "print(f\"XGBoost R¬≤ sur l'ensemble de test : {r2_score(y_test_calories, xgb_reg.predict(X_test_calories)):.4f}\")\n",
    "print(f\"XGBoost MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, xgb_reg.predict(X_test_calories)):.2f}\")\n",
    "\n",
    "# Comparaison avec Random Forest\n",
    "\n",
    "print(f\"\\nRandom Forest R¬≤ sur l'ensemble de test : {r2_score(y_test_calories, best_rf_reg_cal.predict(X_test_calories_dummy)):.4f}\")\n",
    "print(f\"Random Forest MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, best_rf_reg_cal.predict(X_test_calories_dummy)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** \n",
    "\n",
    "Les mod√®les de Gradient Boosting et de XGBoost pr√©sentent **d'excellentes performances** sans ajustement particulier des hyperparam√®tres.  \n",
    "\n",
    "√Ä l'issue de la validation crois√©e 5-folds :\n",
    "- Le Gradient Boosting standard atteint un **R¬≤ moyen de 0.9937 ¬± 0.0014** et un **MSE moyen de 451.17 ¬± 69.83**,\n",
    "- Le mod√®le XGBoost atteint un **R¬≤ moyen de 0.9822 ¬± 0.0047** et un **MSE moyen de 1269.52 ¬± 255.84**.\n",
    "\n",
    "Les performances sur l'ensemble de test confirment cette excellente capacit√© de g√©n√©ralisation :\n",
    "- Le Gradient Boosting obtient un **R¬≤ de 0.9903** et un **MSE de 755.82**,\n",
    "- Le XGBoost obtient un **R¬≤ de 0.9824** et un **MSE de 1377.15**.\n",
    "\n",
    "On constate ainsi que **les deux mod√®les g√©n√©ralisent tr√®s bien**, sans r√©el ph√©nom√®ne de surapprentissage.\n",
    "\n",
    "En termes de co√ªt computationnel, **les deux algorithmes sont tr√®s rapides √† entra√Æner**, avec des temps moyens d'entra√Ænement par fold d'environ **0.27 seconde pour Gradient Boosting** et **0.32 seconde pour XGBoost**.\n",
    "\n",
    "Comparativement, le mod√®le Random Forest, pr√©c√©demment optimis√©, obtient un **R¬≤ de 0.9768** et un **MSE de 1812.48**, tout en n√©cessitant un **temps d'entra√Ænement beaucoup plus important** (~5.2 secondes).\n",
    "\n",
    "Ces r√©sultats confirment que **les m√©thodes de boosting surpassent les for√™ts al√©atoires** √† la fois en termes de performance pr√©dictive et d'efficacit√© computationnelle.\n",
    "\n",
    "Compte tenu de **ces r√©sultats tr√®s satisfaisants**, notamment pour le Gradient Boosting, nous limiterons notre analyse aux mod√®les actuels sans proc√©der √† une optimisation pouss√©e de XGBoost.\n",
    "Toutefois, dans une d√©marche d'optimisation avanc√©e, une recherche d'hyperparam√®tres sur XGBoost pourrait encore permettre d'am√©liorer ses performances.\n",
    "\n",
    "Dans ce contexte, nous allons d√©sormais nous concentrer sur **l'interpr√©tation de l'importance des variables**.\n",
    "\n",
    "#### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_gb_df = pd.DataFrame({'Feature': X_train_calories_dummy.columns, 'Importance': gb_reg.feature_importances_})\n",
    "importances_xgb_df = pd.DataFrame({'Feature': X_train_calories.columns, 'Importance': xgb_reg.feature_importances_})\n",
    "\n",
    "# Trier pour plus de lisibilit√©\n",
    "importances_gb_df = importances_gb_df.sort_values('Importance', ascending=False)\n",
    "importances_xgb_df = importances_xgb_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Tracer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot pour Gradient Boosting\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_gb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Variable Importance - Gradient Boosting\")\n",
    "axes[0].set_xlabel(\"Importance\")\n",
    "axes[0].set_ylabel(\"Feature\")\n",
    "\n",
    "# Plot pour XGBoost\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_xgb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Variable Importance - XGBoost\")\n",
    "axes[1].set_xlabel(\"Importance\")\n",
    "axes[1].set_ylabel(\"\")  # Pas besoin de r√©p√©ter \"Feature\" √† droite\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que Gradient Boosting et XGBoost obtiennent des performances tr√®s proches en termes de R¬≤, une analyse de l'importance des variables r√©v√®le des diff√©rences notables dans les contributions fines.\n",
    "\n",
    "Dans les deux mod√®les, `Session_Duration (hours)` et `Avg_BPM` dominent largement la pr√©diction, ce qui est coh√©rent avec les r√©sultats pr√©c√©dents observ√©s sous for√™ts al√©atoires et en R.\n",
    "\n",
    "Toutefois, lorsque l'on s'int√©resse aux variables secondaires, **les importances relatives divergent** :\n",
    "- Gradient Boosting r√©partit l'importance restante entre les variables `Age`, `SFat_Percentage` et `Gender_Male` alors que `Experience_Level` est inexistant dans le mod√®le.\n",
    "- XGBoost attribue une importance non n√©gligeable directement √† `Gender` en le mettant au m√™me niveau que `Avg_BPM`, tandis que `Age` et `SFat_Percentage` restent marginaux.\n",
    "\n",
    "Ces diff√©rences s'expliquent par :\n",
    "- **La nature des mod√®les** : XGBoost, utilisant du boosting plus r√©gularis√©, capte parfois des combinaisons d'interactions que Gradient Boosting classique ne priorise pas aussi fortement.\n",
    "- **La mani√®re de calculer l‚Äôimportance** : Gradient Boosting utilise la r√©duction moyenne d'impuret√©, alors que XGBoost utilise une mesure fond√©e sur le gain moyen de splits (avec r√©gularisation int√©gr√©e).\n",
    "\n",
    "**Conclusion** : malgr√© des performances globales similaires, les deux m√©thodes peuvent exploiter **diff√©rentes structures locales dans les donn√©es**, ce qui peut √™tre pr√©cieux en cas de recherche d'interpr√©tabilit√© avanc√©e.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train_calories_scale_dummy = pd.get_dummies(X_train_calories_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "X_test_calories_scale_dummy = pd.get_dummies(X_test_calories_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "# Define the MLP Regressor\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', \n",
    "                             max_iter=500, random_state=randomseed)\n",
    "\n",
    "# Train the model on the training data\n",
    "mlp_regressor.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred_mlp = mlp_regressor.predict(X_test_calories_scale_dummy)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_test_mlp = mean_squared_error(y_test_calories, y_test_pred_mlp)\n",
    "r2_test_mlp = r2_score(y_test_calories, y_test_pred_mlp)\n",
    "\n",
    "print(\"MLP Regressor - MSE on test set: \", mse_test_mlp)\n",
    "print(\"MLP Regressor - R2 on test set: \", r2_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir la grille d'hyperparam√®tres\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (150, 100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Configurer le GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPRegressor(max_iter=500, random_state=randomseed),\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Effectuer la recherche sur les donn√©es d'entra√Ænement\n",
    "grid_search.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "\n",
    "# Afficher les meilleurs param√®tres et le score correspondant\n",
    "print(\"Meilleurs param√®tres :\", grid_search.best_params_)\n",
    "print(\"Meilleur score R¬≤ :\", grid_search.best_score_)\n",
    "\n",
    "# √âvaluer le mod√®le optimal sur l'ensemble de test\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_test_pred_best_mlp = best_mlp.predict(X_test_calories_scale_dummy)\n",
    "mse_test_best_mlp = mean_squared_error(y_test_calories, y_test_pred_best_mlp)\n",
    "r2_test_best_mlp = r2_score(y_test_calories, y_test_pred_best_mlp)\n",
    "\n",
    "print(\"MSE sur l'ensemble de test :\", mse_test_best_mlp)\n",
    "print(\"R¬≤ sur l'ensemble de test :\", r2_test_best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Mesurer temps d'entra√Ænement pour le meilleur mod√®le\n",
    "start_time = time.time()\n",
    "best_mlp.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "train_time_mlp = time.time() - start_time\n",
    "\n",
    "print(f\"Temps d'entra√Ænement du meilleur MLP : {train_time_mlp:.2f} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Le meilleur mod√®le de r√©seau de neurones (MLP) a √©t√© entra√Æn√© via **GridSearchCV** avec une architecture de 3 couches cach√©es, utilisant la **fonction d'activation ReLU** et **l'optimiseur Adam**. Il obtient un **R¬≤ g√©n√©ralis√© de 0.9728** et un **R¬≤ de 0.9845** sur l'ensemble de test, avec un **MSE de 1212.49**.\n",
    "\n",
    "Les meilleurs hyperparam√®tres s√©lectionn√©s sont :\n",
    "- Architecture : **(150, 100, 50)** (trois couches cach√©es)\n",
    "- Fonction d'activation : **ReLU**\n",
    "- M√©thode d'optimisation : **Adam**\n",
    "- Apprentissage : **learning rate constant**\n",
    "- R√©gularisation (alpha) : **0.001**\n",
    "\n",
    "En termes de performance pure, le r√©seau de neurones optimis√© se situe juste en-dessous des mod√®les de **Gradient Boosting** (meilleur mod√®le avec R¬≤ g√©n√©ralis√© ‚âà 0.9903) **et XGBoost** (R¬≤ g√©n√©ralis√© ‚âà 0.9824), mais semble l√©g√®rement mieux g√©n√©raliser que le mod√®le XGBoost (bien que ce dernier n'ait pas √©t√© optimis√©) en termes de MSE (1212.49 pour le MPL contre 1377.15 pour XGBoost).\n",
    "\n",
    "Le r√©seau de neurones a su **apprendre efficacement**, bien son **temps d'entra√Ænement soit beaucoup plus important** par rapport aux mod√®les de ce niveau de performances (30 fois plus lent).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpr√©tation finale (comparaison des mod√®les)\n",
    "\n",
    "\n",
    "L'ensemble des mod√®les √©valu√©s pr√©sente des performances tr√®s solides sur la pr√©diction des calories d√©pens√©es :\n",
    "\n",
    "| Mod√®le             | R¬≤ Test  | MSE Test | Temps d'entra√Ænement |\n",
    "|:-------------------|:--------:|:--------:|:--------------------:|\n",
    "| Gradient Boosting   | 0.9903   | 755.82   | ~0.21 sec par fold    |\n",
    "| MLP (r√©seau de neurones) | 0.9845   | 1212.49  | ~6.7 sec (mesur√©)        |\n",
    "| XGBoost             | 0.9824   | 1377.15  | ~0.14 sec par fold    |\n",
    "| Random Forest       | 0.9768   | 1812.48  | 5.2 sec (complet)     |\n",
    "\n",
    "Le **Gradient Boosting** conserve une l√©g√®re avance en termes de pr√©cision et d'erreur quadratique moyenne.  \n",
    "Le **r√©seau de neurones** propose une alternative tr√®s comp√©titive, atteignant un niveau de performance interm√©diaire entre Gradient Boosting et XGBoost.  \n",
    "Le **temps d'entra√Ænement** du MLP reste parfaitement acceptable, comparable √† celui du Gradient Boosting.\n",
    "\n",
    "Enfin, **XGBoost**, bien que l√©g√®rement en retrait sans tuning sp√©cifique, surpasse malgr√© tout la **for√™t al√©atoire** en termes de pr√©cision et de vitesse.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion g√©n√©rale** :\n",
    "\n",
    "> En r√©sum√©, les mod√®les de boosting et de r√©seaux de neurones surpassent les for√™ts al√©atoires en termes de performance et d'efficacit√©.  \n",
    "> Le Gradient Boosting appara√Æt comme le mod√®le le plus performant, tandis que le r√©seau de neurones constitue une alternative comp√©titive et rapide.  \n",
    "> Tous les mod√®les s√©lectionn√©s g√©n√©ralisent correctement, confirmant la qualit√© du jeu de donn√©es et la robustesse des m√©thodes employ√©es.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Comparaison synth√©tique des mod√®les de pr√©diction des calories br√ªl√©es**  \n",
    "Voici une analyse comparative des performances, avantages et limites de chaque m√©thode test√©e :\n",
    "\n",
    "\n",
    "### **Tableau Comparatif Synth√©tique des Mod√®les**\n",
    "\n",
    "| Mod√®le                     | R¬≤ Test    | MSE Test  | Temps d'entra√Ænement (s) | Variables non nulles | Interpr√©tabilit√© | Flexibilit√© (Non-lin√©arit√©) | Commentaire                                                                 |\n",
    "|----------------------------|------------|-----------|--------------------------|----------------------|-------------------|-----------------------------|------------------------------------------------------------------------------|\n",
    "| **Gradient Boosting**       | **0.9903** | **756**   | ~0.21/fold              | N/A                  | Mod√©r√©e          | √âlev√©e                      | Performances √©lev√©es en R¬≤ et MSE, temps rapide.                            |\n",
    "| **Lasso Quadratique**       | 0.993      | 542       | 0.012                   | 18                   | Haute            | Mod√©r√©e (interactions)      | Meilleures performances gr√¢ce aux interactions non lin√©aires.               |\n",
    "| **SVR (noyau RBF)**         | 0.992      | 637       | 0.04                    | N/A                  | Faible           | Tr√®s √©lev√©e                 | Flexible mais peu interpr√©table.                                            |\n",
    "| **XGBoost**                 | 0.9824     | 1,377     | ~0.14/fold              | N/A                  | Mod√©r√©e          | √âlev√©e                      | Rapide et performant pour des donn√©es complexes.                            |\n",
    "| **R√©seau de neurones (MLP)**| 0.9845     | 1,212     | 7.7                     | N/A                  | Faible           | √âlev√©e                      | Complexe, temps d'entra√Ænement √©lev√©.                                       |\n",
    "| **For√™t al√©atoire**         | 0.9768     | 1,812     | 5.2                     | N/A                  | Mod√©r√©e          | Mod√©r√©e                     | √âquilibre entre performance et interpr√©tabilit√©.                            |\n",
    "| **R√©gression Lasso (Œª_min)**| 0.979      | 1,638     | 0.007                   | 12                   | Haute            | Aucune (lin√©aire)           | Performances optimales avec 12 variables.                                   |\n",
    "| **R√©gression Lasso (Œª_1se)**| 0.979      | 1,764.87  | 0.004                   | 5                    | Haute            | Aucune (lin√©aire)           | Simplifi√© (5 variables), id√©al pour l'interpr√©tation.                       |\n",
    "| **R√©gression Ridge**        | 0.9787     | 1,661.23  | 1.67                    | Toutes               | Haute            | Aucune (lin√©aire)           | R√©gularisation L2 l√©g√®rement meilleure que la r√©gression lin√©aire.          |\n",
    "| **Arbre de d√©cision**       | 0.9425     | 4,484     | <1                      | N/A                  | Haute            | Mod√©r√©e                     | Simple et rapide, mais performances limit√©es.                               |\n",
    "\n",
    "---\n",
    "\n",
    "avec\n",
    "\n",
    "1. **Temps d'entra√Ænement** : \n",
    "   - `~0.21/fold` ou `~0.14/fold` : Temps moyen par fold en validation crois√©e.\n",
    "   - Autres valeurs : Temps total en secondes.\n",
    "2. **Variables non nulles** : Applicable uniquement aux mod√®les Lasso/Ridge.\n",
    "3. **Interpr√©tabilit√©** :\n",
    "   - *Haute* : Mod√®les lin√©aires ou structure simple (ex: Lasso, Arbre).\n",
    "   - *Mod√©r√©e* : Mod√®les complexes mais partiellement interpr√©tables (ex: For√™t).\n",
    "   - *Faible* : Mod√®les \"bo√Æte noire\" (ex: SVR, MLP).\n",
    "4. **Flexibilit√©** : Capacit√© √† mod√©liser des relations non lin√©aires.\n",
    "#### **Analyse par m√©thode**  \n",
    "1. **Gradient Boosting**  \n",
    "   - **Avantages** : Meilleure performance globale (R¬≤ ‚âà 0.99, MSE ‚âà 756), rapidit√©, capture de relations non lin√©aires complexes.  \n",
    "   - **Limites** : Interpr√©tabilit√© mod√©r√©e (importance des variables mais pas des interactions pr√©cises).  \n",
    "   - **Cas d‚Äôusage** : Solution par d√©faut pour maximiser la pr√©cision sans contrainte de temps.  \n",
    "\n",
    "2. **Lasso Quadratique (interactions)**  \n",
    "   - **Avantages** : Performance proche du Gradient Boosting (MSE ‚âà 571) avec une **interpr√©tabilit√© √©lev√©e** (coefficients explicites).  \n",
    "   - **Limites** : Flexibilit√© limit√©e aux interactions polynomiales (degr√© 3).  \n",
    "   - **Cas d‚Äôusage** : Mod√®le √©quilibr√© pour expliquer des synergies entre variables (ex : √¢ge √ó BPM).  \n",
    "\n",
    "3. **SVR (noyau RBF)**  \n",
    "   - **Avantages** : Flexibilit√© maximale pour capturer des motifs complexes (R¬≤ ‚âà 0.992).  \n",
    "   - **Limites** : Bo√Æte noire, temps d‚Äôoptimisation long, difficile √† interpr√©ter.  \n",
    "   - **Cas d‚Äôusage** : Donn√©es hautement non lin√©aires o√π l‚Äôinterpr√©tation est secondaire.  \n",
    "\n",
    "4. **XGBoost**  \n",
    "   - **Avantages** : Rapidit√© et performance solide (R¬≤ ‚âà 0.98), r√©gularisation int√©gr√©e.  \n",
    "   - **Limites** : L√©g√®rement moins pr√©cis que le Gradient Boosting standard.  \n",
    "   - **Cas d‚Äôusage** : Grands jeux de donn√©es n√©cessitant rapidit√© et parall√©lisation.  \n",
    "\n",
    "5. **R√©seau de neurones (MLP)**  \n",
    "   - **Avantages** : Performance comp√©titive (R¬≤ ‚âà 0.98), adapt√© aux patterns complexes.  \n",
    "   - **Limites** : Temps d‚Äôentra√Ænement √©lev√©, interpr√©tabilit√© tr√®s faible.  \n",
    "   - **Cas d‚Äôusage** : Alternative aux SVR/boosting si l‚Äôinfrastructure le permet.  \n",
    "\n",
    "6. **For√™t al√©atoire**  \n",
    "   - **Avantages** : Robustesse, interpr√©tabilit√© mod√©r√©e (importance des variables).  \n",
    "   - **Limites** : Performance inf√©rieure aux mod√®les de boosting, temps d‚Äôentra√Ænement long.  \n",
    "   - **Cas d‚Äôusage** : Donn√©es bruyantes, besoin de stabilit√© sans optimisation fine.  \n",
    "\n",
    "7. **Mod√®les lin√©aires (Lasso/Ridge)**  \n",
    "   - **Avantages** : Interpr√©tabilit√© maximale, rapidit√©.  \n",
    "   - **Limites** : Incapables de capturer des non-lin√©arit√©s (MSE > 1,600).  \n",
    "   - **Cas d‚Äôusage** : Analyses exploratoires ou contraintes de simplicit√©.  \n",
    "\n",
    "8. **Arbre de d√©cision**  \n",
    "   - **Avantages** : Interpr√©tabilit√© haute, r√®gles claires.  \n",
    "   - **Limites** : Surapprentissage marqu√© (MSE ‚âà 4,484), performance faible.  \n",
    "   - **Cas d‚Äôusage** : Visualisation p√©dagogique, pas de d√©ploiement en production.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Recommandations finales**  \n",
    "- **Pour la pr√©cision** : **Gradient Boosting** ou **Lasso Quadratique** (selon le besoin d‚Äôinterpr√©tabilit√©).  \n",
    "- **Pour la vitesse** : **XGBoost** ou **Lasso Quadratique**.  \n",
    "- **Pour l‚Äôinterpr√©tabilit√©** : **Lasso Quadratique** (interactions) ou **R√©gression Lasso** (mod√®le lin√©aire).  \n",
    "- **Pour les donn√©es non lin√©aires complexes** : **SVR (RBF)** ou **R√©seau de neurones**.  \n",
    "\n",
    "**Conclusion** : Le choix d√©pend des priorit√©s :  \n",
    "- Le **Gradient Boosting** et le **Lasso Quadratique** se d√©marquent comme les meilleurs compromis performance-interpr√©tabilit√©.  \n",
    "- Les **mod√®les lin√©aires** restent utiles pour des insights rapides, mais sont limit√©s par la nature non lin√©aire des donn√©es.  \n",
    "- Les **arbres (boosting/for√™ts)** et **SVR** sont √† privil√©gier si la flexibilit√© prime sur l‚Äôexplicabilit√©."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
