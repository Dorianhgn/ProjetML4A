{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import time\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "randomseed = 1234\n",
    "\n",
    "## DATA LOADING AND PREPROCESSING\n",
    "# Load the data\n",
    "gym = pd.read_csv('../../gym_members_exercise_tracking.csv')\n",
    "\n",
    "# set 'Gender', 'Workout_Type', 'Workout_Frequency (days/week)' and 'Experience_Level' as categorical\n",
    "for col in ['Gender', 'Workout_Type', 'Workout_Frequency (days/week)', 'Experience_Level']:\n",
    "    gym[col] = gym[col].astype('category')\n",
    "\n",
    "# log transform Weight and BMI\n",
    "gym['Weight (kg)'] = np.log1p(gym['Weight (kg)'])\n",
    "\n",
    "# transform 'Fat_Percentage'\n",
    "max_fat = gym['Fat_Percentage'].max()\n",
    "gym['Fat_Percentage'] = gym['Fat_Percentage'].apply(lambda x: np.sqrt(max_fat+1)-x)\n",
    "\n",
    "# rename transformed columns\n",
    "gym.rename(columns={'Weight (kg)': 'LWeight', 'Fat_Percentage': 'SFat_Percentage'}, inplace=True)\n",
    "\n",
    "gym.drop(columns=['BMI'], inplace=True)\n",
    "\n",
    "# divide into train and test set\n",
    "gym_train, gym_test = train_test_split(gym, test_size=0.2, random_state=randomseed)\n",
    "\n",
    "# Create gym_train_scale, gym_test_scale\n",
    "gym_train_scale = gym_train.copy()\n",
    "gym_test_scale = gym_test.copy()\n",
    "\n",
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "gym_train_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']] = scaler.fit_transform(gym_train_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']])\n",
    "\n",
    "gym_test_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']] = scaler.transform(gym_test_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']])\n",
    "\n",
    "\n",
    "# Create X_train_exp_level, X_test_exp_level, y_train_exp_level, y_test_exp_level\n",
    "X_train_exp_level = gym_train.drop(columns=['Experience_Level'])\n",
    "X_train_exp_level_scale = gym_train_scale.drop(columns=['Experience_Level'])\n",
    "y_train_exp_level = gym_train['Experience_Level']\n",
    "X_test_exp_level = gym_test.drop(columns=['Experience_Level'])\n",
    "X_test_exp_level_scale = gym_test_scale.drop(columns=['Experience_Level'])\n",
    "y_test_exp_level = gym_test['Experience_Level']\n",
    "\n",
    "# Create X_train_calories, X_test_calories, y_train_calories, y_test_calories\n",
    "X_train_calories = gym_train.drop(columns=['Calories_Burned'])\n",
    "X_train_calories_scale = gym_train_scale.drop(columns=['Calories_Burned'])\n",
    "y_train_calories = gym_train['Calories_Burned']\n",
    "X_test_calories = gym_test.drop(columns=['Calories_Burned'])\n",
    "X_test_calories_scale = gym_test_scale.drop(columns=['Calories_Burned'])\n",
    "y_test_calories = gym_test['Calories_Burned']\n",
    "\n",
    "print(\"Data loaded and preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction de Calories Burned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_calories_dummy = pd.get_dummies(X_train_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "X_test_calories_dummy = pd.get_dummies(X_test_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "\n",
    "X_train_calories_dummy1 = pd.get_dummies(X_train_calories, drop_first=True)\n",
    "X_test_calories_dummy1 = pd.get_dummies(X_test_calories, drop_first=True)\n",
    "# Normalisation des données - scaled = Scale + Dummies alors que scale = just scale\n",
    "X_train_calories_scaled = scaler.fit_transform(X_train_calories_dummy1)\n",
    "X_test_calories_scaled = scaler.transform(X_test_calories_dummy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gym_train.head().style.background_gradient(cmap='YlGnBu', low=0, high=0, axis=0))\n",
    "# display unique values of categorical columns\n",
    "display(gym_train.info())\n",
    "for col in gym_train.select_dtypes(include='category').columns:\n",
    "    print(col, gym_train[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gym_train.head().style.background_gradient(cmap='YlGnBu', low=0, high=0, axis=0))\n",
    "# display unique values of categorical columns\n",
    "display(gym_train.info())\n",
    "for col in gym_train.select_dtypes(include='category').columns:\n",
    "    print(col, gym_train[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle Linéaire : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importer les bibliothèques nécessaires\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 2. Créer un modèle de régression linéaire\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "\n",
    "#calcul du temps d'entraînement\n",
    "import time\n",
    "start_time = time.time()\n",
    "# Entraînement du modèle\n",
    "# 3. Entraîner le modèle sur les données d'entraînement\n",
    "linear_model.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entraînement : {end_time - start_time} secondes\")\n",
    "# 4. Faire des prédictions sur l'échantillon de test (X_test_scaled)\n",
    "#y_pred_calories = linear_model.predict(X_test_calories_scaled)\n",
    "y_pred_train_calories = linear_model.predict(X_train_calories_scaled)\n",
    "y_pred_test_calories = linear_model.predict(X_test_calories_scaled)\n",
    "# 5. Évaluer la performance du modèle\n",
    "# Coefficient de détermination R²\n",
    "r2_test = r2_score(y_test_calories, y_pred_test_calories)\n",
    "r2_train = r2_score(y_train_calories, y_pred_train_calories)\n",
    "print(f\"R² test: {r2_test}\")\n",
    "print(f\"R² train: {r2_train}\")\n",
    "\n",
    "# Erreur quadratique moyenne (MSE)\n",
    "mse = mean_squared_error(y_test_calories, y_pred_test_calories)\n",
    "print(f\"MSE: {mse}\")\n",
    "\n",
    "# 6. Afficher les coefficients du modèle\n",
    "print(\"Coefficients du modèle : \", linear_model.coef_)\n",
    "print(\"Intercept du modèle : \", linear_model.intercept_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train_calories, y_pred_train_calories, color='green', alpha=0.5, label='scikit-learn Regression Predictions')\n",
    "plt.plot([y_train_calories.min(), y_pred_train_calories.max()], [y_train_calories.min(), y_pred_train_calories.max()], 'k--', lw=2)\n",
    "plt.scatter(y_test_calories, y_pred_test_calories, color='blue', alpha=0.6)\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], [y_test_calories.min(), y_test_calories.max()], color='red', lw=2)  # Ligne idéale\n",
    "plt.xlabel(\"Calories réelles\")\n",
    "plt.ylabel(\"Calories prédites\")\n",
    "plt.title(\"Prédictions vs Valeurs réelles\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances du modèle\n",
    "\n",
    "R² = 0.978 :\n",
    "Le modèle explique 97.8% de la variance des calories brûlées. Cette valeur exceptionnellement élevée pourrait indiquer un surapprentissage (overfitting), surtout si le modèle a beaucoup de variables (18 coefficients ici).\n",
    "On remarque également que les points verts (entraînement) et bleus (test) semblent bien alignés, ce qui suggère une bonne performance globale du modèle. Toutefois, pour obtenir une analyse complète, il faudrait tracer résidus vs prédictions pour vérifier la répartition uniforme des résidus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calcul des résidus\n",
    "residuals_train = y_train_calories - y_pred_train_calories\n",
    "residuals_test = y_test_calories - y_pred_test_calories\n",
    "\n",
    "# Création d'une seule figure\n",
    "plt.figure(figsize=(8, 6))  # Ajuste la taille selon tes besoins\n",
    "\n",
    "# 1. Résidus vs Valeurs ajustées\n",
    "sns.residplot(x=y_pred_train_calories, y=residuals_train, lowess=True, \n",
    "              line_kws={'color': 'red', 'lw': 1})\n",
    "\n",
    "# Ajout de la ligne horizontale à zéro\n",
    "plt.axhline(0, color='black', linestyle='dotted', alpha=0.6)\n",
    "\n",
    "# Ajout des labels et du titre\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted\")\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forme en banane dans le graphique des résidus (Residuals vs Fitted) révèle une non-linéarité non capturée par le modèle.Ce qui nous indique que la valeur du score R² est trompeuse. En effet, le R² mesure la variance expliquée, pas la justesse des prédictions. Un modèle peut, donc, avoir un R² élevé tout en ayant des erreurs systématiques. Le modèle linéaire est inadéquat pour capturer la vraie relation dans les données, malgré un R² élevé. Ainsi, pour améliorer la généralisation du modèle et identifier les variables réellement influentes, une approche de régularisation s’impose. C’est ici que la régression Lasso (Least Absolute Shrinkage and Selection Operator) entre en jeu. \n",
    "\n",
    "Donc, maintenant, nous allons passer à l’implémentation de Lasso pour voir comment il améliore (ou non) la robustesse du modèle, malgré les limites structurelles de la linéarité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord avec un lambda quelconque puis avec un lambda choisi par validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso\n",
    "lasso1= Lasso(alpha=10)\n",
    "# calcul du temps d'entraînement\n",
    "\n",
    "start_time = time.time()\n",
    "# Entraînement du modèle\n",
    "lasso1.fit(X_train_calories_scaled, y_train_calories)\n",
    "train_score_lasso1=lasso1.score(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entraînement : {end_time - start_time} secondes\")\n",
    "test_score_lasso1=lasso1.score(X_test_calories_scaled, y_test_calories)\n",
    "\n",
    "print(\"The train score for ls model is {}\".format(train_score_lasso1))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso1))\n",
    "# print the lasso coefficient with the name of the variable next to it\n",
    "\n",
    "\n",
    "coef_calories_lasso1 = pd.Series(lasso1.coef_, index=X_train_calories_dummy1.columns)\n",
    "coef_calories_lasso1.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du modèle Lasso pour Calories Burned')\n",
    "plt.show()\n",
    "# Afficher le nombre de variables conservées et éliminées\n",
    "print(f\"Lasso conserve {sum(coef_calories_lasso1 != 0)} variables et en supprime {sum(coef_calories_lasso1 == 0)}\")\n",
    "# print the coefficients of lasso1\n",
    "#print(\"Coefficients du modèle Lasso : \", coef_calories_lasso1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Appliquer Lasso avec validation croisée pour trouver le meilleur alpha\n",
    "#lasso = LassoCV(cv=5, random_state=1234, max_iter=10000)  # 5-fold cross-validation\n",
    "start_time = time.time()\n",
    "lasso = LassoCV(cv=5, alphas=np.array(range(1, 50, 1)) / 20., n_jobs=-1, random_state=13).fit(X_train_calories_scaled, y_train_calories)\n",
    "lasso.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entraînement : {end_time - start_time} secondes\")\n",
    "\n",
    "# Coefficient optimal alpha sélectionné par LassoCV\n",
    "optimal_alpha = lasso.alpha_\n",
    "print(f\"Optimal alpha: {optimal_alpha}\")\n",
    "\n",
    "# Coefficients du modèle Lasso\n",
    "coef_calories_lasso = pd.Series(lasso.coef_, index=X_train_calories_dummy1.columns)\n",
    "\n",
    "# Afficher les coefficients du modèle Lasso\n",
    "print(\"Coefficients du modèle Lasso pour Calories Burned:\")\n",
    "print(coef_calories_lasso)\n",
    "\n",
    "# Afficher le nombre de variables conservées et éliminées\n",
    "print(f\"Lasso conserve {sum(coef_calories_lasso != 0)} variables et en supprime {sum(coef_calories_lasso == 0)}\")\n",
    "\n",
    "# Tracer les coefficients\n",
    "coef_calories_lasso.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du modèle Lasso pour Calories Burned')\n",
    "plt.show()\n",
    "\n",
    "# Prédictions avec le modèle Lasso\n",
    "y_pred_lasso = lasso.predict(X_test_calories_scaled)\n",
    "\n",
    "# Calcul de l'erreur quadratique moyenne pour évaluer les performances du modèle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_lasso_test = mean_squared_error(y_test_calories, y_pred_lasso)\n",
    "\n",
    "print(f\"Test Mean Squared Error (MSE) pour Lasso pour l'échantillon de test: {mse_lasso_test}\")\n",
    "\n",
    "train_score_lasso= lasso.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_lasso= lasso.score(X_test_calories_scaled, y_test_calories)\n",
    "print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances du modèle\n",
    "\n",
    "- On obtient un MSE = 1638.14. On a donc une légère amélioration par rapport au modèle linéaire non régularisé (MSE=1679.54). Cependant, cette différence minime suggère que la régularisation Lasso réduit légèrement le surapprentissage.Toutefois, Le problème fondamental de non-linéarité (forme en banane des résidus) persiste, limitant les gains de performance. Ceci est visible dans le graphe des résidus ci-dessous, où l'on observe un profil en banane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracé des résidus\n",
    "residuals_lasso = y_test_calories - y_pred_lasso\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, residuals_lasso, color='blue', alpha=0.6)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Valeurs réelles\")\n",
    "plt.ylabel(\"Résidus\")\n",
    "plt.title(\"Résidus du modèle Lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- On a un alpha optimal = 0.8 :Une pénalité L1 relativement forte, ce qui explique pourquoi 11 variables sur 18 ont été éliminées (coefficients à zéro).\n",
    "\n",
    "### Interpretation des résultats: \n",
    "\n",
    "#### Relation Session_Duration - Calories Burned\n",
    "- On remarque, d'après le graphe, que la variable Session_Duration domine clairement, c'est à dire qu'une augmentation d’1 heure de la durée de la séance entraîne une augmentation prédite de 243 calories brûlée. Donc, plus la séance est longue, plus le corps puise dans ses réserves énergétiques (glycogène et lipides).\n",
    "\n",
    "Les activités prolongées (ex : cardio, endurance) sollicitent le métabolisme aérobie, favorisant une dépense calorique cumulative.\n",
    "\n",
    "- Remarque: Ce coefficient élevé pourrait aussi refléter une corrélation indirecte (ex : les séances longues incluent souvent des exercices intenses).\n",
    "\n",
    "#### Différence homme femme \n",
    "-  Les hommes brûlent 40.9 calories de plus que les femmes à caractéristiques égales.\n",
    "    Ceci pourrait être dû au fait que les hommes ont généralement une masse musculaire plus élevée, qui consomme plus de calories au repos et à l’effort.Les différences hormonales (testostérone) favorisent un métabolisme énergétique plus actif.\n",
    "\n",
    "- Remarque: Ce coefficient pourrait aussi refléter des biais comportementaux (ex : les hommes choisissent des entraînements plus intenses non mesurés dans les données)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution du MSE en fonction de lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "model = LassoCV(cv=5, alphas=np.array(range(1,200,1))/10.,n_jobs=-1,random_state=13).fit(X_train_calories_scaled, y_train_calories)\n",
    "m_log_alphas = np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "# ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='MSE moyen', linewidth=2)\n",
    "plt.axvline(np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: optimal par VC')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('log(alpha)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE de chaque validation: coordinate descent ')\n",
    "plt.show()\n",
    "#le courbe noire correspond à la moyennes des 5 autres\n",
    "# on decoupe en 5 échantillons d'apprentissage d'ou les 5 courbes \n",
    "# Plot the coefficients as a function of -log(alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque une zone où la MSE est relativement basse et stable autour d’un certain intervalle de alpha. Puis, quand alpha devient trop grand (régularisation trop forte), la MSE monte en flèche (le modèle est trop contraint, sous-apprentissage).\n",
    "\n",
    "À l’opposé, quand alpha est trop petit, la régularisation est quasi nulle : on risque un sur-apprentissage (même si, parfois, la MSE peut rester relativement stable dans cette zone si le dataset n’est pas trop bruyant).\n",
    "\n",
    "Le point choisi par la validation croisée est un compromis : il vise à réduire le nombre de coefficients non nuls (pour la parcimonie) tout en conservant une bonne performance (basse MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# Calculer le chemin du Lasso\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_train_calories_scaled, y_train_calories, alphas=np.array(range(1, 400, 1)))\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "# Styles pour les lignes\n",
    "styles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "# Log des alphas\n",
    "log_alphas_lasso = np.log10(alphas_lasso)\n",
    "\n",
    "# Tracer les coefficients\n",
    "for coef_l, s in zip(coefs_lasso, styles):\n",
    "    plt.plot(log_alphas_lasso, coef_l, linestyle=s, c='b')\n",
    "\n",
    "# Ajouter une ligne verticale pour l'alpha optimal\n",
    "plt.axvline(optimal_alpha, color='red', linestyle='--', label=f'Optimal alpha: {optimal_alpha}')\n",
    "\n",
    "# Ajouter des labels et une légende\n",
    "plt.xlabel('Log(alpha)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.legend()\n",
    "plt.title('Lasso Path with Optimal Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique illustre le mécanisme de régularisation L1 propre à la régression Lasso : lorsque le paramètre de régularisation *alpha* augmente, la contrainte de parcimonie s'intensifie, conduisant progressivement les coefficients les moins informatifs vers zéro. Ce comportement est intrinsèque à l'algorithme, qui privilégie un **modèle simplifié** (moins de variables) au détriment d'une légère dégradation de la précision. En d'autres termes, un *alpha* élevé renforce la pénalisation des coefficients, favorisant ainsi un **équilibre optimal entre simplicité interprétative et généralisation**, au prix d'un biais accru. Cela traduit directement le compromis biais-variance au cœur de l'optimisation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes et std des scores pour chaque alpha\n",
    "mse_path = model.mse_path_.mean(axis=1)\n",
    "stds = model.mse_path_.std(axis=1)\n",
    "alphas = model.alphas_\n",
    "\n",
    "# Index de l'erreur minimale\n",
    "min_idx = np.argmin(mse_path)\n",
    "\n",
    "# lambda_min\n",
    "alpha_min = alphas[min_idx]\n",
    "\n",
    "# lambda_1se = plus grand alpha avec erreur ≤ (erreur min + 1 std)\n",
    "threshold = mse_path[min_idx] + stds[min_idx]\n",
    "alpha_1se = max(alphas[mse_path <= threshold])\n",
    "\n",
    "# Refit pour alpha_min\n",
    "lasso_min = Lasso(alpha=alpha_min, max_iter=10000)\n",
    "start_time = time.time()\n",
    "lasso_min.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entraînement pour alpha_min : {end_time - start_time} secondes\")\n",
    "\n",
    "non_zero_min = (lasso_min.coef_ != 0).sum()\n",
    "r2_min = lasso_min.score(X_test_calories_scaled, y_test_calories)\n",
    "# Refit pour alpha_1se\n",
    "lasso_1se = Lasso(alpha=alpha_1se, max_iter=10000)\n",
    "start_time = time.time()\n",
    "lasso_1se.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entraînement pour alpha_1se : {end_time - start_time} secondes\")\n",
    "non_zero_1se = (lasso_1se.coef_ != 0).sum()\n",
    "r2_1se = lasso_1se.score(X_test_calories_scaled, y_test_calories)\n",
    "# Affichage des résultats\n",
    "print(f\"alpha_min (λ_min) = {alpha_min:.6f}\")\n",
    "print(f\"  -> MSE moyen : {mse_path[min_idx]:.6f}\")\n",
    "print(f\"  -> Écart-type : {stds[min_idx]:.6f}\")\n",
    "print(f\"  -> Nb variables non nulles : {non_zero_min}\")\n",
    "print(f\"  -> R² : {r2_min:.6f}\")\n",
    "# Trouver l'indice correspondant à alpha_1se\n",
    "idx_1se = list(alphas).index(alpha_1se)\n",
    "\n",
    "print(f\"\\nalpha_1se (λ_1se) = {alpha_1se:.6f}\")\n",
    "print(f\"  -> MSE moyen : {mse_path[idx_1se]:.6f}\")\n",
    "print(f\"  -> Écart-type : {stds[idx_1se]:.6f}\")\n",
    "print(f\"  -> Nb variables non nulles : {non_zero_1se}\")\n",
    "print(f\"  -> R² : {r2_1se:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons \n",
    "\n",
    "a. Alpha_min (λ_min = 0.8)\n",
    "- Pour λ_min = 0.8 et MSE moyen = 1629.17 :\n",
    "C'est l'erreur minimale moyenne obtenue par validation croisée.\n",
    "C'est la valeur de  λ qui donne les meilleures performances prédictives (modèle le plus précis).\n",
    "\n",
    "- Écart-type = 139.26 :\n",
    "Indique la variabilité des erreurs entre les folds de validation croisée.\n",
    "Une valeur élevée suggère que le modèle est instable (sensible aux variations des données d'entraînement).\n",
    "\n",
    "- 12 variables non nulles :\n",
    "Le modèle garde 12 coefficients non nuls → modèle complexe mais précis.\n",
    "\n",
    "b. Alpha_1se (λ_1se = 5.7)\n",
    "- λ_1se = 5.7 et MSE moyen = 1764.87 (+8.3% vs λ_min) :\n",
    "L'erreur est dans l'intervalle [λ_min - 1SE, λ_min + 1SE] → considérée comme statistiquement équivalente à l'erreur minimale.\n",
    "\n",
    "- Écart-type = 241.99 :\n",
    "Variabilité accrue → le modèle simplifié est plus sensible aux fluctuations des données.\n",
    "\n",
    "- 5 variables non nulles :\n",
    "Le modèle élimine 7 variables → modèle interprétable mais potentiellement moins précis.\n",
    "\n",
    "Ces valeurs sont logiques vu que Alpha_1se represente la plus grande valeur de lambda dont l’erreur est à moins d’un écart-type de l’erreur minimale(favorise un modèle plus simple) et que Alpha_min minimise l’erreur de validation croisée (MSE) (donne le meilleur ajustement possible sur les données de validation). Toutefois, nous préférons généralement afficher ces coefficients en R et pas en python. Ceci est dû à l'absence de λ_1se natif en python\n",
    "(Scikit-learn ne calcule pas automatiquement λ_1se, contrairement à glmnet en R)\n",
    "→ Calcul manuel sujet à des erreurs (ex: gestion des intervalles de confiance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle quadratique et ordre élevé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipeline pour le Lasso avec les interactions\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(interaction_only=True, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),  # Good practice before Lasso\n",
    "    ('lasso', Lasso(max_iter=10000))\n",
    "])\n",
    "\n",
    "scoring = {\n",
    "    'r2': 'r2',\n",
    "    'neg_mse': 'neg_mean_squared_error'\n",
    "}\n",
    "\n",
    "# grille de paramètres pour le Lasso\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3],      # Tune the interaction degree\n",
    "    'lasso__alpha': np.logspace(-2, 1, 10)  # Tune the Lasso strength\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='r2',  # On choisit sur quelle métrique choisir le best_estimator_\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    n_jobs=6 # run en parallèle\n",
    ")\n",
    "\n",
    "grid.fit(X_train_calories_scaled, y_train_calories)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul du temps d'entrainement pour une configuration spécifique\n",
    "\n",
    "# Création du pipeline avec les hyperparamètres spécifiques\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(interaction_only=True, include_bias=False, degree=2)), # Degré fixé à 2\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso', Lasso(alpha=0.1, max_iter=10000)) # Alpha fixé à 0.1\n",
    "])\n",
    "\n",
    "# Mesure du temps pour UNE configuration\n",
    "start_time = time.time()\n",
    "pipeline.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Temps d'entraînement pour degree=2 et alpha=0.1 : {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "filtered_row = results[results['params'] == {'lasso__alpha': 1.0, 'poly__degree': 3}]\n",
    "filtered_row[['mean_test_neg_mse']]\n",
    "print(\"Best model R² (Cross Validation):\", grid.best_score_)\n",
    "print(\"Best model MSE (Cross Validation):\", -filtered_row['mean_test_neg_mse'].values[0] , \"\\n\")\n",
    "\n",
    "print(\"Best model test R²:\", grid.score(X_test_calories_scaled, y_test_calories))\n",
    "print(\"Best model test MSE:\", mean_squared_error(y_test_calories, best_model.predict(X_test_calories_scaled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Récupérer le PolynomialFeatures entraîné\n",
    "poly = best_model.named_steps['poly']\n",
    "\n",
    "# 2. Récupérer le modèle Lasso entraîné\n",
    "lasso = best_model.named_steps['lasso']\n",
    "\n",
    "# 3. Construire les noms des features\n",
    "feature_names = poly.get_feature_names_out(input_features=X_train_calories_dummy1.columns)\n",
    "\n",
    "# 4. Associer chaque feature à son coefficient\n",
    "coefs = pd.Series(lasso.coef_, index=feature_names)\n",
    "\n",
    "# 5. Afficher ou trier les coefficients\n",
    "pd.set_option('display.max_rows', None)\n",
    "coefs = coefs.sort_values()\n",
    "print(coefs)\n",
    "coefs = coefs[coefs != 0]  # Garder uniquement les coefficients non nuls\n",
    "\n",
    "# 6. Plot\n",
    "coefs.plot(kind='barh', figsize=(10, 12))\n",
    "plt.title('Coefficients Lasso avec interactions')\n",
    "plt.show()\n",
    "#plot the residuals for the lasso model\n",
    "y_fitted_lasso = best_model.predict(X_train_calories_scaled)\n",
    "y_fitted_lasso_test= best_model.predict(X_test_calories_scaled)\n",
    "print(y_fitted_lasso.shape)\n",
    "print(y_test_calories.shape)\n",
    "residuals_lasso = y_train_calories  - y_fitted_lasso\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=y_fitted_lasso, y=residuals_lasso, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Valeurs réelles')\n",
    "plt.ylabel('Résidus')\n",
    "plt.title('Résidus du modèle Lasso avec interactions')\n",
    "plt.show()\n",
    "\n",
    "#print(\"Best model test R²:\", grid.score(X_test_calories_scaled, y_test_calories))\n",
    "mse_lasso_quadratic_test = mean_squared_error(y_test_calories, y_fitted_lasso_test)\n",
    "\n",
    "print(f\"Test Mean Squared Error (MSE) pour Lasso pour l'échantillon de test: {mse_lasso_quadratic_test}\")\n",
    "#compute the score for the lasso model from the previous\n",
    "\n",
    "train_score_lasso= best_model.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_lasso= best_model.score(X_test_calories_scaled, y_test_calories)\n",
    "#print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\n",
    "\n",
    "\n",
    "#print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\"\"\"\n",
    "residuals_train = y_train_calories - y_pred_train_calories\n",
    "residuals_test = y_test_calories - y_pred_test_calories\n",
    "\n",
    "# Création d'une seule figure\n",
    "plt.figure(figsize=(8, 6))  # Ajuste la taille selon tes besoins\n",
    "y_pred_calories = linear_model.predict(X_test_calories_scaled)\n",
    "y_pred_train_calories = linear_model.predict(X_train_calories_scaled)\n",
    "y_pred_test_calories = linear_model.predict(X_test_calories_scaled)\n",
    "# 1. Résidus vs Valeurs ajustées\n",
    "sns.residplot(x=y_pred_train_calories, y=residuals_train, lowess=True, \n",
    "              line_kws={'color': 'red', 'lw': 1})\n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔎 **Interprétation du Modèle Lasso avec Interactions (Polynomial + Lasso)**\n",
    "\n",
    "Ce modèle repose sur un encodage polynomial avec interactions uniquement (`interaction_only=True`, degré 3), suivi d’une régularisation L1 (`Lasso`). Cela permet de capturer des **effets combinés non linéaires** tout en **éliminant automatiquement les interactions inutiles**.\n",
    "\n",
    "#####  Performances\n",
    "- **R² test** = **0.993**, **MSE test** ≈ **542**\n",
    "- Gain substantiel par rapport au Lasso simple (R² ≈ 0.979, MSE ≈ 1638)\n",
    "- Ce modèle capture donc beaucoup mieux la complexité des relations entre variables.\n",
    "\n",
    "#### Temps de Calcul (1 minute)\n",
    "Complexité justifiée : Bien que plus lent qu’un modèle linéaire (quelques secondes), le gain en performance valide l’utilisation d’un modèle quadratique.\n",
    "\n",
    "Optimisation : Le Lasso réduit la complexité en éliminant les termes non pertinents, équilibrant précision et parcimonie.\n",
    "#####  Interprétation des principales interactions retenues\n",
    "\n",
    "Les **coefficients positifs** indiquent des interactions qui **augmentent** la prédiction de `Calories_Burned`, et les **négatifs** celles qui la **diminuent** :\n",
    "\n",
    "---\n",
    "\n",
    "##### Quelques interactions dominantes positives :\n",
    "\n",
    "- **`Avg_BPM × Session_Duration`** → **+21.44**\n",
    "  > Synergie intensité/durée : les longues séances à haut BPM amplifient la dépense calorique (effet non-linéaire critique).\n",
    "\n",
    "- **`Session_Duration (hours) Gender_Male`** → **+11.42**\n",
    "  > Les hommes tirent un bénéfice calorique supplémentaire des sessions longues, possiblement grâce à une endurance musculaire supérieure.\n",
    "\n",
    "##### Quelques interactions dominantes négatives :\n",
    "\n",
    "- **`Age × Session_Duration`** → **−10.6**\n",
    "  > À durée d'entrainement équivalents, l'âge **réduit fortement** la dépense calorique. Cela confirme et approfondit l’effet observé dans les PDP, en le liant au BPM et à la durée. Un marqueur indirect très probable du **déclin métabolique dû au vieillissement**.\n",
    "\n",
    "- **`Age × Avg_BPM`** → **−2.35**\n",
    "  >  À fréquence cardiaque équivalente, les seniors brûlent moins, possiblement dû à une VO₂ max (débit maximum d'oxygène) réduite.\n",
    "\n",
    "\n",
    "#### Conclusion \n",
    "\n",
    "> *Le modèle polynomial régularisé par Lasso améliore significativement la prédiction (R² ≈ 0.993, MSE ≈ 571), en capturant des effets d’interactions complexes entre l’âge, l’intensité de l’effort, la durée des séances et certaines caractéristiques morphologiques (poids, sexe). Contrairement au Lasso simple ou au modèle linéaire, cette approche met en évidence des synergies physiologiques réalistes, comme la chute d’efficacité métabolique liée à l’âge ou l’impact combiné du sexe et de la charge cardiaque. Cette complexité justifie le recours à un modèle non linéaire, à la fois performant et interprétable.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous étudierons brièvement l'effet d'une pénalisation plus stricte sur le modèle via Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1) Instanciation sans n_jobs ni random_state\n",
    "start_time = time.time()\n",
    "ridgereg = RidgeCV(alphas=np.arange(1, 50) / 20., cv=5)\n",
    "\n",
    "# 2) Entraînement\n",
    "\n",
    "ridgereg.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entraînement : {end_time - start_time} secondes\")\n",
    "# 3) Alpha optimal\n",
    "optimal_alpha = ridgereg.alpha_\n",
    "print(f\"Optimal alpha: {optimal_alpha}\")\n",
    "\n",
    "# 4) Coefficients\n",
    "coef_calories_ridge = pd.Series(ridgereg.coef_, index=X_train_calories_dummy1.columns)\n",
    "print(\"Coefficients du modèle Ridge pour Calories Burned:\")\n",
    "print(coef_calories_ridge)\n",
    "\n",
    "# 5) Comme Ridge ne met quasiment jamais un coefficient strictement à 0, \n",
    "#    le comptage « conservé / supprimé » n’est pas très significatif, mais :\n",
    "print(f\"Nombre de coefficients non nuls : {sum(coef_calories_ridge != 0)}\")\n",
    "\n",
    "# 6) Tracé\n",
    "coef_calories_ridge.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du modèle Ridge pour Calories Burned')\n",
    "plt.show()\n",
    "\n",
    "# 7) Prédiction et MSE\n",
    "y_pred_ridge = ridgereg.predict(X_test_calories_scaled)\n",
    "mse_ridge = mean_squared_error(y_test_calories, y_pred_ridge)\n",
    "print(f\"MSE pour Ridge : {mse_ridge:.4f}\")\n",
    "\n",
    "# 8) R² (score) entraînement et test\n",
    "train_score_ridge = ridgereg.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_ridge  = ridgereg.score(X_test_calories_scaled,  y_test_calories)\n",
    "print(f\"Train R² pour Ridge : {train_score_ridge:.4f}\")\n",
    "print(f\"Test  R² pour Ridge : {test_score_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle Ridge obtient un MSE de 1 661,23, un R² entraînement de 0,9791 et un R² test de 0,9787. Ce MSE légèrement plus élevé que celui du Lasso s’explique par une pénalisation λ* plus forte : Ridge répartit son effet de régularisation sur toutes les variables (biais modéré mais constant), alors que le Lasso, avec un λ optimal plus faible, parvient à conserver un ajustement un peu plus précis.\n",
    "\n",
    "Cependant, les performances des deux modèles linéaires restent très proches :\n",
    "\n",
    "Lasso (λ_min) : MSE test ≃ 1 638,14, R² test ≃ 0,9790\n",
    "\n",
    "Ridge : MSE test ≃ 1 661,23, R² test ≃ 0,9787\n",
    "\n",
    "Enfin, le Lasso quadratique (avec interactions) surpasse nettement ces deux approches linéaires, avec un MSE test ≃ 570,61 et un R² test ≃ 0,9927, grâce à sa capacité à capturer des relations non linéaires entre les variables mais reste un peu plus lent niveau temps d'entrainement \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir analysé les performances du modèle Lasso et de Ridge et identifié l'alpha optimal pour régulariser notre régression, nous allons maintenant explorer une approche alternative en utilisant la régression par vecteurs de support (SVR) afin de comparer ses performances et sa capacité à capturer des relations potentiellement non linéaires dans les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR SUR CALORIES BURNED : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#calibrage des paramètres c et gamma\n",
    "\n",
    "param = [{'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']}]\n",
    "param_lin_opt= GridSearchCV(SVR(),param,refit=True,verbose=3)\n",
    "start_time = time.time()\n",
    "param_lin_opt.fit(X_train_calories_scaled,y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entraînement : {end_time - start_time} secondes\")\n",
    "print(param_lin_opt.best_params_)\n",
    "start_time = time.time()\n",
    "y_pred_svr_lin = param_lin_opt.predict(X_test_calories_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"Temps de prédiction : {end_time - start_time} secondes\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_lin, color='darkorange', label='Prédictions vs Réelles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite prédiction')\n",
    "plt.xlabel('Valeurs Réelles (Calories_Burned)')\n",
    "plt.ylabel('Prédictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR Linéaire - Comparaison Prédictions/Réelles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_score_lin= r2_score(y_test_calories,y_pred_svr_lin)\n",
    "print(f\"R² pour SVR lin: {R2_score_lin}\")\n",
    "mse_svr_lin = mean_squared_error(y_test_calories, y_pred_svr_lin)\n",
    "print(f\"MSE pour SVR poly: {mse_svr_lin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rbf=[{'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}]\n",
    "parmopt_rbf = GridSearchCV(SVR(), param_rbf, refit = True, verbose = 3)\n",
    "parmopt_rbf.fit(X_train_calories_scaled, y_train_calories)\n",
    "print(parmopt_rbf.best_params_)\n",
    "start_time = time.time()\n",
    "y_pred_svr_rbf = parmopt_rbf.predict(X_test_calories_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"Temps de prédiction : {end_time - start_time} secondes\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_rbf, color='darkorange', label='Prédictions vs Réelles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite prédiction')\n",
    "plt.xlabel('Valeurs Réelles (Calories_Burned)')\n",
    "plt.ylabel('Prédictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR rbf - Comparaison Prédictions/Réelles')\n",
    "plt.show()\n",
    "\n",
    "R2_score_rbf= r2_score(y_test_calories,y_pred_svr_rbf)\n",
    "print(f\"R² pour SVR rbf: {R2_score_rbf}\")\n",
    "mse_svr_rbf = mean_squared_error(y_test_calories, y_pred_svr_rbf)\n",
    "print(f\"MSE pour SVR rbf: {mse_svr_rbf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_poly=[{'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['poly']}]\n",
    "parmopt_poly = GridSearchCV(SVR(), param_poly, refit = True, verbose = 3)\n",
    "time_start = time.time()\n",
    "parmopt_poly.fit(X_train_calories_scaled, y_train_calories)\n",
    "time_end = time.time()\n",
    "print(f\"Temps d'entraînement : {time_end - time_start} secondes\")\n",
    "print(parmopt_poly.best_params_)\n",
    "\n",
    "y_pred_svr_poly = parmopt_poly.predict(X_test_calories_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_poly, color='darkorange', label='Prédictions vs Réelles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite prédiction')\n",
    "plt.xlabel('Valeurs Réelles (Calories_Burned)')\n",
    "plt.ylabel('Prédictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR poly - Comparaison Prédictions/Réelles')\n",
    "plt.show()\n",
    "\n",
    "R2_score_poly= r2_score(y_test_calories,y_pred_svr_poly)\n",
    "print(f\"R² pour SVR poly: {R2_score_poly}\")\n",
    "mse_svr_poly = mean_squared_error(y_test_calories, y_pred_svr_poly)\n",
    "print(f\"MSE pour SVR poly: {mse_svr_poly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Performances des différents noyaux SVR**\n",
    "\n",
    "a. SVR avec noyau RBF (Radial Basis Function)\n",
    "R² = 0,992 et MSE = 636,57\n",
    "\n",
    "=> Le modèle RBF parvient à expliquer 99,2 % de la variance des calories brûlées, avec une erreur quadratique moyenne extrêmement basse.\n",
    "Ceci est dû au fait que le noyau RBF est capable de capturer des relations non linéaires complexes (par exemple, l’interaction entre la durée de séance et la fréquence cardiaque moyenne). Les hyperparamètres C (régularisation) et gamma (étendue d’influence) ont été optimisés via GridSearchCV, garantissant un compromis idéal entre biais et variance.\n",
    "\n",
    "b. SVR avec noyau linéaire\n",
    "R² = 0,977 et MSE = 1 790,89\n",
    "\n",
    "=> Le SVR linéaire offre également de bonnes performances , mais nettement inférieures au noyau RBF (erreur MSE beaucoup plus élevée).\n",
    "Ceci pourrait être dû au fait que\n",
    "\n",
    "c. SVR avec noyau polynomial\n",
    "R² = 0,949 et MSE = 3 952,21\n",
    "\n",
    "=> Les résultats sont bien plus faibles, avec une erreur environ 6 fois supérieure à celle du RBF.\n",
    "\n",
    "\n",
    "2. **Comparaison des Performances : Lasso Quadratique vs SVR RBF**\n",
    "\n",
    "| Critère               | Lasso Quadratique (Interactions) | SVR RBF              |\n",
    "|-----------------------|-----------------------------------|----------------------|\n",
    "| **MSE (Test)**        | **570.61**                        | 636.57              |\n",
    "| **R² (Test)**         | **0.9927**                        | 0.992               |\n",
    "| **Complexité**        | Modèle linéaire avec interactions | Modèle non linéaire |\n",
    "| **Interprétabilité**  | Coefficients explicables          | \"Boîte noire\"       |\n",
    "| **Flexibilité**       | Capte interactions spécifiques    | Adapté aux relations complexes/génériques |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Points Clés :**\n",
    "1. **Performance Prédictive** :  \n",
    "   - Le **Lasso Quadratique** est légèrement meilleur en MSE (+10% d'erreur pour SVR RBF).  \n",
    "   - Les deux modèles ont un R² quasi identique (> 0.99), indiquant une explication quasi parfaite de la variance.\n",
    "\n",
    "2. **Equilibre Complexité/Interprétabilité** :  \n",
    "   - **Lasso Quadratique** : Moins flexible mais interprétable (coefficients des interactions analysables).  \n",
    "   - **SVR RBF** : Plus flexible mais difficile à expliquer (dépend de la fonction noyau).\n",
    "\n",
    "3. **Choix du modèle** :  \n",
    "   - **Lasso Quadratique** : Si l’on privilégie l’erreur quadratique minimale et la parcimonie  \n",
    "   - **SVR RBF** : Si l’on recherche avant tout la flexibilité pour capter des structures non-linéaires plus subtiles\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbres et Forest aléatoires\n",
    "### Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_calories_dummy = pd.get_dummies(X_train_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "X_test_calories_dummy = pd.get_dummies(X_test_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "# Normalisation des données - scaled = Scale + Dummies alors que scale = just scale\n",
    "X_train_calories_scaled = scaler.fit_transform(X_train_calories_dummy)\n",
    "X_test_calories_scaled = scaler.transform(X_test_calories_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit a regression tree model for Calories_Burned using dummy variables\n",
    "tree_reg_cal = DecisionTreeRegressor(random_state=randomseed, ccp_alpha=0.001)\n",
    "start_time = time.time()\n",
    "tree_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entraînement : {end_time - start_time} secondes\")\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(18, 12))\n",
    "plot_tree(tree_reg_cal, feature_names=X_train_calories_dummy.columns, filled=True, rounded=True, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Compute MSE and R2 on training and test sets\n",
    "y_train_pred = tree_reg_cal.predict(X_train_calories_dummy)\n",
    "y_test_pred = tree_reg_cal.predict(X_test_calories_dummy)\n",
    "\n",
    "mse_train = mean_squared_error(y_train_calories, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test_calories, y_test_pred)\n",
    "r2_train = r2_score(y_train_calories, y_train_pred)\n",
    "r2_test = r2_score(y_test_calories, y_test_pred)\n",
    "\n",
    "print(\"MSE on training set: \", mse_train)\n",
    "print(\"MSE on test set: \", mse_test)\n",
    "print(\"R2 on training set: \", r2_train)\n",
    "print(\"R2 on test set: \", r2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : Nous avons initialement construit un arbre de régression avec un paramètre de complexité extrêmement faible (`cp = 0.01`). Comme attendu, ce modèle présente une structure profondément ramifiée, caractéristique d'un sur-apprentissage. Ce modèle présente queasiment aucun biais sur le jeu d’entraînement (R² = 0.999, MSE = 0.027), mais un écart significatif entre l’erreur d’entraînement et de test (MSE_test = 4484) révèle un sur-apprentissage. Toutefois, le R² sur le test reste élevé (0.934), indiquant que le modèle capture une part substantielle de la variance explicative, malgré sa complexité excessive. Le modèle d'arbre en Python est plus complexe qu'en R alors que nous utilisons un cp plus élevé (`cp=0.01`), tandis que R utilise un `cp=0.001`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for best cp \n",
    "\n",
    "scoring = {\n",
    "    'r2': 'r2',\n",
    "    'neg_mse': 'neg_mean_squared_error'\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'ccp_alpha': np.logspace(-4, 2, 15)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(tree_reg_cal, params, scoring=scoring, cv=5, refit='r2', n_jobs=-1)\n",
    "grid.fit(X_train_calories_dummy, y_train_calories)\n",
    "grid_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# plot the results as a function of ccp_alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(grid_results['param_ccp_alpha'], grid_results['mean_test_neg_mse'] * -1, label='MSE', marker='o')\n",
    "plt.xlabel('Complexity Parameter (alpha)')\n",
    "plt.ylabel('Cross-Validation Error')\n",
    "plt.title('Cross-Validation Error vs Complexity Parameter')\n",
    "plt.grid(True, which=\"both\", ls=\"-\")\n",
    "\n",
    "optimal_alpha = np.argmin(grid_results['mean_test_neg_mse'] * -1)\n",
    "plt.axvline(grid_results['param_ccp_alpha'][optimal_alpha], color='red', linestyle='--', label='Optimal alpha')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(18, 12))\n",
    "tree_reg_cal_optimal = grid.best_estimator_\n",
    "plot_tree(tree_reg_cal_optimal, feature_names=X_train_calories_dummy.columns, filled=True, rounded=True, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# display the dataframe with top 5 results from mean_test_neg_mse\n",
    "display(grid_results[['param_ccp_alpha', 'mean_test_neg_mse', 'std_test_neg_mse', 'rank_test_neg_mse']].sort_values(by='mean_test_neg_mse', ascending=False).head(5))\n",
    "# same for r2\n",
    "\n",
    "display(grid_results[['param_ccp_alpha', 'mean_test_r2', 'std_test_r2', 'rank_test_r2']].sort_values(by='mean_test_r2', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : Par validation croisée, nous avons déterminé que le meilleur paramètre d'élagage est `ccp ≈ 5.18`. L'arbre de regression résultant est moins complexe que le précédent, mais est encore trop ramifié, comme celui de R. Ce modèle est un peu moins performant que celui de R, avec un MSE calculé par cross-validation 5-fold de 5017 ici contre 4521 pour le modèle de R. Le R² est similaire dans les deux langages (~0.93) en revanche. Cela souligne que le modèle de régression est tout de même robuste, malgré la complexité de l'arbre.\n",
    "\n",
    "Nous allons pouvoir explorer d'autres méthodes d'arbres de décision, comme les forêts aléatoires et le boosting, qui sont souvent plus performantes que les arbres de décision simples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forêts aléatoires\n",
    "\n",
    "#### Simple random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Créer et entraîner une forêt aléatoire\n",
    "rf_reg_cal = RandomForestRegressor(random_state=randomseed, oob_score=True)\n",
    "rf_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "\n",
    "# Prédictions sur les ensembles d'entraînement et de test\n",
    "y_train_pred_rf = rf_reg_cal.predict(X_train_calories_dummy)\n",
    "y_test_pred_rf = rf_reg_cal.predict(X_test_calories_dummy)\n",
    "\n",
    "# Calculer le MSE et le R2\n",
    "mse_train_rf = mean_squared_error(y_train_calories, y_train_pred_rf)\n",
    "mse_test_rf = mean_squared_error(y_test_calories, y_test_pred_rf)\n",
    "r2_train_rf = r2_score(y_train_calories, y_train_pred_rf)\n",
    "r2_test_rf = r2_score(y_test_calories, y_test_pred_rf)\n",
    "\n",
    "print(\"Random Forest - OOB score :\", rf_reg_cal.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : Le modèle de base basique aléatoire de `scikit-learn` est construit avec 100 arbres, avec les paramètres `min_samples_split = 2` (nombre minimum d'élements pour considérer une décision) et `min_samples_leaf = 1` (nombre minimum d'élement dans une feuille). Ces paramètres sont les valeurs par défaut de `scikit-learn`, mais nous allons les optimiser par la suite. \n",
    "\n",
    "Le modèle est construit avec un échantillonnage bootstrap, ce qui signifie que chaque arbre est construit sur un sous-ensemble aléatoire des données d'entraînement. Cela nous permet d'extraire l'erreur OOB qui est calculé par défaut avec le score R² dans `scikit-learn`, alors qu'en R, elle est traditionnellement évaluée via la somme des résidus au carré (RSS, Residual Sum of Squares).\n",
    "\n",
    "Contrairement à R ou le paramètre à optimiser est `mtry` (nombre de variables considérées à chaque split), `scikit-learn` nous permet d'optimiser plusieurs hyperparamètres essentiels :\n",
    "- **`max_depth`** : la profondeur maximale de chaque arbre (plus un arbre est profond, plus il peut modéliser des interactions complexes, mais aussi surapprendre). \n",
    "- **`min_samples_split`** : le nombre minimum d'échantillons requis pour diviser un noeud. Plus il est grand, plus l’arbre est contraint et moins il risque de surapprendre.\n",
    "- **`min_samples_leaf`** : le nombre minimum d'échantillons nécessaires dans une feuille terminale. Cela permet d’éviter des feuilles trop petites, ce qui améliore la robustesse.\n",
    "- **`max_features`** : le nombre maximal de variables considérées pour chercher le meilleur split à chaque division (équivalent au `mtry` de R). Peut être fixé à un nombre entier, à une proportion de la taille du sample (`float` entre 0 et 1), ou aux valeurs prédéfinies `'sqrt'` : $\\sqrt{n_\\text{variables}}$ ou `'log2'` : $\\log_2(n_\\text{variables})$.\n",
    "- **`max_leaf_nodes`** : limite le nombre total de feuilles de l’arbre, forçant une structure plus simple.\n",
    "- **`ccp_alpha`** : le paramètre de coût-complexité pour l'élagage (post-pruning) ; plus `ccp_alpha` est grand, plus l'élagage sera fort.\n",
    "\n",
    "Enfin, il nous est également permis de choisir le **critère d’évaluation** de la qualité du split (`criterion`).   \n",
    "Alors qu’en R, la performance est évaluée via le **RSS** (Residual Sum of Squares), l’option la plus proche disponible dans `scikit-learn` est `friedman_mse`, conçue pour optimiser la variance résiduelle de manière similaire au RSS.  \n",
    "Ici, nous avons l'occasion de comparer l'impact du choix du critère (`friedman_mse` vs `squared_error`) sur la construction des arbres.  \n",
    "\n",
    "Nous observerons notamment l'effet sur la performance de généralisation (via le score OOB R²) ainsi que sur le temps d'apprentissage et d'élagage.\n",
    "Le score OOB étant uniquement calculé sur la métrique R² sous `scikit-learn`, le modèle optimal ne sera pas directement comparable aux mesures obtenues en R (RSS).\n",
    "\n",
    "Par ailleurs, ces hyperparamètres **sont interdépendants** : en pratique, optimiser l'hyperparamètre `max_leaf_nodes` peut réduire la nécessité d'élaguer l'arbre, ou la nécéssité de définir `max_depth`. \n",
    "\n",
    "Nous avons décidé de construire un modèle de forêt aléatoire avec les paramètres par défaut et optimiser les hyperparamètres `n_estimators` et `max_features` ainsi que le paramètre `ccp_alpha` pour l'élagage, que nous avons vu en cours, mais que nous avons pas appliqué dans le modèle de R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest avec élagage\n",
    "\n",
    "Comme les forêts aléatoires sont construits avec un échantillonnage bootstrap, nous pouvons estimer l'**erreur OOB (Out-Of-Bag) pour évaluer la performance du modèle**. Ainsi nous n'avons pas besoin d'utiliser la validation croisée pour évaluer le modèle et déterminer les meilleurs hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Définir le grille de paramètres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': np.linspace(0.1, 1.0, 10),  # proportion du nombre total de variables\n",
    "    'ccp_alpha': [0.01, 0.1, 1.0, 5.0, 10.0],\n",
    "    'criterion': ['friedman_mse', 'squared_error'],  # Comparer plusieurs critères !\n",
    "    'oob_score': [True],\n",
    "}\n",
    "\n",
    "# Générer toutes les combinaisons possibles\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# Fonction pour entraîner et évaluer\n",
    "def train_and_evaluate(params):\n",
    "    model = RandomForestRegressor(random_state=randomseed, **params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_calories_dummy, y_train_calories)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'n_estimators': params['n_estimators'],\n",
    "        'max_features': params['max_features'],\n",
    "        'ccp_alpha': params['ccp_alpha'],\n",
    "        'criterion': params['criterion'],\n",
    "        'oob_score': model.oob_score_,\n",
    "        'training_time_sec': elapsed_time,\n",
    "    }\n",
    "\n",
    "# Paralléliser\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_and_evaluate)(params) for params in param_combinations\n",
    ")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Trier par oob_score décroissant\n",
    "results_df = results_df.sort_values(by='oob_score', ascending=False)\n",
    "\n",
    "# Afficher\n",
    "# display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_max_features = [0.9, 0.6, 0.4, 0.1]\n",
    "selected_ccp_alpha = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# Créer 4 sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))  # 2x2 grid\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, alpha in enumerate(selected_ccp_alpha):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Sous-ensemble des résultats pour ce ccp_alpha\n",
    "    subset = results_df[results_df['ccp_alpha'] == alpha]\n",
    "    \n",
    "    for max_feat in selected_max_features:\n",
    "        # Prendre uniquement les lignes correspondant à un max_features donné\n",
    "        curve = subset[np.isclose(subset['max_features'], max_feat)]\n",
    "        # Trier par n_estimators pour des courbes bien propres\n",
    "        curve = curve.sort_values('n_estimators')\n",
    "        \n",
    "        ax.plot(curve['n_estimators'], curve['oob_score'], marker='o', label=f'max_features = {max_feat}')\n",
    "    \n",
    "    ax.set_title(f'OOB Score vs n_estimators (ccp_alpha = {alpha})')\n",
    "    ax.set_xlabel('n_estimators')\n",
    "    ax.set_ylabel('OOB R² Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parmi les 100 meilleures combinaisons, sortir les 10 plus longues à fitter et les 10 plus courtes\n",
    "best_results_df = results_df[results_df['oob_score'] > 0.974].sort_values(by='training_time_sec', ascending=False).copy()\n",
    "display(best_results_df.head(10))\n",
    "\n",
    "display(best_results_df.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Interprétation des résulats de la forêt aléatoire** :\n",
    "\n",
    "Nous avons réalisé une analyse fine de la performance de la forêt aléatoire en fonction de plusieurs hyperparamètres (`n_estimators`, `max_features`, `ccp_alpha`), en nous concentrant sur l'estimation de l'erreur de généralisation via l'**OOB score**.\n",
    "\n",
    "$\\rightarrow$ **Influence du critère de split (`criterion`)**\n",
    "\n",
    "En observant le tableau des résultats, nous constatons que **le choix du critère `friedman_mse` ou `squared_error` n’impacte pratiquement pas la performance du modèle**.  \n",
    "Que ce soit en termes de **score OOB** ou de **temps d'entraînement**, les deux critères mènent aux **mêmes choix optimaux d'hyperparamètres**, avec des performances quasi-identiques.  \n",
    "Cela montre que, dans le cas de la forêt aléatoire, **le critère de construction locale des arbres influence peu la qualité globale du modèle**.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Influence de `max_features`**\n",
    "\n",
    "Comme nous l'avions observé lors de la modélisation sous R, **plus la proportion de variables sélectionnées à chaque split est élevée, meilleure est la performance de la forêt**.  \n",
    "Ici, c'est avec `max_features = 0.9` que nous obtenons les meilleurs scores OOB.\n",
    "\n",
    "En proposant davantage de variables au moment de créer les divisions, chaque arbre a accès à plus d'information pour produire des splits efficaces, ce qui améliore la qualité globale de la forêt.\n",
    "\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Influence de `ccp_alpha` (élagage)**\n",
    "\n",
    "L'élagage, contrôlé via le paramètre `ccp_alpha`, **semble avoir un effet négligeable sur la performance OOB**.\n",
    "\n",
    "Quelle que soit la valeur choisie (0.01, 0.1, 1.0, 10.0), l'OOB score reste quasiment stable.  \n",
    "Cela indique que **le modèle est naturellement robuste** et peu sensible au surapprentissage, même sans élagage agressif.\n",
    "\n",
    "Cela confirme l'intuition classique en forêt aléatoire : **l'overfitting n'est pas un problème majeur** grâce à l'agrégation de nombreux arbres faibles.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Performances extrêmes (meilleur modèle)**\n",
    "\n",
    "- Le **meilleur modèle** atteint un **OOB score** de **0.975666** et a nécessité **5.211 secondes** pour être entraîné.\n",
    "- Ce modèle utilise :\n",
    "  - `n_estimators = 500`\n",
    "  - `max_features = 1.0`\n",
    "  - `ccp_alpha = 1.0`\n",
    "  - `criterion = friedman_mse`\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Trade-off performance/temps**\n",
    "\n",
    "Parmi les modèles ayant un OOB score > 0.974, **le plus rapide** a pris seulement **0.897 secondes** pour un OOB score de **0.974343** (`n_estimators=100`, `max_features=0.8`, `ccp_alpha=0.10`).\n",
    "\n",
    "Cela montre que **des modèles plus légers peuvent offrir des performances presque équivalentes** tout en étant **beaucoup plus rapides** à entraîner.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Détail des modèles extrêmes**\n",
    "\n",
    "- **Top 10 modèles les plus longs à entraîner** (extraits du tableau) : majoritairement avec `n_estimators = 500`.\n",
    "- **Top 10 modèles les plus rapides** : configurations avec `n_estimators = 100` et `max_features` entre 0.8 et 1.0.\n",
    "\n",
    "Cela est cohérent avec l'idée que **plus le nombre d'arbres est élevé, plus le temps d'entraînement augmente**.\n",
    "\n",
    "--- \n",
    "\n",
    "**Conclusion** :\n",
    "\n",
    "Dans l'ensemble, nous constatons que :\n",
    "- **Un `max_features` élevé** permet d'améliorer significativement la performance du modèle.\n",
    "- **Le paramètre `ccp_alpha` (élagage) impacte très peu la qualité de la forêt**.\n",
    "- **Réduire `n_estimators`** permet **d’accélérer considérablement** l'entraînement sans perte substantielle de performance.\n",
    "- **La forêt aléatoire reste robuste** face au surapprentissage, même avec des arbres profonds et peu élagués.\n",
    "\n",
    "  \n",
    "Après avoir validé ces résultats, nous allons désormais nous intéresser à **l’importance des variables**, afin d’identifier les facteurs les plus influents dans la prédiction des calories, comme nous l'avions fait sous R.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the best random forest model \n",
    "\n",
    "best_rf_reg_cal = RandomForestRegressor(random_state=randomseed, n_estimators=500, max_features=0.9, ccp_alpha=0.01, criterion='friedman_mse', oob_score=True)\n",
    "best_rf_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "\n",
    "# extract variable importance\n",
    "importances = best_rf_reg_cal.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train_calories_dummy.columns[indices]\n",
    "importances = importances[indices]\n",
    "importances_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importances_df['Cumulative Importance'] = importances_df['Importance'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(importances_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_df,\n",
    "    palette='cool',\n",
    ")\n",
    "plt.title(\"Variable Importance from Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : À partir du modèle de forêt aléatoire optimal entraîné sous `scikit-learn`, nous avons extrait l'importance des variables basée sur la réduction de l'impureté cumulée (Gini importance). \n",
    "\n",
    "- Le prédicateur `Session_Duration (hours)` domine, expliquant 73.67% de la variance, ce qui est intuitif puisqu'une **session plus longue** implique mécaniquement **une dépense énergétique plus élevée**.\n",
    "- Il est suivi par `Avg_BPM`, qui contribue à 10.56% de la variance, ce qui est également logique car un rythme cardiaque lors d'une séane de sport plus élevé est souvent associé à une **dépense calorique accrue**.\n",
    "- Enfin, `SFat_Percentage`, `Experience_Level` et `Age` ont des contributions faibles, mais permettent de capter des interactions intéressantes et améliorent la performance globale du modèle.\n",
    "\n",
    "On observe ainsi que 5 variables expliquent à elles seules plus de **97 % de l'importance totale du modèle**.\n",
    "\n",
    "En revanche, sous R, les variables `Session_Duration (hours)` et `Avg_BPM` étaient les seules à ressortir comme les plus importantes, tandis que toutes les autres variables avaient une importance très faible. Ainsi, on peut déduire que `scikit-learn` ne construit pas les forêts aléatoires de la même manière que `caret` sous R.\n",
    "\n",
    "Nous allons maintenant nous intéresser à un autre algorithme d'arbres de décision, le **boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Gradient Boosting & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Définir le nombre de folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=randomseed)\n",
    "\n",
    "# Stocker les scores et temps\n",
    "r2_scores_gb = []\n",
    "mse_scores_gb = []\n",
    "times_gb = []\n",
    "\n",
    "r2_scores_xgb = []\n",
    "mse_scores_xgb = []\n",
    "times_xgb = []\n",
    "\n",
    "# Boucle sur les folds\n",
    "for train_index, val_index in kf.split(X_train_calories):\n",
    "    X_train_fold, X_val_fold = X_train_calories.iloc[train_index], X_train_calories.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_calories.iloc[train_index], y_train_calories.iloc[val_index]\n",
    "    \n",
    "    # Dummifier pour Gradient Boosting (pas pour XGBoost car enable_categorical=True)\n",
    "    X_train_fold_dummies = pd.get_dummies(X_train_fold, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "    X_val_fold_dummies = pd.get_dummies(X_val_fold, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "    \n",
    "    ## 1. Gradient Boosting\n",
    "    start_time = time.time()\n",
    "    gb_reg = GradientBoostingRegressor(random_state=randomseed)\n",
    "    gb_reg.fit(X_train_fold_dummies, y_train_fold)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_gb = gb_reg.predict(X_val_fold_dummies)\n",
    "    \n",
    "    r2_scores_gb.append(r2_score(y_val_fold, y_pred_gb))\n",
    "    mse_scores_gb.append(mean_squared_error(y_val_fold, y_pred_gb))\n",
    "    times_gb.append(elapsed_time)\n",
    "    \n",
    "    ## 2. XGBoost\n",
    "    start_time = time.time()\n",
    "    xgb_reg = XGBRegressor(random_state=randomseed, enable_categorical=True)\n",
    "    xgb_reg.fit(X_train_fold, y_train_fold)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_xgb = xgb_reg.predict(X_val_fold)\n",
    "    \n",
    "    r2_scores_xgb.append(r2_score(y_val_fold, y_pred_xgb))\n",
    "    mse_scores_xgb.append(mean_squared_error(y_val_fold, y_pred_xgb))\n",
    "    times_xgb.append(elapsed_time)\n",
    "\n",
    "# Résultats finaux\n",
    "print(f\"Gradient Boosting R² moyen (CV) : {np.mean(r2_scores_gb):.4f} ± {np.std(r2_scores_gb):.4f}\")\n",
    "print(f\"Gradient Boosting MSE moyen (CV) : {np.mean(mse_scores_gb):.2f} ± {np.std(mse_scores_gb):.2f}\")\n",
    "print(f\"Gradient Boosting Temps moyen d'entraînement (par fold) : {np.mean(times_gb):.2f} sec\")\n",
    "\n",
    "print(f\"\\nXGBoost R² moyen (CV) : {np.mean(r2_scores_xgb):.4f} ± {np.std(r2_scores_xgb):.4f}\")\n",
    "print(f\"XGBoost MSE moyen (CV) : {np.mean(mse_scores_xgb):.2f} ± {np.std(mse_scores_xgb):.2f}\")\n",
    "print(f\"XGBoost Temps moyen d'entraînement (par fold) : {np.mean(times_xgb):.2f} sec\")\n",
    "\n",
    "# Performance sur le test final\n",
    "print(f\"\\nGradient Boosting R² sur l'ensemble de test : {r2_score(y_test_calories, gb_reg.predict(X_test_calories_dummy)):.4f}\")\n",
    "print(f\"Gradient Boosting MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, gb_reg.predict(X_test_calories_dummy)):.2f}\")\n",
    "\n",
    "print(f\"XGBoost R² sur l'ensemble de test : {r2_score(y_test_calories, xgb_reg.predict(X_test_calories)):.4f}\")\n",
    "print(f\"XGBoost MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, xgb_reg.predict(X_test_calories)):.2f}\")\n",
    "\n",
    "# Comparaison avec Random Forest\n",
    "\n",
    "print(f\"\\nRandom Forest R² sur l'ensemble de test : {r2_score(y_test_calories, best_rf_reg_cal.predict(X_test_calories_dummy)):.4f}\")\n",
    "print(f\"Random Forest MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, best_rf_reg_cal.predict(X_test_calories_dummy)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** \n",
    "\n",
    "Les modèles de Gradient Boosting et de XGBoost présentent **d'excellentes performances** sans ajustement particulier des hyperparamètres.  \n",
    "\n",
    "À l'issue de la validation croisée 5-folds :\n",
    "- Le Gradient Boosting standard atteint un **R² moyen de 0.9937 ± 0.0014** et un **MSE moyen de 451.17 ± 69.83**,\n",
    "- Le modèle XGBoost atteint un **R² moyen de 0.9822 ± 0.0047** et un **MSE moyen de 1269.52 ± 255.84**.\n",
    "\n",
    "Les performances sur l'ensemble de test confirment cette excellente capacité de généralisation :\n",
    "- Le Gradient Boosting obtient un **R² de 0.9903** et un **MSE de 755.82**,\n",
    "- Le XGBoost obtient un **R² de 0.9824** et un **MSE de 1377.15**.\n",
    "\n",
    "On constate ainsi que **les deux modèles généralisent très bien**, sans réel phénomène de surapprentissage.\n",
    "\n",
    "En termes de coût computationnel, **les deux algorithmes sont très rapides à entraîner**, avec des temps moyens d'entraînement par fold d'environ **0.27 seconde pour Gradient Boosting** et **0.32 seconde pour XGBoost**.\n",
    "\n",
    "Comparativement, le modèle Random Forest, précédemment optimisé, obtient un **R² de 0.9768** et un **MSE de 1812.48**, tout en nécessitant un **temps d'entraînement beaucoup plus important** (~5.2 secondes).\n",
    "\n",
    "Ces résultats confirment que **les méthodes de boosting surpassent les forêts aléatoires** à la fois en termes de performance prédictive et d'efficacité computationnelle.\n",
    "\n",
    "Compte tenu de **ces résultats très satisfaisants**, notamment pour le Gradient Boosting, nous limiterons notre analyse aux modèles actuels sans procéder à une optimisation poussée de XGBoost.\n",
    "Toutefois, dans une démarche d'optimisation avancée, une recherche d'hyperparamètres sur XGBoost pourrait encore permettre d'améliorer ses performances.\n",
    "\n",
    "Dans ce contexte, nous allons désormais nous concentrer sur **l'interprétation de l'importance des variables**.\n",
    "\n",
    "#### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_gb_df = pd.DataFrame({'Feature': X_train_calories_dummy.columns, 'Importance': gb_reg.feature_importances_})\n",
    "importances_xgb_df = pd.DataFrame({'Feature': X_train_calories.columns, 'Importance': xgb_reg.feature_importances_})\n",
    "\n",
    "# Trier pour plus de lisibilité\n",
    "importances_gb_df = importances_gb_df.sort_values('Importance', ascending=False)\n",
    "importances_xgb_df = importances_xgb_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Tracer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot pour Gradient Boosting\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_gb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Variable Importance - Gradient Boosting\")\n",
    "axes[0].set_xlabel(\"Importance\")\n",
    "axes[0].set_ylabel(\"Feature\")\n",
    "\n",
    "# Plot pour XGBoost\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_xgb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Variable Importance - XGBoost\")\n",
    "axes[1].set_xlabel(\"Importance\")\n",
    "axes[1].set_ylabel(\"\")  # Pas besoin de répéter \"Feature\" à droite\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que Gradient Boosting et XGBoost obtiennent des performances très proches en termes de R², une analyse de l'importance des variables révèle des différences notables dans les contributions fines.\n",
    "\n",
    "Dans les deux modèles, `Session_Duration (hours)` et `Avg_BPM` dominent largement la prédiction, ce qui est cohérent avec les résultats précédents observés sous forêts aléatoires et en R.\n",
    "\n",
    "Toutefois, lorsque l'on s'intéresse aux variables secondaires, **les importances relatives divergent** :\n",
    "- Gradient Boosting répartit l'importance restante entre les variables `Age`, `SFat_Percentage` et `Gender_Male` alors que `Experience_Level` est inexistant dans le modèle.\n",
    "- XGBoost attribue une importance non négligeable directement à `Gender` en le mettant au même niveau que `Avg_BPM`, tandis que `Age` et `SFat_Percentage` restent marginaux.\n",
    "\n",
    "Ces différences s'expliquent par :\n",
    "- **La nature des modèles** : XGBoost, utilisant du boosting plus régularisé, capte parfois des combinaisons d'interactions que Gradient Boosting classique ne priorise pas aussi fortement.\n",
    "- **La manière de calculer l’importance** : Gradient Boosting utilise la réduction moyenne d'impureté, alors que XGBoost utilise une mesure fondée sur le gain moyen de splits (avec régularisation intégrée).\n",
    "\n",
    "**Conclusion** : malgré des performances globales similaires, les deux méthodes peuvent exploiter **différentes structures locales dans les données**, ce qui peut être précieux en cas de recherche d'interprétabilité avancée.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train_calories_scale_dummy = pd.get_dummies(X_train_calories_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "X_test_calories_scale_dummy = pd.get_dummies(X_test_calories_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "# Define the MLP Regressor\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', \n",
    "                             max_iter=500, random_state=randomseed)\n",
    "\n",
    "# Train the model on the training data\n",
    "mlp_regressor.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred_mlp = mlp_regressor.predict(X_test_calories_scale_dummy)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_test_mlp = mean_squared_error(y_test_calories, y_test_pred_mlp)\n",
    "r2_test_mlp = r2_score(y_test_calories, y_test_pred_mlp)\n",
    "\n",
    "print(\"MLP Regressor - MSE on test set: \", mse_test_mlp)\n",
    "print(\"MLP Regressor - R2 on test set: \", r2_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la grille d'hyperparamètres\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (150, 100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Configurer le GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPRegressor(max_iter=500, random_state=randomseed),\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Effectuer la recherche sur les données d'entraînement\n",
    "grid_search.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le score correspondant\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "print(\"Meilleur score R² :\", grid_search.best_score_)\n",
    "\n",
    "# Évaluer le modèle optimal sur l'ensemble de test\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_test_pred_best_mlp = best_mlp.predict(X_test_calories_scale_dummy)\n",
    "mse_test_best_mlp = mean_squared_error(y_test_calories, y_test_pred_best_mlp)\n",
    "r2_test_best_mlp = r2_score(y_test_calories, y_test_pred_best_mlp)\n",
    "\n",
    "print(\"MSE sur l'ensemble de test :\", mse_test_best_mlp)\n",
    "print(\"R² sur l'ensemble de test :\", r2_test_best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Mesurer temps d'entraînement pour le meilleur modèle\n",
    "start_time = time.time()\n",
    "best_mlp.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "train_time_mlp = time.time() - start_time\n",
    "\n",
    "print(f\"Temps d'entraînement du meilleur MLP : {train_time_mlp:.2f} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : Le meilleur modèle de réseau de neurones (MLP) a été entraîné via **GridSearchCV** avec une architecture de 3 couches cachées, utilisant la **fonction d'activation ReLU** et **l'optimiseur Adam**. Il obtient un **R² généralisé de 0.9728** et un **R² de 0.9845** sur l'ensemble de test, avec un **MSE de 1212.49**.\n",
    "\n",
    "Les meilleurs hyperparamètres sélectionnés sont :\n",
    "- Architecture : **(150, 100, 50)** (trois couches cachées)\n",
    "- Fonction d'activation : **ReLU**\n",
    "- Méthode d'optimisation : **Adam**\n",
    "- Apprentissage : **learning rate constant**\n",
    "- Régularisation (alpha) : **0.001**\n",
    "\n",
    "En termes de performance pure, le réseau de neurones optimisé se situe juste en-dessous des modèles de **Gradient Boosting** (meilleur modèle avec R² généralisé ≈ 0.9903) **et XGBoost** (R² généralisé ≈ 0.9824), mais semble légèrement mieux généraliser que le modèle XGBoost (bien que ce dernier n'ait pas été optimisé) en termes de MSE (1212.49 pour le MPL contre 1377.15 pour XGBoost).\n",
    "\n",
    "Le réseau de neurones a su **apprendre efficacement**, bien son **temps d'entraînement soit beaucoup plus important** par rapport aux modèles de ce niveau de performances (30 fois plus lent).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interprétation finale (comparaison des modèles)\n",
    "\n",
    "\n",
    "L'ensemble des modèles évalués présente des performances très solides sur la prédiction des calories dépensées :\n",
    "\n",
    "| Modèle             | R² Test  | MSE Test | Temps d'entraînement |\n",
    "|:-------------------|:--------:|:--------:|:--------------------:|\n",
    "| Gradient Boosting   | 0.9903   | 755.82   | ~0.21 sec par fold    |\n",
    "| MLP (réseau de neurones) | 0.9845   | 1212.49  | ~6.7 sec (mesuré)        |\n",
    "| XGBoost             | 0.9824   | 1377.15  | ~0.14 sec par fold    |\n",
    "| Random Forest       | 0.9768   | 1812.48  | 5.2 sec (complet)     |\n",
    "\n",
    "Le **Gradient Boosting** conserve une légère avance en termes de précision et d'erreur quadratique moyenne.  \n",
    "Le **réseau de neurones** propose une alternative très compétitive, atteignant un niveau de performance intermédiaire entre Gradient Boosting et XGBoost.  \n",
    "Le **temps d'entraînement** du MLP reste parfaitement acceptable, comparable à celui du Gradient Boosting.\n",
    "\n",
    "Enfin, **XGBoost**, bien que légèrement en retrait sans tuning spécifique, surpasse malgré tout la **forêt aléatoire** en termes de précision et de vitesse.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion générale** :\n",
    "\n",
    "> En résumé, les modèles de boosting et de réseaux de neurones surpassent les forêts aléatoires en termes de performance et d'efficacité.  \n",
    "> Le Gradient Boosting apparaît comme le modèle le plus performant, tandis que le réseau de neurones constitue une alternative compétitive et rapide.  \n",
    "> Tous les modèles sélectionnés généralisent correctement, confirmant la qualité du jeu de données et la robustesse des méthodes employées.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Comparaison synthétique des modèles de prédiction des calories brûlées**  \n",
    "Voici une analyse comparative des performances, avantages et limites de chaque méthode testée :\n",
    "\n",
    "\n",
    "### **Tableau Comparatif Synthétique des Modèles**\n",
    "\n",
    "| Modèle                     | R² Test    | MSE Test  | Temps d'entraînement (s) | Variables non nulles | Interprétabilité | Flexibilité (Non-linéarité) | Commentaire                                                                 |\n",
    "|----------------------------|------------|-----------|--------------------------|----------------------|-------------------|-----------------------------|------------------------------------------------------------------------------|\n",
    "| **Gradient Boosting**       | **0.9903** | **756**   | ~0.21/fold              | N/A                  | Modérée          | Élevée                      | Performances élevées en R² et MSE, temps rapide.                            |\n",
    "| **Lasso Quadratique**       | 0.993      | 542       | 0.012                   | 18                   | Haute            | Modérée (interactions)      | Meilleures performances grâce aux interactions non linéaires.               |\n",
    "| **SVR (noyau RBF)**         | 0.992      | 637       | 0.04                    | N/A                  | Faible           | Très élevée                 | Flexible mais peu interprétable.                                            |\n",
    "| **XGBoost**                 | 0.9824     | 1,377     | ~0.14/fold              | N/A                  | Modérée          | Élevée                      | Rapide et performant pour des données complexes.                            |\n",
    "| **Réseau de neurones (MLP)**| 0.9845     | 1,212     | 7.7                     | N/A                  | Faible           | Élevée                      | Complexe, temps d'entraînement élevé.                                       |\n",
    "| **Forêt aléatoire**         | 0.9768     | 1,812     | 5.2                     | N/A                  | Modérée          | Modérée                     | Équilibre entre performance et interprétabilité.                            |\n",
    "| **Régression Lasso (λ_min)**| 0.979      | 1,638     | 0.007                   | 12                   | Haute            | Aucune (linéaire)           | Performances optimales avec 12 variables.                                   |\n",
    "| **Régression Lasso (λ_1se)**| 0.979      | 1,764.87  | 0.004                   | 5                    | Haute            | Aucune (linéaire)           | Simplifié (5 variables), idéal pour l'interprétation.                       |\n",
    "| **Régression Ridge**        | 0.9787     | 1,661.23  | 1.67                    | Toutes               | Haute            | Aucune (linéaire)           | Régularisation L2 légèrement meilleure que la régression linéaire.          |\n",
    "| **Arbre de décision**       | 0.9425     | 4,484     | <1                      | N/A                  | Haute            | Modérée                     | Simple et rapide, mais performances limitées.                               |\n",
    "\n",
    "---\n",
    "\n",
    "avec\n",
    "\n",
    "1. **Temps d'entraînement** : \n",
    "   - `~0.21/fold` ou `~0.14/fold` : Temps moyen par fold en validation croisée.\n",
    "   - Autres valeurs : Temps total en secondes.\n",
    "2. **Variables non nulles** : Applicable uniquement aux modèles Lasso/Ridge.\n",
    "3. **Interprétabilité** :\n",
    "   - *Haute* : Modèles linéaires ou structure simple (ex: Lasso, Arbre).\n",
    "   - *Modérée* : Modèles complexes mais partiellement interprétables (ex: Forêt).\n",
    "   - *Faible* : Modèles \"boîte noire\" (ex: SVR, MLP).\n",
    "4. **Flexibilité** : Capacité à modéliser des relations non linéaires.\n",
    "#### **Analyse par méthode**  \n",
    "1. **Gradient Boosting**  \n",
    "   - **Avantages** : Meilleure performance globale (R² ≈ 0.99, MSE ≈ 756), rapidité, capture de relations non linéaires complexes.  \n",
    "   - **Limites** : Interprétabilité modérée (importance des variables mais pas des interactions précises).  \n",
    "   - **Cas d’usage** : Solution par défaut pour maximiser la précision sans contrainte de temps.  \n",
    "\n",
    "2. **Lasso Quadratique (interactions)**  \n",
    "   - **Avantages** : Performance proche du Gradient Boosting (MSE ≈ 571) avec une **interprétabilité élevée** (coefficients explicites).  \n",
    "   - **Limites** : Flexibilité limitée aux interactions polynomiales (degré 3).  \n",
    "   - **Cas d’usage** : Modèle équilibré pour expliquer des synergies entre variables (ex : âge × BPM).  \n",
    "\n",
    "3. **SVR (noyau RBF)**  \n",
    "   - **Avantages** : Flexibilité maximale pour capturer des motifs complexes (R² ≈ 0.992).  \n",
    "   - **Limites** : Boîte noire, temps d’optimisation long, difficile à interpréter.  \n",
    "   - **Cas d’usage** : Données hautement non linéaires où l’interprétation est secondaire.  \n",
    "\n",
    "4. **XGBoost**  \n",
    "   - **Avantages** : Rapidité et performance solide (R² ≈ 0.98), régularisation intégrée.  \n",
    "   - **Limites** : Légèrement moins précis que le Gradient Boosting standard.  \n",
    "   - **Cas d’usage** : Grands jeux de données nécessitant rapidité et parallélisation.  \n",
    "\n",
    "5. **Réseau de neurones (MLP)**  \n",
    "   - **Avantages** : Performance compétitive (R² ≈ 0.98), adapté aux patterns complexes.  \n",
    "   - **Limites** : Temps d’entraînement élevé, interprétabilité très faible.  \n",
    "   - **Cas d’usage** : Alternative aux SVR/boosting si l’infrastructure le permet.  \n",
    "\n",
    "6. **Forêt aléatoire**  \n",
    "   - **Avantages** : Robustesse, interprétabilité modérée (importance des variables).  \n",
    "   - **Limites** : Performance inférieure aux modèles de boosting, temps d’entraînement long.  \n",
    "   - **Cas d’usage** : Données bruyantes, besoin de stabilité sans optimisation fine.  \n",
    "\n",
    "7. **Modèles linéaires (Lasso/Ridge)**  \n",
    "   - **Avantages** : Interprétabilité maximale, rapidité.  \n",
    "   - **Limites** : Incapables de capturer des non-linéarités (MSE > 1,600).  \n",
    "   - **Cas d’usage** : Analyses exploratoires ou contraintes de simplicité.  \n",
    "\n",
    "8. **Arbre de décision**  \n",
    "   - **Avantages** : Interprétabilité haute, règles claires.  \n",
    "   - **Limites** : Surapprentissage marqué (MSE ≈ 4,484), performance faible.  \n",
    "   - **Cas d’usage** : Visualisation pédagogique, pas de déploiement en production.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Recommandations finales**  \n",
    "- **Pour la précision** : **Gradient Boosting** ou **Lasso Quadratique** (selon le besoin d’interprétabilité).  \n",
    "- **Pour la vitesse** : **XGBoost** ou **Lasso Quadratique**.  \n",
    "- **Pour l’interprétabilité** : **Lasso Quadratique** (interactions) ou **Régression Lasso** (modèle linéaire).  \n",
    "- **Pour les données non linéaires complexes** : **SVR (RBF)** ou **Réseau de neurones**.  \n",
    "\n",
    "**Conclusion** : Le choix dépend des priorités :  \n",
    "- Le **Gradient Boosting** et le **Lasso Quadratique** se démarquent comme les meilleurs compromis performance-interprétabilité.  \n",
    "- Les **modèles linéaires** restent utiles pour des insights rapides, mais sont limités par la nature non linéaire des données.  \n",
    "- Les **arbres (boosting/forêts)** et **SVR** sont à privilégier si la flexibilité prime sur l’explicabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction d'Experience Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des différents formats de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = X_train_exp_level.shape[0]\n",
    "N_test = X_test_exp_level.shape[0]\n",
    "\n",
    "print(\"Dimension\")\n",
    "print(\"Données unidimensionelles, : \" + str(X_train_exp_level.shape))\n",
    "print(\"Données Normalisées, : \" + str(X_train_exp_level_scale.shape))\n",
    "print(\"Vecteur réponse (scikit-learn) : \" + str(y_train_exp_level.shape))\n",
    "\n",
    "results = []\n",
    "\n",
    "def add_model_result(name, y_true, y_pred, runtime): # err_gene_vc\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append({\n",
    "        'Modèle': name,\n",
    "        'Score de généralisation': round(acc, 3),\n",
    "        #'+ par validation croisée': round(err_gene_vc,3),\n",
    "        'Durée (s)': round(runtime, 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Principe\n",
    "Une méthode statistique ancienne mais finalement efficace sur ces données. La régression logistique est adaptée à la prévision d'une variable binaire. Dans le cas multiclasse, la fonction logistique de la librairie `Scikit-learn` estime *par défaut* **un modèle par classe**: une classe contre les autres. \n",
    "\n",
    "La probabilité d'appartenance d'un individu à une classe est modélisée à l'aide d'une combinaison linéaire des variables explicatives. Pour transformer une combinaison linéaire à valeur dans $R$ en une probabilité à valeurs dans l'intervalle $[0, 1]$, une fonction de forme sigmoïdale est appliquée.  Ceci donne: $$P(y_i=1)=\\frac{e^{Xb}}{1+e^{Xb}}$$ ou, c'est équivalent, une décomposition linéaire du *logit* ou *log odd ratio* de  $P(y_i=1)$:  $$\\log\\frac{P(y_i=1)}{1-P(y_i=1)}=Xb.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation sans optimisation / sans régularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_exp_level_dummies = pd.get_dummies(X_train_exp_level, drop_first=True)\n",
    "X_test_exp_level_dummies = pd.get_dummies(X_test_exp_level, drop_first=True)\n",
    "\n",
    "print(X_train_exp_level_dummies)\n",
    "print(X_train_exp_level_dummies.shape)\n",
    "print(X_train_exp_level.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "ts = time.time()\n",
    "for solver in ['liblinear','lbfgs', 'saga', 'sag', 'newton-cg']:\n",
    "    method = LogisticRegression(solver=solver ,multi_class='auto')  #lbfgs, saga, sag, newton-cg\n",
    "    method.fit(X_train_exp_level_dummies,y_train_exp_level)\n",
    "    score = method.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "    ypred = method.predict(X_test_exp_level_dummies)\n",
    "    te = time.time()\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "    pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "ts = time.time()\n",
    "method = LogisticRegression(solver='liblinear' , penalty='l1', multi_class='auto')  #lbfgs, saga, sag, newton-cg\n",
    "method.fit(X_train_exp_level_dummies,y_train_exp_level)\n",
    "score = method.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "ypred = method.predict(X_test_exp_level_dummies)\n",
    "te = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(X_test_exp_level_dummies))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes \"niveau 1\" et \"niveau 2\", correspondant respectivement aux niveaux d'expérience faibles et moyens, sont mal différenciées par ce modèle. En revanche, le niveau 3 est parfaitement appris, avec aucune erreur de classification sur l'échantillon de test. Ces résultats sont cohérents avec l'analyse exploratoire, qui avait déjà mis en évidence la proximité entre les niveaux d'expérience 1 et 2, rendant leur distinction plus difficile.\n",
    "\n",
    "Le modèle présente une erreur de prévision de 10,9 %, avec un temps d'exécution extrêmement faible (0 seconde)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les autres options de solver (lbfgs, saga, sag, newton-cg) ne convergent pas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"Logistic Regression\", y_test_exp_level, ypred, te-ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation du modèle par pénalisation Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ts = time.time()\n",
    "param=[{\"C\":[0.94,0.95,0.96,0.99,1]}]   #[0.5,1,5,10,12,15,30] [0.1, 0.5, 1, 2, 5, 10, 20] [ 0.5, 1, 5, 10, 30, 100, 200]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\",solver='liblinear', \n",
    "                                        multi_class='auto'), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(X_train_exp_level_dummies, y_train_exp_level)  \n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "te = time.time()\n",
    "print(\"Temps : %d secondes\" %(te-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur score par validation croisée = %f, Meilleur paramètre = %s\" % (logitOpt.best_score_,logitOpt.best_params_)) #score apprentisage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur paramètre trouvé est C = 0.95, une valeur très proche de C = 1. Par conséquent, les résultats obtenus, notamment la matrice de confusion et l'erreur de prévision, restent identiques à ceux de la régression logistique non optimisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yChap = logitOpt.predict(X_test_exp_level_dummies)\n",
    "# matrice de confusion\n",
    "score=logitOpt.score(X_test_exp_level_dummies, y_test_exp_level)  #score généralisation= prédiction \n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(X_test_exp_level_dummies))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve les mêmes résultats: la matrice de confusion et l'erreure de prévision sont les mêmes que pour la regression logistique non optimisée.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats obtenus sont identiques : la matrice de confusion et l'erreur de prévision restent les mêmes que pour la régression logistique non optimisée.\n",
    "\n",
    "L'objet regLassOpt issu de GridSearchCV ne conserve pas directement les coefficients du modèle final. Pour obtenir et interpréter ces coefficients, il est nécessaire de réentraîner un modèle LogisticRegression avec la valeur optimale de C sur l'ensemble des données d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logit=LogisticRegression(penalty=\"l1\",solver='liblinear', \n",
    "                                        multi_class='auto', C=logitOpt.best_params_['C'])\n",
    "model_lasso=logit.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "model_lasso.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients du modèle Lasso\n",
    "coefs = model_lasso.coef_\n",
    "\n",
    "# Compter le nombre de coefficients non nuls pour chaque classe\n",
    "non_zero_coefs_per_class = np.sum(coefs != 0, axis=1)\n",
    "\n",
    "# Afficher le nombre de coefficients non nuls par classe\n",
    "for i, count in enumerate(non_zero_coefs_per_class):\n",
    "    print(f\"Classe {i+1} : {count} coefficients non nuls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes 1 et 2 : Deux coefficients ont été réduits à zéro, ce qui signifie que ces variables explicatives n'ont pas d'impact significatif sur la probabilité d'appartenance à ces classes.\n",
    "\n",
    "Classe 3 : Dix coefficients ont été supprimés, ce qui montre que davantage de variables sont jugées non pertinentes pour cette classe.\n",
    "\n",
    "Interprétation : La régularisation Lasso favorise un modèle plus parcimonieux en éliminant les variables inutiles, ce qui peut améliorer l'interprétabilité et réduire le risque de sur-apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    # Créer une série pandas avec les noms de variables\n",
    "    coefs = pd.Series(model_lasso.coef_[i], index=X_train_exp_level_dummies.columns)\n",
    "    \n",
    "    # Filtrer les coefficients non nuls\n",
    "    coefs = coefs[coefs != 0].sort_values()\n",
    "    \n",
    "    # Affichage\n",
    "    coefs.plot(kind='barh', figsize=(6, 4))\n",
    "    plt.title(f\"Variables sélectionnées par Lasso - Classe {i+1}\")\n",
    "    plt.xlabel(\"Coefficient\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprétation des coefficients du modèle\n",
    "Les coefficients obtenus à partir du modèle permettent d’évaluer l’influence des variables explicatives sur la probabilité d’appartenance à chaque niveau d’expérience. Voici une analyse détaillée pour chaque niveau :\n",
    "\n",
    "Niveau d’expérience 1 :\n",
    "Coefficients proches de zéro ou nuls : Plusieurs variables explicatives ont des coefficients très faibles ou nuls, indiquant qu’elles ont peu d’impact sur la probabilité d’appartenir à ce niveau.\n",
    "\n",
    "Fréquence d’entraînement : Les coefficients associés aux variables \"entraînement 3, 4 ou 5 fois/semaine\" sont faibles, voire négatifs. Cela signifie qu’un individu qui s’entraîne fréquemment a une probabilité plus faible d’être classé au niveau 1. Ce résultat est cohérent avec l’idée que les individus ayant une faible expérience s’entraînent généralement moins souvent.\n",
    "\n",
    "Niveau d’expérience 2 :\n",
    "Poids négatifs importants : Certaines variables, comme un entraînement hebdomadaire régulier ou une masse grasse élevée, ont des coefficients négatifs marqués. Cela indique que ces caractéristiques réduisent la probabilité d’appartenir au niveau 2.\n",
    "\n",
    "Coefficients positifs : Certaines catégories, comme une tranche d’âge ou une masse grasse spécifique, présentent des coefficients positifs. Cela peut refléter un profil particulier d’individus ayant une probabilité accrue d’appartenir à ce niveau intermédiaire.\n",
    "\n",
    "Niveau d’expérience 3 :\n",
    "Variable dominante : Le coefficient le plus élevé (environ 1.91) est associé à une variable liée au pourcentage de masse grasse. Cela signifie que plus le pourcentage de masse grasse est élevé, plus la probabilité d’appartenir au niveau 3 augmente.\n",
    "\n",
    "Interprétation : Ce résultat est cohérent avec l’idée que les individus de niveau 3, souvent plus expérimentés, peuvent avoir des caractéristiques physiques spécifiques, comme un pourcentage de masse grasse plus élevé, qui reflètent leur profil d’entraînement ou leur morphologie.\n",
    "\n",
    "Cette methode a supprimé moins de variables que l'équivalent en R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les coefficients\n",
    "coefs = model_lasso.coef_  # array de shape (n_classes, n_features)\n",
    "\n",
    "# Transformer les coefficients en DataFrame\n",
    "coef_df = pd.DataFrame(coefs, columns=X_train_exp_level_dummies.columns)  # colonnes = noms des variables\n",
    "coef_df.index = [f\"Class_{i}\" for i in range(coefs.shape[0])]  # index = classes\n",
    "\n",
    "# Trouver les variables utilisées (au moins une fois ≠ 0 dans une classe)\n",
    "coef_used = (coef_df != 0).any(axis=0)  # un mask booléen sur les colonnes\n",
    "selected_coefs = coef_df.loc[:, coef_used]\n",
    "\n",
    "# Calculer la moyenne par variable et trier\n",
    "mean_coefs = selected_coefs.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# 7. Afficher\n",
    "print(\"Variables sélectionnées et triées (moyenne des coefficients) :\")\n",
    "print(mean_coefs)\n",
    "\n",
    "# 8. Visualiser\n",
    "mean_coefs.plot(kind='barh', figsize=(10,6))\n",
    "plt.title(\"Variables sélectionnées par Lasso (moyenne des coefs sur les classes)\")\n",
    "plt.xlabel(\"Coefficient moyen\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de l'impact des variables explicatives\n",
    "Cette analyse identifie les variables ayant le plus d'influence sur la probabilité d'appartenance à la classe cible :\n",
    "\n",
    "Variables augmentant la probabilité :\n",
    "\n",
    "SFat_Percentage : Un pourcentage de masse grasse élevé augmente significativement la probabilité d'appartenance à la classe cible.\n",
    "Gender_Male : Être de sexe masculin est également associé à une probabilité accrue.\n",
    "Variables réduisant la probabilité :\n",
    "\n",
    "Workout_Frequency (days/week)_5 : Une fréquence d'entraînement élevée diminue la probabilité d'appartenance.\n",
    "Height et Water_Intake : Une plus grande taille et une consommation d'eau élevée réduisent également cette probabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"Logistic Regression avec optimisation Lasso\", y_test_exp_level, yChap, te-ts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Discriminante Linéaire  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "ts=time.time()\n",
    "\n",
    "method=LinearDiscriminantAnalysis()\n",
    "method.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "score = method.score(X_test_exp_level_dummies,y_test_exp_level)\n",
    "ypred = method.predict(X_test_exp_level_dummies)\n",
    "te=time.time()\n",
    "t_total = te-ts\n",
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# matrice de confusion\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(X_test_exp_level_dummies))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur de généralisation de 10,3 % reflète une bonne capacité prédictive du modèle. La répartition des erreurs est similaire à celle de la régression logistique classique, avec une légère amélioration (une erreur de moins). Les individus de niveau 3 sont bien identifiés, tandis que les niveaux 1 et 2 restent plus souvent confondus, confirmant leur proximité observée lors de l'analyse exploratoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"ADL\", y_test_exp_level, ypred, te-ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-nearest neighbors (KNN)\n",
    "\n",
    "Cas particulier d'analyse discriminante avec estimation locale des fonctions de densité conditionnelle . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Convert the test data to a DataFrame with the same columns as the training data\n",
    "X_test_exp_level_dummies_df = pd.DataFrame(np.array(X_test_exp_level_dummies), columns=X_train_exp_level_dummies.columns)\n",
    "\n",
    "ts=time.time()\n",
    "method=KNeighborsClassifier(n_jobs=-1)\n",
    "method.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "score = method.score(np.array(X_test_exp_level_dummies),y_test_exp_level)\n",
    "ypred = method.predict(np.array(X_test_exp_level_dummies_df))\n",
    "te=time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score obtenu est nettement inférieur à celui des autres méthodes testées précédemment, mais le temps d'exécution est très court. Le nombre de voisins utilisé par défaut est fixé à 5. Nous allons à présent chercher à optimiser ce paramètre pour améliorer les performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "ts=time.time()\n",
    "\n",
    "param_grid = {'n_neighbors': list(range(1, 16))}  # Tester de 1 à 15 voisins\n",
    "\n",
    "method=KNeighborsClassifier(n_jobs=-1)\n",
    "kn= GridSearchCV(method, param_grid, cv=10, scoring='accuracy')# recherche par validation croisée\n",
    "knOpt=kn.fit(np.array(X_train_exp_level_dummies), y_train_exp_level)  # Assurez-vous que X_train_np est bien un np.array\n",
    "\n",
    "te=time.time()\n",
    "t_total=te-ts\n",
    "\n",
    "print(\"temps : %d secondes\" %(t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur nombre de voisins :\", knOpt.best_params_['n_neighbors']) #paramètre trouvé \n",
    "print(\"Meilleure score en validation croisée :\", knOpt.best_score_) #score généralisation vc\n",
    "yChap=knOpt.predict(np.array(X_test_exp_level_dummies))\n",
    "score=accuracy_score(y_test_exp_level, yChap) # score généralisation \n",
    "print(\"Score : %f, time running : %d secondes\" %(score, t_total))\n",
    "expected_loss = log_loss(y_test_exp_level, knOpt.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur nombre de voisins est 11, avec un score de généralisation moyen en validation croisée de 0.695 (erreur de 31 %). L'erreur de généralisation simple obtenue est de 28 %, ce qui reste comparable au score obtenu sans optimisation du nombre de voisins.\n",
    "\n",
    "Même après optimisation, la méthode des K plus proches voisins (KNN) montre des performances limitées. La matrice de confusion révèle que le niveau 1 est particulièrement mal classé : 47 correctement prédits, mais 28 confondus avec le niveau 2 et 3 avec le niveau 3. De plus, le temps d'exécution est relativement long (22 secondes contre 5 secondes sans optimisation), rendant cette amélioration peu rentable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"KNN\", y_test_exp_level, yChap, te-ts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate \n",
    "ts = time.time()\n",
    "method = SVC(kernel='linear', gamma='auto', probability=True)\n",
    "method.fit(X_train_exp_level_dummies,y_train_exp_level)\n",
    "score = method.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "score_cv=cross_validate(method, X_train_exp_level_dummies, y_train_exp_level, cv=5, scoring='accuracy')\n",
    "#mettre erreure validation croisée \n",
    "ypred = method.predict(X_test_exp_level_dummies)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score de généraisation : %f, time running : %d secondes\" %(score, te-ts))\n",
    "print(\"Score de généralisation par validation croisée : %f\" %(score_cv['test_score'].mean()))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)\n",
    "#ajouter erreur de généralisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode SVM linéaire, avec les paramètres par défaut, s'est révélée très efficace, avec une erreur de généralisation en validation croisée de 15 % pour un temps d'exécution raisonnable de 21 secondes.\n",
    "\n",
    "Pour améliorer davantage les performances, nous allons optimiser le paramètre de régularisation C à l'aide d'une recherche sur grille (GridSearchCV). Cela permettra de trouver la valeur optimale de C qui équilibre le biais et la variance, tout en maintenant une bonne capacité de généralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"SVM linéaire défault\", y_test_exp_level, ypred, te-ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "param=[{\"C\":[0.1,0.5,1,2,5,10]}]\n",
    "svm= GridSearchCV(SVC(kernel='linear'),param,cv=5,n_jobs=-1)\n",
    "svmOpt=svm.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "te = time.time()\n",
    "te-ts\n",
    "print(\"Meilleur score de généralisation en valisation croisée= %f, Meilleur paramètre = %s\" % (svmOpt.best_score_,svmOpt.best_params_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmOpt=SVC(kernel='linear',C=svmOpt.best_params_['C'],probability=True)\n",
    "svmOpt.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "yChap = svmOpt.predict(X_test_exp_level_dummies)\n",
    "score = accuracy_score(y_test_exp_level, yChap) \n",
    "print(\"Score de généralisation : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, svmOpt.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les erreurs de généralisation simple et en validation croisée sont presque identiques à celles obtenues avec le modèle SVM linéaire non optimisé. Cependant, le temps d'exécution de ce modèle optimisé est particulièrement long (1 minute 54 secondes). Cette amélioration marginale des performances ne justifie pas le coût en temps de calcul, rendant cette optimisation peu rentable par rapport au modèle non optimisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"SVM linéaire optimisée\", y_test_exp_level, yChap, te-ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM radiale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts = time.time()\n",
    "method = SVC(kernel='rbf',gamma='auto', probability=True)\n",
    "method.fit(X_train_exp_level_dummies,y_train_exp_level)\n",
    "score = method.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "score_cv=cross_validate(method, X_train_exp_level_dummies, y_train_exp_level, cv=5, scoring='accuracy')\n",
    "ypred = method.predict(X_test_exp_level_dummies)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score de généralisation : %f, time running : %d secondes\" %(score, te-ts))\n",
    "print(\"Score de généralisation par validation croisée : %f\" %(score_cv['test_score'].mean()))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle semble prédire uniquement la classe \"niveau 2\", indépendamment des vraies classes. Aucune observation des niveaux 1 et 3 n'est correctement classée. Avec une erreur de généralisation de 50 %, cette méthode s'avère inefficace pour classer correctement les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"SVM radiale défault\", y_test_exp_level, ypred, te-ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "param=[{\"C\":[0.1,0.5,1,2,10],\"gamma\":[0.001,.01,.1,.5,1]}]\n",
    "svm= GridSearchCV(SVC(),param,cv=10,n_jobs=-1)\n",
    "svmOpt=svm.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "te = time.time()\n",
    "te-ts\n",
    "print(\"Meilleur score de généralisation en validation croisée = %f, Meilleur paramètre = %s\" % (svmOpt.best_score_,svmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmOpt=SVC(C=1, gamma=0.001, probability=True)\n",
    "svmOpt.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "yChap=svmOpt.predict(X_test_exp_level_dummies)\n",
    "score = svmOpt.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "print(\"Score de généraisation : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les erreurs de classification restent significatives, et la qualité des prédictions demeure limitée. Cependant, l'optimisation des paramètres a permis une amélioration notable : le modèle prédit désormais des individus dans les classes 1 et 3, contrairement à la version initiale. L'erreur de généralisation reste élevée à 25 %, mais le temps d'exécution est plus raisonnable (17 secondes) comparé à celui observé lors de l'optimisation des paramètres de la SVM linéaire.\n",
    "\n",
    "En conclusion, la SVM linéaire sans optimisation des paramètres reste la méthode la plus adaptée à ce problème de classification parmi les différentes SVM testées. Elle offre un bon compromis entre précision et temps d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"SVM radiale optimisée\", y_test_exp_level, yChap, te-ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# One-Hot Encoding des variables catégorielles\n",
    "X_train_exp_level_encoded = pd.get_dummies(X_train_exp_level)\n",
    "X_test_exp_level_encoded = pd.get_dummies(X_test_exp_level)\n",
    "\n",
    "# Assurer que les colonnes de train et test sont alignées\n",
    "X_train_exp_level_encoded, X_test_exp_level_encoded = X_train_exp_level_encoded.align(X_test_exp_level_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Initialiser le modèle CART\n",
    "cart_model = DecisionTreeClassifier(random_state=randomseed)\n",
    "\n",
    "# Entraîner sur les données encodées\n",
    "cart_model.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "\n",
    "# Prédictions\n",
    "y_pred_cart = cart_model.predict(X_test_exp_level_encoded)\n",
    "\n",
    "# Affichage de l'arbre\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    cart_model, \n",
    "    feature_names=X_train_exp_level_encoded.columns.tolist(),\n",
    "    class_names=cart_model.classes_.astype(str).tolist(),\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de décision - CART\")\n",
    "plt.show()\n",
    "\n",
    "# Évaluation\n",
    "conf_mat_cart_test = confusion_matrix(y_test_exp_level, y_pred_cart)\n",
    "conf_mat_cart_train = confusion_matrix(y_train_exp_level, cart_model.predict(X_train_exp_level_encoded))\n",
    "\n",
    "print(\"Accuracy CART (test):\", round(accuracy_score(y_test_exp_level, y_pred_cart), 4))\n",
    "print(\"Accuracy CART (train):\", round(accuracy_score(y_train_exp_level, cart_model.predict(X_train_exp_level_encoded)), 4))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_mat_cart_test, display_labels=cart_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - CART (test)\")\n",
    "plt.show()\n",
    "\n",
    "ConfusionMatrixDisplay(conf_mat_cart_train, display_labels=cart_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - CART (train)\")\n",
    "plt.show()\n",
    "\n",
    "#Logloss\n",
    "y_pred_cart_proba = cart_model.predict_proba(X_test_exp_level_encoded)\n",
    "logloss_cart_test = log_loss(y_test_exp_level, y_pred_cart_proba,normalize=True)\n",
    "print(\"Logloss CART (test):\", round(logloss_cart_test, 4))\n",
    "print(\"Logloss CART (train):\", round(log_loss(y_train_exp_level, cart_model.predict_proba(X_train_exp_level_encoded)), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-créer un arbre sans élagage\n",
    "cart_model_full = DecisionTreeClassifier(random_state=randomseed, ccp_alpha=0.0)\n",
    "cart_model_full.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "\n",
    "\n",
    "# Extraire les valeurs de ccp_alpha possibles\n",
    "path = cart_model_full.cost_complexity_pruning_path(X_train_exp_level_encoded, y_train_exp_level)\n",
    "\n",
    "ccp_alphas = path.ccp_alphas[:-1]\n",
    "impurities = path.impurities[:-1]\n",
    "\n",
    "\n",
    "# Liste pour stocker les modèles entraînés pour chaque ccp_alpha\n",
    "models = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    model = DecisionTreeClassifier(random_state=randomseed, ccp_alpha=ccp_alpha)\n",
    "    model.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "    models.append(model)\n",
    "\n",
    "# Accuracy pour chaque arbre\n",
    "train_scores = [model.score(X_train_exp_level_encoded, y_train_exp_level) for model in models]\n",
    "test_scores = [model.score(X_test_exp_level_encoded, y_test_exp_level) for model in models]\n",
    "\n",
    "# Tracer la courbe\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas, train_scores, marker='o', label='train', drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas, test_scores, marker='o', label='test', drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"ccp_alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy en fonction de ccp_alpha\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Choisir le modèle avec la meilleure accuracy sur le test\n",
    "best_idx = np.argmax(test_scores)\n",
    "best_ccp_alpha = ccp_alphas[best_idx]\n",
    "print(f\"Meilleur ccp_alpha : {best_ccp_alpha:.5f}\")\n",
    "\n",
    "# Recréer l'arbre élagué\n",
    "cart_model_pruned = DecisionTreeClassifier(random_state=randomseed, ccp_alpha=best_ccp_alpha)\n",
    "cart_model_pruned.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    cart_model_pruned, \n",
    "    feature_names=X_train_exp_level_encoded.columns.tolist(), \n",
    "    class_names=cart_model_pruned.classes_.astype(str).tolist(),\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de décision élagué - CART\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Prédictions avec l'arbre élagué\n",
    "y_pred_cart_pruned = cart_model_pruned.predict(X_test_exp_level_encoded)\n",
    "\n",
    "# Évaluation\n",
    "conf_mat_cart_pruned_test = confusion_matrix(y_test_exp_level, y_pred_cart_pruned)\n",
    "conf_mat_cart_pruned_train = confusion_matrix(y_train_exp_level, cart_model_pruned.predict(X_train_exp_level_encoded))\n",
    "\n",
    "print(\"Accuracy CART élagué (test):\", round(accuracy_score(y_test_exp_level, y_pred_cart_pruned), 4))\n",
    "print(\"Accuracy CART élagué (train):\", round(accuracy_score(y_train_exp_level, cart_model_pruned.predict(X_train_exp_level_encoded)), 4))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_mat_cart_pruned_test, display_labels=cart_model_pruned.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - CART élagué (test)\")\n",
    "plt.show()\n",
    "\n",
    "ConfusionMatrixDisplay(conf_mat_cart_pruned_train, display_labels=cart_model_pruned.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - CART élagué (train)\")\n",
    "plt.show()\n",
    "\n",
    "# Calcul du log loss (multiclass log loss) pour l'arbre élagué\n",
    "y_test_exp_level_int = y_test_exp_level.astype(int)\n",
    "proba_cart_pruned = cart_model_pruned.predict_proba(X_test_exp_level_encoded)\n",
    "logloss_cart_pruned = log_loss(y_test_exp_level_int, proba_cart_pruned, labels=cart_model_pruned.classes_)\n",
    "print(\"Log loss (CART élagué, test):\", round(logloss_cart_pruned, 4))\n",
    "\n",
    "# Log loss pour l'arbre élagué sur le train\n",
    "logloss_cart_pruned_train = log_loss(y_train_exp_level, cart_model_pruned.predict_proba(X_train_exp_level_encoded))\n",
    "print(\"Log loss (CART élagué, train):\", round(logloss_cart_pruned_train, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Modèle initial (sans élagage)\n",
    "- Un arbre de décision a été construit avec `DecisionTreeClassifier` de `sklearn` sans élagage explicite.\n",
    "- **Accuracy** :\n",
    "  - Jeu d'entraînement : **1.0 (100%)**\n",
    "  - Jeu de test : **0.8769 (87.69%)**\n",
    "  - Le logloss sur le jeu d'entraînement est très faible, ce qui traduit un ajustement quasi parfait du modèle aux données d'apprentissage (sur-apprentissage).\n",
    "  - Sur le jeu de test, le logloss est plus élevé, indiquant que le modèle est moins confiant et fait davantage d'erreurs de probabilité sur des données non vues.\n",
    "- **Analyse** :\n",
    "  - Le modèle a parfaitement classé les données d'entraînement, ce qui indique un **sur-apprentissage** (*overfitting*).\n",
    "  - Sur le jeu de test, l'accuracy est élevée mais inférieure à celle du jeu d'entraînement, confirmant une capacité de généralisation limitée.\n",
    "- **Matrice de confusion (test)** :\n",
    "  - Quelques confusions entre les classes 1 et 2.\n",
    "  - La classe 3 est parfaitement prédite.\n",
    "\n",
    "#### b. Élagage de l'arbre\n",
    "- Un arbre complet a été construit pour explorer les valeurs possibles de `ccp_alpha` (paramètre de complexité).\n",
    "- Une validation croisée a été réalisée pour sélectionner la valeur optimale de `ccp_alpha` en maximisant l'accuracy sur le jeu de test.\n",
    "- **Meilleur `ccp_alpha`** : 0.00432\n",
    "- Un nouvel arbre élagué a été construit avec cette valeur.\n",
    "\n",
    "---\n",
    "\n",
    "### Résultats après élagage\n",
    "- **Accuracy** :\n",
    "  - Jeu d'entraînement : **0.9037 (90.37%)**\n",
    "  - Jeu de test : **0.9021 (90.21%)**\n",
    "  - Après élagage, le logloss augmente légèrement sur le train mais diminue sur le test, ce qui montre une meilleure calibration des probabilités et une généralisation accrue.\n",
    "  - Un logloss plus faible sur le jeu de test signifie que le modèle attribue des probabilités plus justes aux bonnes classes, et pas seulement des prédictions correctes.\n",
    "\n",
    "- **Analyse** :\n",
    "  - L'élagage a permis de réduire le sur-apprentissage, avec une accuracy plus équilibrée entre le jeu d'entraînement et le jeu de test.\n",
    "  - La performance sur le jeu de test a légèrement augmenté par rapport au modèle initial.\n",
    "- **Matrice de confusion (test)** :\n",
    "  - Réduction des confusions entre les classes 1 et 2.\n",
    "  - La classe 3 reste parfaitement prédite.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualisation des arbres\n",
    "#### a. Arbre initial (sans élagage)\n",
    "- L'arbre initial est très complexe, avec de nombreux nœuds et feuilles.\n",
    "- Cette complexité excessive reflète un ajustement excessif aux données d'entraînement.\n",
    "\n",
    "#### b. Arbre élagué\n",
    "- L'arbre élagué est plus simple, avec moins de nœuds et de feuilles.\n",
    "- Il conserve une bonne capacité de prédiction tout en améliorant la généralisation.\n",
    "\n",
    "### Conclusion\n",
    "- **Modèle initial** : Bien qu'il atteigne une accuracy élevée sur le jeu de test, il souffre de sur-apprentissage en raison de sa complexité excessive.\n",
    "- **Modèle élagué** : L'élagage a permis de simplifier l'arbre, réduisant le sur-apprentissage et améliorant la capacité de généralisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest et Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement du modèle Random Forest\n",
    "\n",
    "L'objectif des forets aléatoires est de réduire la variance des arbres tout en conservant leur pouvoir prédictif via le bagging, qui est une technique combinant bootstraping et agrégation d'arbres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Encodage des variables catégorielles\n",
    "X_train_rf = pd.get_dummies(X_train_exp_level)\n",
    "X_test_rf = pd.get_dummies(X_test_exp_level)\n",
    "X_train_rf, X_test_rf = X_train_rf.align(X_test_rf, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Entraînement du modèle\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,      # nombre d'arbres\n",
    "    max_features=4,        # nombre de variables testées à chaque split\n",
    "    random_state=24,\n",
    "    oob_score=True,        # permet d'obtenir l'erreur OOB\n",
    "    n_jobs=-1,             # accélère l'entraînement\n",
    ")\n",
    "rf_model.fit(X_train_rf, y_train_exp_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score OOB et matrice de confusion\n",
    "y_pred_rf = rf_model.predict(X_test_rf)\n",
    "conf_mat_rf_test = confusion_matrix(y_test_exp_level, y_pred_rf)\n",
    "conf_mat_rf_train = confusion_matrix(y_train_exp_level, rf_model.predict(X_train_rf))\n",
    "print(\"Accuracy Random Forest (test):\", round(accuracy_score(y_test_exp_level, y_pred_rf), 4))\n",
    "print(\"Accuracy Random Forest (train):\", round(accuracy_score(y_train_exp_level, rf_model.predict(X_train_rf)), 4))\n",
    "ConfusionMatrixDisplay(conf_mat_rf_test, display_labels=rf_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - Random Forest (test)\")\n",
    "plt.show()\n",
    "ConfusionMatrixDisplay(conf_mat_rf_train, display_labels=rf_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - Random Forest (train)\")\n",
    "plt.show()\n",
    "print(f\"OOB score: {rf_model.oob_score_:.4f}\")\n",
    "\n",
    "# Log loss\n",
    "y_pred_rf_proba = rf_model.predict_proba(X_test_rf)\n",
    "logloss_rf_test = log_loss(y_test_exp_level, y_pred_rf_proba, normalize=True)\n",
    "print(\"Log loss Random Forest (test):\", round(logloss_rf_test, 4))\n",
    "print(\"Log loss Random Forest (train):\", round(log_loss(y_train_exp_level, rf_model.predict_proba(X_train_rf)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats du modèle Random Forest\n",
    "\n",
    "#### 1. Précision (Accuracy)\n",
    "- **Accuracy sur le jeu d'entraînement** : **1.0 (100%)**\n",
    "  - Le modèle Random Forest classe parfaitement toutes les observations du jeu d'entraînement.\n",
    "  - Cela indique un fort sur-apprentissage (*overfitting*), le modèle ayant mémorisé les données d'entraînement.\n",
    "- **Accuracy sur le jeu de test** : **0.9077 (90.77%)**\n",
    "  - La précision sur le jeu de test est très bonne, supérieure à celle obtenue avec l'arbre de décision seul.\n",
    "  - L'écart avec l'entraînement montre que le modèle généralise bien, même si le sur-apprentissage reste présent.\n",
    "\n",
    "#### 2. Matrices de confusion\n",
    "- **Jeu d'entraînement** :\n",
    "  - Toutes les classes sont parfaitement prédites (aucune erreur).\n",
    "  - Cela confirme l'ajustement parfait du modèle sur les données d'entraînement.\n",
    "- **Jeu de test** :\n",
    "  - **Classe 1** : 67 bien classés, 11 confondus avec la classe 2.\n",
    "  - **Classe 2** : 72 bien classés, 7 confondus avec la classe 1.\n",
    "  - **Classe 3** : 38 bien classés, aucune confusion.\n",
    "  - Les erreurs concernent principalement la confusion entre les classes 1 et 2, la classe 3 étant parfaitement identifiée.\n",
    "\n",
    "#### 3. Logloss\n",
    "- **Log loss (test)** : **0.1989**\n",
    "- **Log loss (entraînement)** : **0.0681**\n",
    "\n",
    "L'écart entre le log loss du train (très faible) et celui du test (plus élevé) confirme le sur-apprentissage du modèle sur les données d'entraînement. Toutefois, la valeur relativement basse du log loss sur le test indique que le modèle attribue des probabilités assez fiables aux bonnes classes, ce qui est un atout supplémentaire par rapport à l'arbre de décision simple. Le log loss permet ainsi d'évaluer non seulement la justesse des prédictions, mais aussi la qualité de la calibration des probabilités fournies par la Random Forest.\n",
    "#### 4. Interprétation\n",
    "- Le modèle Random Forest offre une excellente capacité de classification sur le jeu de test, avec une précision supérieure à 90%.\n",
    "- La classe 3 est parfaitement prédite, ce qui montre la robustesse du modèle pour cette catégorie.\n",
    "- Les confusions entre les classes 1 et 2 sont réduites par rapport à l'arbre de décision simple, mais restent présentes.\n",
    "- Le sur-apprentissage est visible sur le jeu d'entraînement, mais l'utilisation de l'**OOB score** et l'évaluation sur le jeu de test permettent de valider la bonne généralisation du modèle.\n",
    "\n",
    "#### 5. Conclusion\n",
    "- **Random Forest** améliore la performance globale par rapport à un arbre unique, notamment sur la capacité de généralisation.\n",
    "- Il reste important de surveiller le sur-apprentissage, mais la robustesse du modèle sur le jeu de test confirme son efficacité pour ce problème de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : Le modèle de base Random Forest de `scikit-learn` est construit avec 100 arbres, avec les paramètres `min_samples_split = 2` (nombre minimum d'élements pour considérer une décision) et `min_samples_leaf = 1` (nombre minimum d'élement dans une feuille). Ces paramètres sont les valeurs par défaut de `scikit-learn`, mais nous allons les optimiser par la suite. \n",
    "\n",
    "Le modèle est construit avec un échantillonnage bootstrap, ce qui signifie que chaque arbre est construit sur un sous-ensemble aléatoire des données d'entraînement. Cela nous permet d'extraire l'erreur OOB.\n",
    "\n",
    "Contrairement à R ou le paramètre à optimiser est `mtry` (nombre de variables considérées à chaque split), `scikit-learn` nous permet d'optimiser plusieurs hyperparamètres essentiels :\n",
    "- **`max_depth`** : la profondeur maximale de chaque arbre (plus un arbre est profond, plus il peut modéliser des interactions complexes, mais aussi surapprendre). \n",
    "- **`min_samples_split`** : le nombre minimum d'échantillons requis pour diviser un noeud. Plus il est grand, plus l’arbre est contraint et moins il risque de surapprendre.\n",
    "- **`min_samples_leaf`** : le nombre minimum d'échantillons nécessaires dans une feuille terminale. Cela permet d’éviter des feuilles trop petites, ce qui améliore la robustesse.\n",
    "- **`max_features`** : le nombre maximal de variables considérées pour chercher le meilleur split à chaque division (équivalent au `mtry` de R). Peut être fixé à un nombre entier, à une proportion de la taille du sample (`float` entre 0 et 1), ou aux valeurs prédéfinies `'sqrt'` : $\\sqrt{n_\\text{variables}}$ ou `'log2'` : $\\log_2(n_\\text{variables})$.\n",
    "- **`max_leaf_nodes`** : limite le nombre total de feuilles de l’arbre, forçant une structure plus simple.\n",
    "- **`ccp_alpha`** : le paramètre de coût-complexité pour l'élagage (post-pruning) ; plus `ccp_alpha` est grand, plus l'élagage sera fort.\n",
    "\n",
    "Enfin, il nous est également permis de choisir le **critère d’évaluation** de la qualité du split (`criterion`). \n",
    "Ici, nous avons l'occasion de comparer l'impact du choix du critère (`gini` vs `log_loss`) sur la construction des arbres.  \n",
    "\n",
    "Nous observerons notamment l'effet sur la performance de généralisation (via le score OOB) ainsi que sur le temps d'apprentissage et d'élagage.\n",
    "\n",
    "Par ailleurs, ces hyperparamètres **sont interdépendants** : en pratique, optimiser l'hyperparamètre `max_leaf_nodes` peut réduire la nécessité d'élaguer l'arbre, ou la nécéssité de définir `max_depth`. \n",
    "\n",
    "Nous avons décidé de construire un modèle de forêt aléatoire avec les paramètres par défaut et optimiser les hyperparamètres `n_estimators` et `max_features` ainsi que le paramètre `ccp_alpha` pour l'élagage, que nous avons vu en cours, mais que nous n'avons pas appliqué dans le modèle de R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Définir la grille de paramètres pour la classification\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': np.linspace(0.1, 1.0, 10),  # proportion du nombre total de variables\n",
    "    'ccp_alpha': [0.01, 0.1, 1.0, 5.0, 10.0],\n",
    "    'criterion': ['gini', 'log_loss'],  # critères pour la classification\n",
    "    'oob_score': [True],\n",
    "}\n",
    "\n",
    "# Générer toutes les combinaisons possibles\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# Fonction pour entraîner et évaluer (classification)\n",
    "def train_and_evaluate(params):\n",
    "    model = RandomForestClassifier(random_state=randomseed, **params)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_rf, y_train_exp_level)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return {\n",
    "        'n_estimators': params['n_estimators'],\n",
    "        'max_features': params['max_features'],\n",
    "        'ccp_alpha': params['ccp_alpha'],\n",
    "        'criterion': params['criterion'],\n",
    "        'oob_score': model.oob_score_,\n",
    "        'training_time_sec': elapsed_time,\n",
    "    }\n",
    "\n",
    "# Paralléliser\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_and_evaluate)(params) for params in param_combinations\n",
    ")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Trier par oob_score décroissant\n",
    "results_df = results_df.sort_values(by='oob_score', ascending=False)\n",
    "\n",
    "# Afficher\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parmi les meilleures combinaisons, afficher les 10 plus longues et les 10 plus rapides à entraîner\n",
    "best_results_df = results_df[results_df['oob_score'] > 0.85].sort_values(by='training_time_sec', ascending=False).copy()\n",
    "\n",
    "display(best_results_df.head(10))   # 10 plus longues à fitter\n",
    "display(best_results_df.tail(10))   # 10 plus courtes à fitter\n",
    "\n",
    "# Logloss sur best_rf_clf\n",
    "# Entraîner le meilleur modèle de forêt aléatoire pour la classification\n",
    "\n",
    "best_rf_clf = RandomForestClassifier(\n",
    "    random_state=randomseed,\n",
    "    n_estimators=500,\n",
    "    max_features=0.2,\n",
    "    ccp_alpha=0.01,\n",
    "    criterion='gini',\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "best_rf_clf.fit(X_train_rf, y_train_exp_level)\n",
    "y_pred_best_rf_clf = best_rf_clf.predict(X_test_rf)\n",
    "y_pred_best_rf_clf_proba = best_rf_clf.predict_proba(X_test_rf)\n",
    "logloss_best_rf_clf_test = log_loss(y_test_exp_level, y_pred_best_rf_clf_proba, normalize=True)\n",
    "print(\"Log loss Random Forest (test):\", round(logloss_best_rf_clf_test, 4))\n",
    "print(\"Log loss Random Forest (train):\", round(log_loss(y_train_exp_level, best_rf_clf.predict_proba(X_train_rf)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation du log loss pour la Random Forest\n",
    "\n",
    "- **Log loss (test) : 0.2186**\n",
    "- **Log loss (train) : 0.2368**\n",
    "\n",
    "- Ici, le log loss est très proche entre le jeu d'entraînement et le jeu de test, ce qui indique que la Random Forest ne sur-apprend pas et généralise bien.\n",
    "- Une valeur de log loss autour de 0.22 est considérée comme très bonne pour un problème de classification à 3 classes, surtout avec une accuracy supérieure à 90%.\n",
    "- Cela signifie que le modèle ne se contente pas de prédire la bonne classe, mais qu'il est également bien calibré dans ses probabilités.\n",
    "\n",
    "En résumé, la Random Forest fournit à la fois des prédictions précises et bien calibrées sur ce jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Interprétation des résultats de la forêt aléatoire (classification)**\n",
    "\n",
    "Nous avons analysé la performance de la forêt aléatoire selon plusieurs hyperparamètres (`n_estimators`, `max_features`, `ccp_alpha`, `criterion`), en nous concentrant sur l’**OOB score** (ici, l’accuracy OOB).\n",
    "\n",
    "$\\rightarrow$ **Performances maximales (modèles les plus longs à entraîner)**\n",
    "\n",
    "Les 10 modèles les plus longs à entraîner utilisent tous `n_estimators = 500`, ce qui est cohérent : plus il y a d’arbres, plus le temps de calcul augmente.  \n",
    "On observe que :\n",
    "- Les meilleurs OOB scores atteignent **0.8933** (soit ~89,3% de bonne classification sur les données OOB).\n",
    "- Ces scores sont obtenus avec différentes valeurs de `max_features` (0.6 à 1.0) et de `ccp_alpha` (0.01 ou 0.10), et avec différents critères (`gini`, `entropy`, `log_loss`).\n",
    "- Le critère de split (`criterion`) n’a pas d’impact significatif sur la performance, confirmant l’observation générale que ce choix influence peu la qualité globale du modèle.\n",
    "- L’élagage (`ccp_alpha`) a un effet négligeable sur l’OOB score, la performance restant stable quelle que soit la valeur choisie.\n",
    "\n",
    "$\\rightarrow$ **Performances maximales (modèles les plus rapides à entraîner)**\n",
    "\n",
    "Les 10 modèles les plus rapides utilisent `n_estimators = 100` et des valeurs de `max_features` comprises entre 0.1 et 0.2.  \n",
    "On note que :\n",
    "- Les meilleurs OOB scores atteignent **0.8946**, soit un niveau équivalent aux modèles les plus longs à entraîner.\n",
    "- Le temps d’entraînement est très court (~0.35 secondes), soit près de 15 fois plus rapide que les modèles à 500 arbres.\n",
    "- Les critères de split (`gini`, `entropy`, `log_loss`) et l’élagage (`ccp_alpha`) n’influencent pas significativement la performance.\n",
    "\n",
    "$\\rightarrow$ **Synthèse**\n",
    "\n",
    "- **Le nombre d’arbres (`n_estimators`)** : augmenter le nombre d’arbres n’apporte pas de gain significatif en OOB score, mais augmente fortement le temps de calcul.  \n",
    "- **La proportion de variables (`max_features`)** : des valeurs intermédiaires à élevées (0.2 à 1.0) donnent les meilleurs résultats, mais il n’y a pas de gain net à utiliser toutes les variables.\n",
    "- **L’élagage (`ccp_alpha`)** : a peu d’effet sur la performance, la forêt étant naturellement robuste au surapprentissage.\n",
    "- **Le critère de split** : le choix entre `gini`, `entropy` ou `log_loss` n’a pas d’impact majeur sur l’OOB score.\n",
    "\n",
    "**Conclusion** :\n",
    "Dans l'ensemble, nous constatons que :\n",
    "- **Un `max_features` élevé** permet d'améliorer significativement la performance du modèle.\n",
    "- **Le paramètre `ccp_alpha` (élagage) impacte très peu la qualité de la forêt**.\n",
    "- **Réduire `n_estimators`** permet **d’accélérer considérablement** l'entraînement sans perte substantielle de performance.\n",
    "- **La forêt aléatoire reste robuste** face au surapprentissage, même avec des arbres profonds et peu élagués.\n",
    "\n",
    "  \n",
    "Après avoir validé ces résultats, nous allons désormais nous intéresser à **l’importance des variables**, afin d’identifier les facteurs les plus influents dans la prédiction du niveau des sportifs, comme nous l'avions fait sous R.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_rf_clf.fit(X_train_rf, y_train_exp_level)\n",
    "\n",
    "# Extraire l'importance des variables\n",
    "importances = best_rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train_rf.columns[indices]\n",
    "importances = importances[indices]\n",
    "importances_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importances_df['Cumulative Importance'] = importances_df['Importance'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_df,\n",
    "    palette='cool'\n",
    ")\n",
    "plt.title(\"Importance des variables selon la forêt aléatoire\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : À partir du modèle de forêt aléatoire optimal entraîné sous scikit-learn, nous avons extrait l'importance des variables basée sur la réduction de l'impureté cumulée (Gini importance).\n",
    "\n",
    "Le prédicteur `Session_Duration (hours)` domine très nettement, expliquant à lui seul la plus grande part de la variance du modèle. Cela est cohérent, car une durée de séance plus longue est naturellement associée à un niveau d'expérience plus élevé chez les sportifs.\n",
    "\n",
    "Il est suivi par `SFat_Percentage` et les modalités de `Workout_Frequency (days/week)`, qui contribuent également de façon significative à la prédiction du niveau d'expérience. Ces variables traduisent l’intensité et la régularité de la pratique sportive, des facteurs logiquement liés à l’expérience.\n",
    "\n",
    "On note aussi l’importance de la variable `Calories_Burned`, qui reflète l’effort fourni, ainsi que de l’hydratation (`Water_Intake (liters)`), qui peut être un indicateur indirect de l’intensité ou de la durée des séances.\n",
    "\n",
    "Les autres variables (`LWeight`, `Avg_BPM`, `Max_BPM`, `Height (m)`, `Age`, etc.) ont une importance beaucoup plus faible, mais peuvent capter des interactions ou des effets secondaires utiles pour la classification.\n",
    "\n",
    "On observe ainsi que les 5 à 6 premières variables expliquent à elles seules la majeure partie de l’importance totale du modèle, ce qui montre que la prédiction du niveau d’expérience repose principalement sur la durée, la fréquence et l’intensité de l’activité physique.\n",
    "\n",
    "En comparaison, sous R, seules la durée de séance et la fréquence ressortaient comme déterminantes, alors que scikit-learn attribue une importance plus répartie à plusieurs variables. Cela illustre que la construction des forêts aléatoires peut différer selon l’implémentation et les critères utilisés.\n",
    "\n",
    "Nous allons maintenant nous intéresser à un autre algorithme d'arbres de décision, le boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Gradient Boosting & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Définir le nombre de folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=randomseed)\n",
    "\n",
    "# Stocker les scores et temps\n",
    "accuracy_scores_gb = []\n",
    "times_gb = []\n",
    "\n",
    "accuracy_scores_xgb = []\n",
    "times_xgb = []\n",
    "\n",
    "# Créer un encodeur de labels pour transformer les classes [1, 2, 3] en [0, 1, 2]\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Ajouter les listes pour stocker l'accuracy sur le jeu de test\n",
    "test_accuracy_gb = []\n",
    "test_accuracy_xgb = []\n",
    "\n",
    "# Boucle sur les folds\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_exp_level):\n",
    "    X_train_fold, X_val_fold = X_train_exp_level.iloc[train_index], X_train_exp_level.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_exp_level.iloc[train_index], y_train_exp_level.iloc[val_index]\n",
    "    \n",
    "    # Encoder les étiquettes pour XGBoost\n",
    "    y_train_fold_encoded = le.fit_transform(y_train_fold)\n",
    "    \n",
    "    # Dummifier pour Gradient Boosting ET XGBoost\n",
    "    X_train_fold_dummies = pd.get_dummies(X_train_fold)\n",
    "    X_val_fold_dummies = pd.get_dummies(X_val_fold)\n",
    "    X_test_dummies = pd.get_dummies(X_test_exp_level)\n",
    "    \n",
    "    # Aligner les colonnes\n",
    "    X_train_fold_dummies, X_val_fold_dummies = X_train_fold_dummies.align(X_val_fold_dummies, join='left', axis=1, fill_value=0)\n",
    "    X_train_fold_dummies, X_test_dummies_fold = X_train_fold_dummies.align(X_test_dummies, join='left', axis=1, fill_value=0)\n",
    "    \n",
    "    ## 1. Gradient Boosting\n",
    "    start_time = time.time()\n",
    "    gb_clf = GradientBoostingClassifier(random_state=randomseed)\n",
    "    gb_clf.fit(X_train_fold_dummies, y_train_fold)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_gb = gb_clf.predict(X_val_fold_dummies)\n",
    "    accuracy_scores_gb.append(accuracy_score(y_val_fold, y_pred_gb))\n",
    "    times_gb.append(elapsed_time)\n",
    "    # Accuracy sur le jeu de test\n",
    "    y_pred_gb_test = gb_clf.predict(X_test_dummies_fold)\n",
    "    test_accuracy_gb.append(accuracy_score(y_test_exp_level, y_pred_gb_test))\n",
    "    \n",
    "    ## 2. XGBoost (toujours sur les dummies pour cohérence)\n",
    "    start_time = time.time()\n",
    "    xgb_clf = XGBClassifier(random_state=randomseed, enable_categorical=False)\n",
    "    xgb_clf.fit(X_train_fold_dummies, y_train_fold_encoded)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_xgb_encoded = xgb_clf.predict(X_val_fold_dummies)\n",
    "    y_pred_xgb = le.inverse_transform(y_pred_xgb_encoded)\n",
    "    # Prédiction sur le jeu de test\n",
    "    y_pred_xgb_test_encoded = xgb_clf.predict(X_test_dummies_fold)\n",
    "    y_pred_xgb_test = le.inverse_transform(y_pred_xgb_test_encoded)\n",
    "    accuracy_scores_xgb.append(accuracy_score(y_val_fold, y_pred_xgb))\n",
    "    times_xgb.append(elapsed_time)\n",
    "    test_accuracy_xgb.append(accuracy_score(y_test_exp_level, y_pred_xgb_test))\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Gradient Boosting:\")\n",
    "print(f\"Accuracy moyenne (CV): {np.mean(accuracy_scores_gb):.4f} ± {np.std(accuracy_scores_gb):.4f}\")\n",
    "print(f\"Temps d'entraînement moyen: {np.mean(times_gb):.4f} secondes\")\n",
    "print(f\"Accuracy moyenne sur le jeu de test: {np.mean(test_accuracy_gb):.4f} ± {np.std(test_accuracy_gb):.4f}\")\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "print(f\"Accuracy moyenne (CV): {np.mean(accuracy_scores_xgb):.4f} ± {np.std(accuracy_scores_xgb):.4f}\")\n",
    "print(f\"Temps d'entraînement moyen: {np.mean(times_xgb):.4f} secondes\")\n",
    "print(f\"Accuracy moyenne sur le jeu de test: {np.mean(test_accuracy_xgb):.4f} ± {np.std(test_accuracy_xgb):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Logloss sur Gradient Boosting\n",
    "# Entraîner le modèle Gradient Boosting sur l'ensemble d'entraînement complet\n",
    "gb_clf = GradientBoostingClassifier(random_state=randomseed)\n",
    "gb_clf.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "y_pred_gb = gb_clf.predict(X_test_exp_level_encoded)\n",
    "y_pred_gb_proba = gb_clf.predict_proba(X_test_exp_level_encoded)\n",
    "logloss_gb_test = log_loss(y_test_exp_level, y_pred_gb_proba, normalize=True)\n",
    "print(\"Log loss Gradient Boosting (test):\", round(logloss_gb_test, 4))\n",
    "print(\"Log loss Gradient Boosting (train):\", round(log_loss(y_train_exp_level, gb_clf.predict_proba(X_train_exp_level_encoded)), 4))\n",
    "\n",
    "#Logloss sur XGBoost\n",
    "# Entraîner le modèle XGBoost sur l'ensemble d'entraînement complet\n",
    "\n",
    "\n",
    "# Encoder les labels pour correspondre à [0, 1, 2]\n",
    "le = LabelEncoder()\n",
    "y_train_exp_level_enc = le.fit_transform(y_train_exp_level)\n",
    "y_test_exp_level_enc = le.transform(y_test_exp_level)\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=randomseed, enable_categorical=False)\n",
    "xgb_clf.fit(X_train_exp_level_encoded, y_train_exp_level_enc)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_exp_level_encoded)\n",
    "y_pred_xgb_proba = xgb_clf.predict_proba(X_test_exp_level_encoded)\n",
    "logloss_xgb_test = log_loss(y_test_exp_level_enc, y_pred_xgb_proba, normalize=True)\n",
    "print(\"Log loss XGBoost (test):\", round(logloss_xgb_test, 4))\n",
    "print(\"Log loss XGBoost (train):\", round(log_loss(y_train_exp_level_enc, xgb_clf.predict_proba(X_train_exp_level_encoded)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation**\n",
    "\n",
    "Les modèles de Gradient Boosting et de XGBoost affichent **d’excellentes performances** sans optimisation avancée des hyperparamètres.\n",
    "\n",
    "### Résultats de la validation croisée (5-folds) :\n",
    "- **Gradient Boosting** : accuracy moyenne de **0.8741 ± 0.0251**\n",
    "- **XGBoost** : accuracy moyenne de **0.8625 ± 0.0287**\n",
    "\n",
    "### Performances sur le jeu de test :\n",
    "- **Gradient Boosting** : accuracy moyenne de **0.9118 ± 0.0123**\n",
    "- **XGBoost** : accuracy moyenne de **0.8933 ± 0.0060**\n",
    "\n",
    "**Log loss :**\n",
    "\n",
    "- **Gradient Boosting** :  \n",
    "    - Log loss (test) : **0.1756**  \n",
    "    - Log loss (train) : **0.0820**\n",
    "\n",
    "- **XGBoost** :  \n",
    "    - Log loss (test) : **0.2291**  \n",
    "    - Log loss (train) : **0.0078**\n",
    "\n",
    "Un log loss faible sur le test indique que les probabilités prédites sont bien calibrées. On observe que Gradient Boosting généralise mieux (écart train/test plus faible), tandis que XGBoost sur-apprend davantage (log loss train très bas, test plus élevé). Les deux modèles restent toutefois très performants.\n",
    "\n",
    "### Temps de calcul :\n",
    "- **Gradient Boosting** : **0.90 seconde** en moyenne par fold\n",
    "- **XGBoost** : **0.92 seconde** en moyenne par fold\n",
    "\n",
    "---\n",
    "\n",
    "On observe que **les deux modèles généralisent très bien**, avec des scores très proches entre validation croisée et test, et sans surapprentissage marqué.\n",
    "\n",
    "En termes de rapidité, **les deux algorithmes sont très efficaces**, avec des temps d’entraînement similaires et très courts.\n",
    "\n",
    "Comparé à la Random Forest optimisée, les méthodes de boosting offrent ici une **légère supériorité en généralisation et robustesse**.\n",
    "\n",
    "Ces résultats confirment que **le boosting est particulièrement adapté** à la classification du niveau d’expérience dans ce contexte.\n",
    "\n",
    "Compte tenu de ces performances très satisfaisantes, notamment pour le Gradient Boosting, il n’est pas nécessaire d’optimiser davantage XGBoost pour ce projet. Une recherche d’hyperparamètres pourrait toutefois permettre de gagner encore quelques points de performance si besoin.\n",
    "\n",
    "Nous pouvons donc passer à l’**analyse de l’importance des variables** pour mieux comprendre les facteurs déterminants de la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables pour la classification (niveau d'expérience)\n",
    "# Les deux modèles utilisent maintenant les mêmes dummies\n",
    "\n",
    "# Pour Gradient Boosting (avec dummies)\n",
    "importances_gb_df = pd.DataFrame({\n",
    "    'Feature': X_train_fold_dummies.columns,\n",
    "    'Importance': gb_clf.feature_importances_\n",
    "})\n",
    "\n",
    "# Pour XGBoost (avec dummies)\n",
    "importances_xgb_df = pd.DataFrame({\n",
    "    'Feature': X_train_fold_dummies.columns,\n",
    "    'Importance': xgb_clf.feature_importances_\n",
    "})\n",
    "\n",
    "# Trier pour plus de lisibilité\n",
    "importances_gb_df = importances_gb_df.sort_values('Importance', ascending=False)\n",
    "importances_xgb_df = importances_xgb_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Tracer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot pour Gradient Boosting\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_gb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Variable Importance - Gradient Boosting (Classification)\")\n",
    "axes[0].set_xlabel(\"Importance\")\n",
    "axes[0].set_ylabel(\"Feature\")\n",
    "\n",
    "# Plot pour XGBoost\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_xgb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Variable Importance - XGBoost (Classification)\")\n",
    "axes[1].set_xlabel(\"Importance\")\n",
    "axes[1].set_ylabel(\"\")  # Pas besoin de répéter \"Feature\" à droite\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation des importances des variables\n",
    "\n",
    "Les graphiques ci-dessus présentent l’importance des variables pour la classification du niveau d’expérience selon deux modèles : **Gradient Boosting** (à gauche) et **XGBoost** (à droite).\n",
    "\n",
    "#### Points communs\n",
    "- **Variables dominantes** : Dans les deux modèles, la durée des séances (`Session_Duration (hours)`) et la fréquence d’entraînement (`Workout_Frequency (days/week)_2`, `_3`, `_4`) sont les variables les plus importantes. Cela confirme que l’intensité et la régularité de la pratique sportive sont des facteurs clés pour prédire le niveau d’expérience.\n",
    "- **SFat_Percentage** (masse grasse transformée) ressort également comme un prédicteur important dans les deux cas.\n",
    "- Les autres variables (calories brûlées, âge, poids, BPM, hydratation, etc.) ont une importance beaucoup plus faible.\n",
    "\n",
    "#### Différences entre Gradient Boosting et XGBoost\n",
    "- **Gradient Boosting** accorde une importance maximale à la durée de séance, suivie de près par la fréquence d’entraînement et la masse grasse.\n",
    "- **XGBoost** met davantage l’accent sur les modalités de fréquence d’entraînement, qui passent devant la durée de séance. Il attribue aussi un peu plus d’importance à certaines modalités de type d’entraînement (`Workout_Type_Yoga`, `Workout_Type_HIIT`), ce qui n’est pas le cas pour Gradient Boosting.\n",
    "- Les importances sont plus « réparties » dans XGBoost, alors que Gradient Boosting concentre l’importance sur un plus petit nombre de variables.\n",
    "\n",
    "#### Explication des différences\n",
    "- **Nature de l’algorithme** : XGBoost utilise des techniques de régularisation et une gestion différente des splits, ce qui peut conduire à privilégier d’autres interactions ou modalités.\n",
    "- **Critère d’importance** : Les deux modèles calculent l’importance différemment (réduction d’impureté moyenne pour Gradient Boosting, gain moyen de split pour XGBoost), ce qui peut modifier le classement des variables.\n",
    "- **Stochasticité et interactions** : XGBoost explore parfois plus d’interactions entre variables, ce qui peut expliquer l’apparition de modalités secondaires dans son top des importances.\n",
    "- **Régularisation** : XGBoost pénalise davantage les variables peu informatives, ce qui peut « lisser » la distribution des importances.\n",
    "\n",
    "#### Conclusion\n",
    "Malgré ces différences, les deux modèles s’accordent sur les facteurs principaux : **durée et fréquence des séances** et **masse grasse**. Les divergences sur les variables secondaires sont normales et reflètent la sensibilité des algorithmes à la structure des données et à leur propre méthode d’optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Encodage des variables catégorielles pour la classification du niveau d'expérience\n",
    "X_train_exp_level_scale_dummy = pd.get_dummies(X_train_exp_level_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "X_test_exp_level_scale_dummy = pd.get_dummies(X_test_exp_level_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "# Aligner les colonnes entre train et test\n",
    "X_train_exp_level_scale_dummy, X_test_exp_level_scale_dummy = X_train_exp_level_scale_dummy.align(\n",
    "    X_test_exp_level_scale_dummy, join='left', axis=1, fill_value=0\n",
    ")\n",
    "\n",
    "# Définir le MLP Classifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',\n",
    "                              max_iter=500, random_state=randomseed)\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "mlp_classifier.fit(X_train_exp_level_scale_dummy, y_train_exp_level)\n",
    "\n",
    "# Prédire sur le jeu de test\n",
    "y_test_pred_mlp = mlp_classifier.predict(X_test_exp_level_scale_dummy)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy_test_mlp = accuracy_score(y_test_exp_level, y_test_pred_mlp)\n",
    "print(\"MLP Classifier - Accuracy sur le jeu de test :\", round(accuracy_test_mlp, 4))\n",
    "\n",
    "#Accuracy sur le jeu d'entraînement\n",
    "accuracy_train_mlp = accuracy_score(y_train_exp_level, mlp_classifier.predict(X_train_exp_level_scale_dummy))\n",
    "print(\"MLP Classifier - Accuracy sur le jeu d'entraînement :\", round(accuracy_train_mlp, 4))\n",
    "\n",
    "# Afficher le rapport de classification et la matrice de confusion\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test_exp_level, y_test_pred_mlp)).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - MLP Classifier (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [\n",
    "        (50,), (100,), (150,),\n",
    "        (100, 50), (150, 100), (150, 100, 50),\n",
    "        (200, 100, 50)\n",
    "    ],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPClassifier(max_iter=1000, early_stopping=True, random_state=randomseed),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Effectuer la recherche sur les données d'entraînement\n",
    "grid_search.fit(X_train_exp_level_scale_dummy, y_train_exp_level)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le score correspondant\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "print(\"Meilleur score accuracy (CV) :\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afficher les paramètres optimaux\n",
    "print(\"Meilleurs paramètres du MLP Classifier :\", grid_search.best_params_)\n",
    "\n",
    "#Fit le meilleur modèle sur l'ensemble d'entraînement\n",
    "best_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=grid_search.best_params_['hidden_layer_sizes'],\n",
    "    activation=grid_search.best_params_['activation'],\n",
    "    solver=grid_search.best_params_['solver'],\n",
    "    alpha=grid_search.best_params_['alpha'],\n",
    "    learning_rate=grid_search.best_params_['learning_rate'],\n",
    "    max_iter=1000,\n",
    "    random_state=randomseed\n",
    ")\n",
    "best_mlp.fit(X_train_exp_level_scale_dummy, y_train_exp_level)\n",
    "\n",
    "#Afficher la courbe de loss pour le meilleur modèle\n",
    "plt.plot(best_mlp.loss_curve_)\n",
    "plt.title(\"Courbe de perte du MLP\")\n",
    "plt.xlabel(\"Itérations\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Prédire sur le jeu de test\n",
    "y_test_pred_best_mlp = best_mlp.predict(X_test_exp_level_scale_dummy)\n",
    "# Évaluer le modèle\n",
    "accuracy_test_best_mlp = accuracy_score(y_test_exp_level, y_test_pred_best_mlp)\n",
    "print(\"MLP Classifier - Accuracy sur le jeu de test (meilleur modèle) :\", round(accuracy_test_best_mlp, 4))\n",
    "# Accuracy sur le jeu d'entraînement\n",
    "accuracy_train_best_mlp = accuracy_score(y_train_exp_level, best_mlp.predict(X_train_exp_level_scale_dummy))\n",
    "print(\"MLP Classifier - Accuracy sur le jeu d'entraînement (meilleur modèle) :\", round(accuracy_train_best_mlp, 4))\n",
    "#Afficher la matrice de confusion\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test_exp_level, y_test_pred_best_mlp)).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - MLP Classifier (meilleur modèle)\")\n",
    "plt.show()\n",
    "\n",
    "#Logloss sur le meilleur MLP\n",
    "y_test_pred_best_mlp_proba = best_mlp.predict_proba(X_test_exp_level_scale_dummy)\n",
    "logloss_best_mlp_test = log_loss(y_test_exp_level, y_test_pred_best_mlp_proba, normalize=True)\n",
    "print(\"Log loss MLP Classifier (test) :\", round(logloss_best_mlp_test, 4))\n",
    "print(\"Log loss MLP Classifier (train) :\", round(log_loss(y_train_exp_level, best_mlp.predict_proba(X_train_exp_level_scale_dummy)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** :  \n",
    "Le meilleur modèle de réseau de neurones (MLP) a été entraîné via **GridSearchCV** en testant différentes architectures, fonctions d’activation et méthodes d’optimisation. L’architecture optimale sélectionnée comporte **deux couches cachées** avec la fonction d’activation **tanh** et l’optimiseur **Adam**. Le modèle obtient une **accuracy de 0.8564** sur le jeu de test, et **0.9383** sur le jeu d’entraînement, ce qui indique un certain sur-apprentissage mais une capacité de généralisation correcte.\n",
    "\n",
    "Les meilleurs hyperparamètres sélectionnés sont :\n",
    "- Architecture : **(150, 100)** (deux couches cachées)\n",
    "- Fonction d’activation : **tanh**\n",
    "- Méthode d’optimisation : **Adam**\n",
    "- Apprentissage : **learning rate constant**\n",
    "- Régularisation (alpha) : **0.0001**\n",
    "\n",
    "En termes de performance, le réseau de neurones optimisé se situe en-dessous des meilleurs modèles d’ensemble comme le **Gradient Boosting** ou la **Random Forest** (accuracy ≈ 0.91-0.92). Le log loss du MLP Classifier est nettement plus élevé sur le jeu de test (0.3113) que sur le jeu d’entraînement (0.1382), ce qui traduit un sur-apprentissage : le modèle est très confiant sur les données d’entraînement mais ses probabilités sont moins bien calibrées sur des données nouvelles. Le MLP montre une bonne capacité à apprendre, mais reste plus sensible au sur-apprentissage et nécessite un temps d’entraînement plus important pour un gain de performance limité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilan des performances des différentes techniques\n",
    "\n",
    "## Tableau comparatif synthétique des modèles\n",
    "\n",
    "| Modèle                              | Accuracy Test (%) | Temps d'entraînement (s) | Interprétabilité       |\n",
    "|-------------------------------------|:-----------------:|:------------------------:|:----------------------:|\n",
    "| Gradient Boosting                   | 91.2              | ~0.9                     | Modérée                |\n",
    "| SVM linéaire (défaut)               | 91.3              | 169.03                  | Faible                 |\n",
    "| SVM linéaire (optimisée)            | 91.3              | 92.62                   | Faible                 |\n",
    "| Random Forest                       | 90.8              | ~5                      | Modérée                |\n",
    "| Analyse Discriminante Linéaire (ADL)| 90.8              | 0.02                    | Haute                  |\n",
    "| Logistic Regression                 | 90.3              | 0.09                    | Haute                  |\n",
    "| Logistic Regression avec Lasso      | 90.3              | 2.67                    | Haute                  |\n",
    "| XGBoost                             | 89.3              | ~0.9                    | Modérée                |\n",
    "| Réseau de neurones (MLP)            | 85.6              | 7.7                     | Faible                 |\n",
    "| Arbre élagué (CART)                 | 90.2              | Très rapide             | Haute                  |\n",
    "| Arbre de décision (CART)            | 87.7              | Très rapide             | Haute                  |\n",
    "| SVM radiale (optimisée)             | 74.4              | 9.90                    | Faible                 |\n",
    "| KNN                                 | 72.8              | 20.93                   | Faible                 |\n",
    "| SVM radiale (défaut)                | 40.5              | 2.40                    | Faible                 |\n",
    "\n",
    "---\n",
    "\n",
    "### Analyse par méthode\n",
    "\n",
    "**Gradient Boosting**\n",
    "- **Avantages** : Meilleure performance globale (accuracy ≈ 91.2 %), temps d'entraînement rapide.\n",
    "- **Limites** : Interprétabilité modérée (importance des variables mais pas des interactions précises).\n",
    "- **Cas d’usage** : Solution par défaut pour maximiser la précision sans contrainte de temps.\n",
    "\n",
    "**SVM linéaire**\n",
    "- **Avantages** : Très bonne précision (accuracy ≈ 91.3 %).\n",
    "- **Limites** : Temps d'entraînement élevé pour la version par défaut (169 s) et faible interprétabilité.\n",
    "- **Cas d’usage** : Précision maximale si le temps de calcul n'est pas une contrainte.\n",
    "\n",
    "**Random Forest**\n",
    "- **Avantages** : Bonne robustesse, interprétabilité modérée (importance des variables).\n",
    "- **Limites** : Performance légèrement inférieure au Gradient Boosting, temps d'entraînement plus long.\n",
    "- **Cas d’usage** : Données bruyantes ou besoin de stabilité sans optimisation fine.\n",
    "\n",
    "**Analyse Discriminante Linéaire (ADL)**\n",
    "- **Avantages** : Excellent compromis entre précision (accuracy ≈ 90.8 %) et rapidité (0.02 s).\n",
    "- **Limites** : Moins performant que les modèles d'ensemble.\n",
    "- **Cas d’usage** : Résultats rapides et fiables avec une interprétabilité élevée.\n",
    "\n",
    "**Logistic Regression (avec ou sans Lasso)**\n",
    "- **Avantages** : Interprétabilité élevée, rapidité d'entraînement.\n",
    "- **Limites** : Moins performant sur des données non linéaires.\n",
    "- **Cas d’usage** : Analyses exploratoires ou contraintes de simplicité.\n",
    "\n",
    "**XGBoost**\n",
    "- **Avantages** : Rapide et performant (accuracy ≈ 89.3 %), régularisation intégrée.\n",
    "- **Limites** : Légèrement moins précis que le Gradient Boosting.\n",
    "- **Cas d’usage** : Grands jeux de données nécessitant rapidité et parallélisation.\n",
    "\n",
    "**Réseau de neurones (MLP)**\n",
    "- **Avantages** : Capacité à capturer des patterns complexes.\n",
    "- **Limites** : Temps d'entraînement élevé (7.7 s), sur-apprentissage marqué, faible interprétabilité.\n",
    "- **Cas d’usage** : Alternative aux modèles d'ensemble si l'infrastructure le permet.\n",
    "\n",
    "**Arbres de décision (CART)**\n",
    "- **Avantages** : Interprétabilité élevée, règles claires.\n",
    "- **Limites** : Surapprentissage marqué, performance limitée.\n",
    "- **Cas d’usage** : Visualisation pédagogique ou analyses simples.\n",
    "\n",
    "**SVM radiale et KNN**\n",
    "- **Avantages** : Flexibilité pour des données complexes.\n",
    "- **Limites** : Faible précision (accuracy < 75 %), temps d'entraînement élevé pour KNN.\n",
    "- **Cas d’usage** : Non adaptés à ce jeu de données.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommandations finales\n",
    "\n",
    "- **Pour la précision** : Gradient Boosting ou SVM linéaire (défaut).\n",
    "- **Pour la rapidité** : ADL ou Logistic Regression.\n",
    "- **Pour l’interprétabilité** : ADL ou Logistic Regression avec Lasso.\n",
    "- **Pour des données complexes** : Gradient Boosting ou Random Forest.\n",
    "\n",
    "### **Conclusion**  \n",
    "Le **Gradient Boosting** et l'**ADL** se démarquent comme les meilleurs compromis entre performance et rapidité. Les modèles linéaires restent utiles pour des analyses rapides et interprétables, tandis que les modèles d'ensemble (Random Forest, XGBoost) offrent une robustesse accrue pour des données plus complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
