{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import prince\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym = pd.read_csv('gym_members_exercise_tracking.csv')\n",
    "\n",
    "display(gym.head().style.background_gradient(cmap='Reds'))\n",
    "gym.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_cat = gym.select_dtypes(include='object')\n",
    "gym_num = gym.select_dtypes(exclude='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 5x3 grid of subplots\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "columns = [\n",
    "    'Age', 'Weight (kg)', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM',\n",
    "    'Session_Duration (hours)', 'Calories_Burned', 'Fat_Percentage', 'Water_Intake (liters)',\n",
    "    'Workout_Frequency (days/week)', 'Experience_Level', 'BMI'\n",
    "]\n",
    "\n",
    "\n",
    "# Plot each column in a separate subplot\n",
    "for i, col in enumerate(columns):\n",
    "    gym_num[col].value_counts().plot(kind='hist', ax=axes[i], bins=20)\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# plt.hist(gym_num['Age'].value_counts())\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelisation\n",
    "## Prédiction des calories brûlées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = gym.drop(columns=['Calories_Burned', 'Gender', 'Workout_Type', 'Experience_Level', 'Workout_Frequency (days/week)'], axis=1)\n",
    "y = gym['Calories_Burned']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# adding the intercept\n",
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train] # concatenate over the second axis in order to X = [[1,vector_1],[1,vector_2],...]\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape est nécessaire car..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)  # Already added intercept manually\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_sklearn = model.predict(X_train)\n",
    "y_pred_test_sklearn = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "rmse_train_sklearn = mean_squared_error(y_train, y_pred_train_sklearn)\n",
    "rmse_test_sklearn = mean_squared_error(y_test, y_pred_test_sklearn)\n",
    "r2_train_sklearn = r2_score(y_train, y_pred_train_sklearn)\n",
    "r2_test_sklearn = r2_score(y_test, y_pred_test_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scikit-Learn Model - Training RMSE:\", rmse_train_sklearn, \"R2:\", r2_train_sklearn)\n",
    "print(\"Scikit-Learn Model - Testing RMSE:\", rmse_test_sklearn, \"R2:\", r2_test_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train, y_pred_train_sklearn, color='green', alpha=0.5, label='scikit-learn Regression Predictions')\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=2)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('scikit-learn Linear Regression Predictions')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Calcul des résidus\n",
    "residuals_train = y_train - y_pred_train_sklearn\n",
    "\n",
    "# Création de la figure et des subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Résidus vs Valeurs ajustées\n",
    "sns.residplot(x=y_pred_train_sklearn, y=residuals_train, lowess=True, line_kws={'color':'red', 'lw':1}, ax=axs[0,0])\n",
    "axs[0, 0].axhline(0, color='black', linestyle='dotted', alpha=0.6)\n",
    "axs[0, 0].set_xlabel(\"Fitted values\")\n",
    "axs[0, 0].set_ylabel(\"Residuals\")\n",
    "axs[0, 0].set_title(\"Residuals vs Fitted\")\n",
    "\n",
    "# 2. Graphique Q-Q\n",
    "stats.probplot(residuals_train, dist=\"norm\", plot=axs[0, 1])\n",
    "axs[0, 1].set_title(\"Normal Q-Q\")\n",
    "\n",
    "# 3. Scale-Location\n",
    "sns.residplot(x=y_pred_train_sklearn, y=np.sqrt(np.abs(residuals_train)), lowess=True, line_kws={'color':'red', 'lw':1}, ax=axs[1,0])\n",
    "axs[1, 0].set_xlabel(\"Fitted values\")\n",
    "axs[1, 0].set_ylabel(r\"$\\sqrt{\\text{Standardized Residuals}}$\")\n",
    "axs[1, 0].set_title(\"Scale-Location\")\n",
    "\n",
    "# 4. Leverage vs Résidus\n",
    "# Calcul du leverage pour les sous-graphiques\n",
    "hat_matrix = X_train @ np.linalg.inv(X_train.T @ X_train) @ X_train.T\n",
    "leverage = np.diag(hat_matrix)\n",
    "\n",
    "# Calcul de Cook's distance\n",
    "n, p = X_train.shape  # nombre d'observations et nombre de paramètres\n",
    "MSE = np.mean(residuals_train**2)\n",
    "cooks_d = (residuals_train**2 / (p * MSE)) * (leverage / (1 - leverage)**2)\n",
    "\n",
    "axs[1, 1].scatter(leverage, residuals_train, alpha=0.5)\n",
    "axs[1, 1].axhline(0, color='red', linestyle='--')\n",
    "axs[1, 1].set_xlabel(\"Leverage\")\n",
    "axs[1, 1].set_ylabel(\"Standardized Residuals\")\n",
    "axs[1, 1].set_title(\"Residuals vs Leverage\")\n",
    "\n",
    "\n",
    "# Ajout des courbes de Cook à 0.5 et 1\n",
    "x_vals = np.linspace(min(leverage), max(leverage), 100)\n",
    "cooks_0_5 = np.sqrt(0.5 * p * MSE * (1 - x_vals) / x_vals)\n",
    "cooks_1 = np.sqrt(1 * p * MSE * (1 - x_vals) / x_vals)\n",
    "\n",
    "axs[1, 1].plot(x_vals, cooks_0_5, 'r--', label=\"Cook's distance = 0.5\")\n",
    "axs[1, 1].plot(x_vals, -cooks_0_5, 'r--')\n",
    "axs[1, 1].plot(x_vals, cooks_1, 'g--', label=\"Cook's distance = 1\")\n",
    "axs[1, 1].plot(x_vals, -cooks_1, 'g--')\n",
    "\n",
    "# Légende pour les courbes de Cook\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Ajustement de l'espace entre les sous-plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "1. **Residials vs Fitted values**: \n",
    "This plot shows residuals against fitted values to assess model linearity. In this case, the red line isn't horizontal but in a banana form, indicating that the residuals have a quadratic pattern.\n",
    "2. **Q-Q plot**: Most points align with the diagonal, suggesting that residuals are normally distribued. Minor deviations at the tails indicate potential outliers or slight non-normality, but overall, the residuals are sufficiently normal for this model.\n",
    "3. **Scale-Location Plot**: This plot assesses homoscedasticity, ideally showing a horizontal spread of points, emphasised by the red line. ~~Again, the red line is roughly horizontal, suggesting the possibility of homoscedasticity. While mostly consistent, slight variation in spread suggests mild heteroscedasticity, implying that variance stabilization techniques could improve the model.~~\n",
    "4. **Residuals vs Leverage with Cook’s Distance**:\n",
    "This plot identifies influential observations, with Cook’s distance curves at 0.5 and 1. No points fall outside these thresholds, indicating no single observation overly influences the model. This suggests that the model is robust to individual data points.\n",
    "\n",
    "#### Conclusion \n",
    "Overall, the diagnostics show that the model follows the linear regression assumptions fairly well, with a few areas for improvement:\n",
    "- Slightly uncertain homoscedasticity (Scale-Location).\n",
    "- No major influential values (thanks to Cook's distance curves).\n",
    "\n",
    "Possible improvements could include transforming the variables to ensure homoscedasticity and exploring non-linear terms to better capture the variance of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
