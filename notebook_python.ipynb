{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import time\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "randomseed = 1234\n",
    "\n",
    "## DATA LOADING AND PREPROCESSING\n",
    "# Load the data\n",
    "gym = pd.read_csv('../../gym_members_exercise_tracking.csv')\n",
    "\n",
    "# set 'Gender', 'Workout_Type', 'Workout_Frequency (days/week)' and 'Experience_Level' as categorical\n",
    "for col in ['Gender', 'Workout_Type', 'Workout_Frequency (days/week)', 'Experience_Level']:\n",
    "    gym[col] = gym[col].astype('category')\n",
    "\n",
    "# log transform Weight and BMI\n",
    "gym['Weight (kg)'] = np.log1p(gym['Weight (kg)'])\n",
    "\n",
    "# transform 'Fat_Percentage'\n",
    "max_fat = gym['Fat_Percentage'].max()\n",
    "gym['Fat_Percentage'] = gym['Fat_Percentage'].apply(lambda x: np.sqrt(max_fat+1)-x)\n",
    "\n",
    "# rename transformed columns\n",
    "gym.rename(columns={'Weight (kg)': 'LWeight', 'Fat_Percentage': 'SFat_Percentage'}, inplace=True)\n",
    "\n",
    "gym.drop(columns=['BMI'], inplace=True)\n",
    "\n",
    "# divide into train and test set\n",
    "gym_train, gym_test = train_test_split(gym, test_size=0.2, random_state=randomseed)\n",
    "\n",
    "# Create gym_train_scale, gym_test_scale\n",
    "gym_train_scale = gym_train.copy()\n",
    "gym_test_scale = gym_test.copy()\n",
    "\n",
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "gym_train_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']] = scaler.fit_transform(gym_train_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']])\n",
    "\n",
    "gym_test_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']] = scaler.transform(gym_test_scale[['LWeight', 'Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)',\n",
    "                             'Water_Intake (liters)', 'SFat_Percentage', 'Workout_Frequency (days/week)', 'Calories_Burned']])\n",
    "\n",
    "\n",
    "# Create X_train_exp_level, X_test_exp_level, y_train_exp_level, y_test_exp_level\n",
    "X_train_exp_level = gym_train.drop(columns=['Experience_Level'])\n",
    "X_train_exp_level_scale = gym_train_scale.drop(columns=['Experience_Level'])\n",
    "y_train_exp_level = gym_train['Experience_Level']\n",
    "X_test_exp_level = gym_test.drop(columns=['Experience_Level'])\n",
    "X_test_exp_level_scale = gym_test_scale.drop(columns=['Experience_Level'])\n",
    "y_test_exp_level = gym_test['Experience_Level']\n",
    "\n",
    "# Create X_train_calories, X_test_calories, y_train_calories, y_test_calories\n",
    "X_train_calories = gym_train.drop(columns=['Calories_Burned'])\n",
    "X_train_calories_scale = gym_train_scale.drop(columns=['Calories_Burned'])\n",
    "y_train_calories = gym_train['Calories_Burned']\n",
    "X_test_calories = gym_test.drop(columns=['Calories_Burned'])\n",
    "X_test_calories_scale = gym_test_scale.drop(columns=['Calories_Burned'])\n",
    "y_test_calories = gym_test['Calories_Burned']\n",
    "\n",
    "print(\"Data loaded and preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©diction de Calories Burned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_calories_dummy = pd.get_dummies(X_train_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "X_test_calories_dummy = pd.get_dummies(X_test_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "\n",
    "X_train_calories_dummy1 = pd.get_dummies(X_train_calories, drop_first=True)\n",
    "X_test_calories_dummy1 = pd.get_dummies(X_test_calories, drop_first=True)\n",
    "# Normalisation des donn√©es - scaled = Scale + Dummies alors que scale = just scale\n",
    "X_train_calories_scaled = scaler.fit_transform(X_train_calories_dummy1)\n",
    "X_test_calories_scaled = scaler.transform(X_test_calories_dummy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gym_train.head().style.background_gradient(cmap='YlGnBu', low=0, high=0, axis=0))\n",
    "# display unique values of categorical columns\n",
    "display(gym_train.info())\n",
    "for col in gym_train.select_dtypes(include='category').columns:\n",
    "    print(col, gym_train[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gym_train.head().style.background_gradient(cmap='YlGnBu', low=0, high=0, axis=0))\n",
    "# display unique values of categorical columns\n",
    "display(gym_train.info())\n",
    "for col in gym_train.select_dtypes(include='category').columns:\n",
    "    print(col, gym_train[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mod√®le Lin√©aire : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importer les biblioth√®ques n√©cessaires\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 2. Cr√©er un mod√®le de r√©gression lin√©aire\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "\n",
    "#calcul du temps d'entra√Ænement\n",
    "import time\n",
    "start_time = time.time()\n",
    "# Entra√Ænement du mod√®le\n",
    "# 3. Entra√Æner le mod√®le sur les donn√©es d'entra√Ænement\n",
    "linear_model.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "# 4. Faire des pr√©dictions sur l'√©chantillon de test (X_test_scaled)\n",
    "#y_pred_calories = linear_model.predict(X_test_calories_scaled)\n",
    "y_pred_train_calories = linear_model.predict(X_train_calories_scaled)\n",
    "y_pred_test_calories = linear_model.predict(X_test_calories_scaled)\n",
    "# 5. √âvaluer la performance du mod√®le\n",
    "# Coefficient de d√©termination R¬≤\n",
    "r2_test = r2_score(y_test_calories, y_pred_test_calories)\n",
    "r2_train = r2_score(y_train_calories, y_pred_train_calories)\n",
    "print(f\"R¬≤ test: {r2_test}\")\n",
    "print(f\"R¬≤ train: {r2_train}\")\n",
    "\n",
    "# Erreur quadratique moyenne (MSE)\n",
    "mse = mean_squared_error(y_test_calories, y_pred_test_calories)\n",
    "print(f\"MSE: {mse}\")\n",
    "\n",
    "# 6. Afficher les coefficients du mod√®le\n",
    "print(\"Coefficients du mod√®le : \", linear_model.coef_)\n",
    "print(\"Intercept du mod√®le : \", linear_model.intercept_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train_calories, y_pred_train_calories, color='green', alpha=0.5, label='scikit-learn Regression Predictions')\n",
    "plt.plot([y_train_calories.min(), y_pred_train_calories.max()], [y_train_calories.min(), y_pred_train_calories.max()], 'k--', lw=2)\n",
    "plt.scatter(y_test_calories, y_pred_test_calories, color='blue', alpha=0.6)\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], [y_test_calories.min(), y_test_calories.max()], color='red', lw=2)  # Ligne id√©ale\n",
    "plt.xlabel(\"Calories r√©elles\")\n",
    "plt.ylabel(\"Calories pr√©dites\")\n",
    "plt.title(\"Pr√©dictions vs Valeurs r√©elles\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances du mod√®le\n",
    "\n",
    "R¬≤ = 0.978 :\n",
    "Le mod√®le explique 97.8% de la variance des calories br√ªl√©es. Cette valeur exceptionnellement √©lev√©e pourrait indiquer un surapprentissage (overfitting), surtout si le mod√®le a beaucoup de variables (18 coefficients ici).\n",
    "On remarque √©galement que les points verts (entra√Ænement) et bleus (test) semblent bien align√©s, ce qui sugg√®re une bonne performance globale du mod√®le. Toutefois, pour obtenir une analyse compl√®te, il faudrait tracer r√©sidus vs pr√©dictions pour v√©rifier la r√©partition uniforme des r√©sidus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calcul des r√©sidus\n",
    "residuals_train = y_train_calories - y_pred_train_calories\n",
    "residuals_test = y_test_calories - y_pred_test_calories\n",
    "\n",
    "# Cr√©ation d'une seule figure\n",
    "plt.figure(figsize=(8, 6))  # Ajuste la taille selon tes besoins\n",
    "\n",
    "# 1. R√©sidus vs Valeurs ajust√©es\n",
    "sns.residplot(x=y_pred_train_calories, y=residuals_train, lowess=True, \n",
    "              line_kws={'color': 'red', 'lw': 1})\n",
    "\n",
    "# Ajout de la ligne horizontale √† z√©ro\n",
    "plt.axhline(0, color='black', linestyle='dotted', alpha=0.6)\n",
    "\n",
    "# Ajout des labels et du titre\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted\")\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forme en banane dans le graphique des r√©sidus (Residuals vs Fitted) r√©v√®le une non-lin√©arit√© non captur√©e par le mod√®le.Ce qui nous indique que la valeur du score R¬≤ est trompeuse. En effet, le R¬≤ mesure la variance expliqu√©e, pas la justesse des pr√©dictions. Un mod√®le peut, donc, avoir un R¬≤ √©lev√© tout en ayant des erreurs syst√©matiques. Le mod√®le lin√©aire est inad√©quat pour capturer la vraie relation dans les donn√©es, malgr√© un R¬≤ √©lev√©. Ainsi, pour am√©liorer la g√©n√©ralisation du mod√®le et identifier les variables r√©ellement influentes, une approche de r√©gularisation s‚Äôimpose. C‚Äôest ici que la r√©gression Lasso (Least Absolute Shrinkage and Selection Operator) entre en jeu. \n",
    "\n",
    "Donc, maintenant, nous allons passer √† l‚Äôimpl√©mentation de Lasso pour voir comment il am√©liore (ou non) la robustesse du mod√®le, malgr√© les limites structurelles de la lin√©arit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord avec un lambda quelconque puis avec un lambda choisi par validation crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso\n",
    "lasso1= Lasso(alpha=10)\n",
    "# calcul du temps d'entra√Ænement\n",
    "\n",
    "start_time = time.time()\n",
    "# Entra√Ænement du mod√®le\n",
    "lasso1.fit(X_train_calories_scaled, y_train_calories)\n",
    "train_score_lasso1=lasso1.score(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "test_score_lasso1=lasso1.score(X_test_calories_scaled, y_test_calories)\n",
    "\n",
    "print(\"The train score for ls model is {}\".format(train_score_lasso1))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso1))\n",
    "# print the lasso coefficient with the name of the variable next to it\n",
    "\n",
    "\n",
    "coef_calories_lasso1 = pd.Series(lasso1.coef_, index=X_train_calories_dummy1.columns)\n",
    "coef_calories_lasso1.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du mod√®le Lasso pour Calories Burned')\n",
    "plt.show()\n",
    "# Afficher le nombre de variables conserv√©es et √©limin√©es\n",
    "print(f\"Lasso conserve {sum(coef_calories_lasso1 != 0)} variables et en supprime {sum(coef_calories_lasso1 == 0)}\")\n",
    "# print the coefficients of lasso1\n",
    "#print(\"Coefficients du mod√®le Lasso : \", coef_calories_lasso1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Appliquer Lasso avec validation crois√©e pour trouver le meilleur alpha\n",
    "#lasso = LassoCV(cv=5, random_state=1234, max_iter=10000)  # 5-fold cross-validation\n",
    "start_time = time.time()\n",
    "lasso = LassoCV(cv=5, alphas=np.array(range(1, 50, 1)) / 20., n_jobs=-1, random_state=13).fit(X_train_calories_scaled, y_train_calories)\n",
    "lasso.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "\n",
    "# Coefficient optimal alpha s√©lectionn√© par LassoCV\n",
    "optimal_alpha = lasso.alpha_\n",
    "print(f\"Optimal alpha: {optimal_alpha}\")\n",
    "\n",
    "# Coefficients du mod√®le Lasso\n",
    "coef_calories_lasso = pd.Series(lasso.coef_, index=X_train_calories_dummy1.columns)\n",
    "\n",
    "# Afficher les coefficients du mod√®le Lasso\n",
    "print(\"Coefficients du mod√®le Lasso pour Calories Burned:\")\n",
    "print(coef_calories_lasso)\n",
    "\n",
    "# Afficher le nombre de variables conserv√©es et √©limin√©es\n",
    "print(f\"Lasso conserve {sum(coef_calories_lasso != 0)} variables et en supprime {sum(coef_calories_lasso == 0)}\")\n",
    "\n",
    "# Tracer les coefficients\n",
    "coef_calories_lasso.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du mod√®le Lasso pour Calories Burned')\n",
    "plt.show()\n",
    "\n",
    "# Pr√©dictions avec le mod√®le Lasso\n",
    "y_pred_lasso = lasso.predict(X_test_calories_scaled)\n",
    "\n",
    "# Calcul de l'erreur quadratique moyenne pour √©valuer les performances du mod√®le\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_lasso_test = mean_squared_error(y_test_calories, y_pred_lasso)\n",
    "\n",
    "print(f\"Test Mean Squared Error (MSE) pour Lasso pour l'√©chantillon de test: {mse_lasso_test}\")\n",
    "\n",
    "train_score_lasso= lasso.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_lasso= lasso.score(X_test_calories_scaled, y_test_calories)\n",
    "print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances du mod√®le\n",
    "\n",
    "- On obtient un MSE = 1638.14. On a donc une l√©g√®re am√©lioration par rapport au mod√®le lin√©aire non r√©gularis√© (MSE=1679.54). Cependant, cette diff√©rence minime sugg√®re que la r√©gularisation Lasso r√©duit l√©g√®rement le surapprentissage.Toutefois, Le probl√®me fondamental de non-lin√©arit√© (forme en banane des r√©sidus) persiste, limitant les gains de performance. Ceci est visible dans le graphe des r√©sidus ci-dessous, o√π l'on observe un profil en banane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trac√© des r√©sidus\n",
    "residuals_lasso = y_test_calories - y_pred_lasso\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, residuals_lasso, color='blue', alpha=0.6)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Valeurs r√©elles\")\n",
    "plt.ylabel(\"R√©sidus\")\n",
    "plt.title(\"R√©sidus du mod√®le Lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- On a un alpha optimal = 0.8 :Une p√©nalit√© L1 relativement forte, ce qui explique pourquoi 11 variables sur 18 ont √©t√© √©limin√©es (coefficients √† z√©ro).\n",
    "\n",
    "### Interpretation des r√©sultats: \n",
    "\n",
    "#### Relation Session_Duration - Calories Burned\n",
    "- On remarque, d'apr√®s le graphe, que la variable Session_Duration domine clairement, c'est √† dire qu'une augmentation d‚Äô1 heure de la dur√©e de la s√©ance entra√Æne une augmentation pr√©dite de 243 calories br√ªl√©e. Donc, plus la s√©ance est longue, plus le corps puise dans ses r√©serves √©nerg√©tiques (glycog√®ne et lipides).\n",
    "\n",
    "Les activit√©s prolong√©es (ex : cardio, endurance) sollicitent le m√©tabolisme a√©robie, favorisant une d√©pense calorique cumulative.\n",
    "\n",
    "- Remarque: Ce coefficient √©lev√© pourrait aussi refl√©ter une corr√©lation indirecte (ex : les s√©ances longues incluent souvent des exercices intenses).\n",
    "\n",
    "#### Diff√©rence homme femme \n",
    "-  Les hommes br√ªlent 40.9 calories de plus que les femmes √† caract√©ristiques √©gales.\n",
    "    Ceci pourrait √™tre d√ª au fait que les hommes ont g√©n√©ralement une masse musculaire plus √©lev√©e, qui consomme plus de calories au repos et √† l‚Äôeffort.Les diff√©rences hormonales (testost√©rone) favorisent un m√©tabolisme √©nerg√©tique plus actif.\n",
    "\n",
    "- Remarque: Ce coefficient pourrait aussi refl√©ter des biais comportementaux (ex : les hommes choisissent des entra√Ænements plus intenses non mesur√©s dans les donn√©es)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution du MSE en fonction de lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "model = LassoCV(cv=5, alphas=np.array(range(1,200,1))/10.,n_jobs=-1,random_state=13).fit(X_train_calories_scaled, y_train_calories)\n",
    "m_log_alphas = np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "# ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='MSE moyen', linewidth=2)\n",
    "plt.axvline(np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: optimal par VC')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('log(alpha)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE de chaque validation: coordinate descent ')\n",
    "plt.show()\n",
    "#le courbe noire correspond √† la moyennes des 5 autres\n",
    "# on decoupe en 5 √©chantillons d'apprentissage d'ou les 5 courbes \n",
    "# Plot the coefficients as a function of -log(alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque une zone o√π la MSE est relativement basse et stable autour d‚Äôun certain intervalle de alpha. Puis, quand alpha devient trop grand (r√©gularisation trop forte), la MSE monte en fl√®che (le mod√®le est trop contraint, sous-apprentissage).\n",
    "\n",
    "√Ä l‚Äôoppos√©, quand alpha est trop petit, la r√©gularisation est quasi nulle : on risque un sur-apprentissage (m√™me si, parfois, la MSE peut rester relativement stable dans cette zone si le dataset n‚Äôest pas trop bruyant).\n",
    "\n",
    "Le point choisi par la validation crois√©e est un compromis : il vise √† r√©duire le nombre de coefficients non nuls (pour la parcimonie) tout en conservant une bonne performance (basse MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# Calculer le chemin du Lasso\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_train_calories_scaled, y_train_calories, alphas=np.array(range(1, 400, 1)))\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "# Styles pour les lignes\n",
    "styles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "# Log des alphas\n",
    "log_alphas_lasso = np.log10(alphas_lasso)\n",
    "\n",
    "# Tracer les coefficients\n",
    "for coef_l, s in zip(coefs_lasso, styles):\n",
    "    plt.plot(log_alphas_lasso, coef_l, linestyle=s, c='b')\n",
    "\n",
    "# Ajouter une ligne verticale pour l'alpha optimal\n",
    "plt.axvline(optimal_alpha, color='red', linestyle='--', label=f'Optimal alpha: {optimal_alpha}')\n",
    "\n",
    "# Ajouter des labels et une l√©gende\n",
    "plt.xlabel('Log(alpha)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.legend()\n",
    "plt.title('Lasso Path with Optimal Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique illustre le m√©canisme de r√©gularisation L1 propre √† la r√©gression Lasso : lorsque le param√®tre de r√©gularisation *alpha* augmente, la contrainte de parcimonie s'intensifie, conduisant progressivement les coefficients les moins informatifs vers z√©ro. Ce comportement est intrins√®que √† l'algorithme, qui privil√©gie un **mod√®le simplifi√©** (moins de variables) au d√©triment d'une l√©g√®re d√©gradation de la pr√©cision. En d'autres termes, un *alpha* √©lev√© renforce la p√©nalisation des coefficients, favorisant ainsi un **√©quilibre optimal entre simplicit√© interpr√©tative et g√©n√©ralisation**, au prix d'un biais accru. Cela traduit directement le compromis biais-variance au c≈ìur de l'optimisation du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes et std des scores pour chaque alpha\n",
    "mse_path = model.mse_path_.mean(axis=1)\n",
    "stds = model.mse_path_.std(axis=1)\n",
    "alphas = model.alphas_\n",
    "\n",
    "# Index de l'erreur minimale\n",
    "min_idx = np.argmin(mse_path)\n",
    "\n",
    "# lambda_min\n",
    "alpha_min = alphas[min_idx]\n",
    "\n",
    "# lambda_1se = plus grand alpha avec erreur ‚â§ (erreur min + 1 std)\n",
    "threshold = mse_path[min_idx] + stds[min_idx]\n",
    "alpha_1se = max(alphas[mse_path <= threshold])\n",
    "\n",
    "# Refit pour alpha_min\n",
    "lasso_min = Lasso(alpha=alpha_min, max_iter=10000)\n",
    "start_time = time.time()\n",
    "lasso_min.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement pour alpha_min : {end_time - start_time} secondes\")\n",
    "\n",
    "non_zero_min = (lasso_min.coef_ != 0).sum()\n",
    "r2_min = lasso_min.score(X_test_calories_scaled, y_test_calories)\n",
    "# Refit pour alpha_1se\n",
    "lasso_1se = Lasso(alpha=alpha_1se, max_iter=10000)\n",
    "start_time = time.time()\n",
    "lasso_1se.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement pour alpha_1se : {end_time - start_time} secondes\")\n",
    "non_zero_1se = (lasso_1se.coef_ != 0).sum()\n",
    "r2_1se = lasso_1se.score(X_test_calories_scaled, y_test_calories)\n",
    "# Affichage des r√©sultats\n",
    "print(f\"alpha_min (Œª_min) = {alpha_min:.6f}\")\n",
    "print(f\"  -> MSE moyen : {mse_path[min_idx]:.6f}\")\n",
    "print(f\"  -> √âcart-type : {stds[min_idx]:.6f}\")\n",
    "print(f\"  -> Nb variables non nulles : {non_zero_min}\")\n",
    "print(f\"  -> R¬≤ : {r2_min:.6f}\")\n",
    "# Trouver l'indice correspondant √† alpha_1se\n",
    "idx_1se = list(alphas).index(alpha_1se)\n",
    "\n",
    "print(f\"\\nalpha_1se (Œª_1se) = {alpha_1se:.6f}\")\n",
    "print(f\"  -> MSE moyen : {mse_path[idx_1se]:.6f}\")\n",
    "print(f\"  -> √âcart-type : {stds[idx_1se]:.6f}\")\n",
    "print(f\"  -> Nb variables non nulles : {non_zero_1se}\")\n",
    "print(f\"  -> R¬≤ : {r2_1se:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons \n",
    "\n",
    "a. Alpha_min (Œª_min = 0.8)\n",
    "- Pour Œª_min = 0.8 et MSE moyen = 1629.17 :\n",
    "C'est l'erreur minimale moyenne obtenue par validation crois√©e.\n",
    "C'est la valeur de  Œª qui donne les meilleures performances pr√©dictives (mod√®le le plus pr√©cis).\n",
    "\n",
    "- √âcart-type = 139.26 :\n",
    "Indique la variabilit√© des erreurs entre les folds de validation crois√©e.\n",
    "Une valeur √©lev√©e sugg√®re que le mod√®le est instable (sensible aux variations des donn√©es d'entra√Ænement).\n",
    "\n",
    "- 12 variables non nulles :\n",
    "Le mod√®le garde 12 coefficients non nuls ‚Üí mod√®le complexe mais pr√©cis.\n",
    "\n",
    "b. Alpha_1se (Œª_1se = 5.7)\n",
    "- Œª_1se = 5.7 et MSE moyen = 1764.87 (+8.3% vs Œª_min) :\n",
    "L'erreur est dans l'intervalle [Œª_min - 1SE, Œª_min + 1SE] ‚Üí consid√©r√©e comme statistiquement √©quivalente √† l'erreur minimale.\n",
    "\n",
    "- √âcart-type = 241.99 :\n",
    "Variabilit√© accrue ‚Üí le mod√®le simplifi√© est plus sensible aux fluctuations des donn√©es.\n",
    "\n",
    "- 5 variables non nulles :\n",
    "Le mod√®le √©limine 7 variables ‚Üí mod√®le interpr√©table mais potentiellement moins pr√©cis.\n",
    "\n",
    "Ces valeurs sont logiques vu que Alpha_1se represente la plus grande valeur de lambda dont l‚Äôerreur est √† moins d‚Äôun √©cart-type de l‚Äôerreur minimale(favorise un mod√®le plus simple) et que Alpha_min minimise l‚Äôerreur de validation crois√©e (MSE) (donne le meilleur ajustement possible sur les donn√©es de validation). Toutefois, nous pr√©f√©rons g√©n√©ralement afficher ces coefficients en R et pas en python. Ceci est d√ª √† l'absence de Œª_1se natif en python\n",
    "(Scikit-learn ne calcule pas automatiquement Œª_1se, contrairement √† glmnet en R)\n",
    "‚Üí Calcul manuel sujet √† des erreurs (ex: gestion des intervalles de confiance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mod√®le quadratique et ordre √©lev√©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipeline pour le Lasso avec les interactions\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(interaction_only=True, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),  # Good practice before Lasso\n",
    "    ('lasso', Lasso(max_iter=10000))\n",
    "])\n",
    "\n",
    "scoring = {\n",
    "    'r2': 'r2',\n",
    "    'neg_mse': 'neg_mean_squared_error'\n",
    "}\n",
    "\n",
    "# grille de param√®tres pour le Lasso\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3],      # Tune the interaction degree\n",
    "    'lasso__alpha': np.logspace(-2, 1, 10)  # Tune the Lasso strength\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='r2',  # On choisit sur quelle m√©trique choisir le best_estimator_\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    n_jobs=6 # run en parall√®le\n",
    ")\n",
    "\n",
    "grid.fit(X_train_calories_scaled, y_train_calories)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul du temps d'entrainement pour une configuration sp√©cifique\n",
    "\n",
    "# Cr√©ation du pipeline avec les hyperparam√®tres sp√©cifiques\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(interaction_only=True, include_bias=False, degree=2)), # Degr√© fix√© √† 2\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso', Lasso(alpha=0.1, max_iter=10000)) # Alpha fix√© √† 0.1\n",
    "])\n",
    "\n",
    "# Mesure du temps pour UNE configuration\n",
    "start_time = time.time()\n",
    "pipeline.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Temps d'entra√Ænement pour degree=2 et alpha=0.1 : {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "filtered_row = results[results['params'] == {'lasso__alpha': 1.0, 'poly__degree': 3}]\n",
    "filtered_row[['mean_test_neg_mse']]\n",
    "print(\"Best model R¬≤ (Cross Validation):\", grid.best_score_)\n",
    "print(\"Best model MSE (Cross Validation):\", -filtered_row['mean_test_neg_mse'].values[0] , \"\\n\")\n",
    "\n",
    "print(\"Best model test R¬≤:\", grid.score(X_test_calories_scaled, y_test_calories))\n",
    "print(\"Best model test MSE:\", mean_squared_error(y_test_calories, best_model.predict(X_test_calories_scaled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. R√©cup√©rer le PolynomialFeatures entra√Æn√©\n",
    "poly = best_model.named_steps['poly']\n",
    "\n",
    "# 2. R√©cup√©rer le mod√®le Lasso entra√Æn√©\n",
    "lasso = best_model.named_steps['lasso']\n",
    "\n",
    "# 3. Construire les noms des features\n",
    "feature_names = poly.get_feature_names_out(input_features=X_train_calories_dummy1.columns)\n",
    "\n",
    "# 4. Associer chaque feature √† son coefficient\n",
    "coefs = pd.Series(lasso.coef_, index=feature_names)\n",
    "\n",
    "# 5. Afficher ou trier les coefficients\n",
    "pd.set_option('display.max_rows', None)\n",
    "coefs = coefs.sort_values()\n",
    "print(coefs)\n",
    "coefs = coefs[coefs != 0]  # Garder uniquement les coefficients non nuls\n",
    "\n",
    "# 6. Plot\n",
    "coefs.plot(kind='barh', figsize=(10, 12))\n",
    "plt.title('Coefficients Lasso avec interactions')\n",
    "plt.show()\n",
    "#plot the residuals for the lasso model\n",
    "y_fitted_lasso = best_model.predict(X_train_calories_scaled)\n",
    "y_fitted_lasso_test= best_model.predict(X_test_calories_scaled)\n",
    "print(y_fitted_lasso.shape)\n",
    "print(y_test_calories.shape)\n",
    "residuals_lasso = y_train_calories  - y_fitted_lasso\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=y_fitted_lasso, y=residuals_lasso, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Valeurs r√©elles')\n",
    "plt.ylabel('R√©sidus')\n",
    "plt.title('R√©sidus du mod√®le Lasso avec interactions')\n",
    "plt.show()\n",
    "\n",
    "#print(\"Best model test R¬≤:\", grid.score(X_test_calories_scaled, y_test_calories))\n",
    "mse_lasso_quadratic_test = mean_squared_error(y_test_calories, y_fitted_lasso_test)\n",
    "\n",
    "print(f\"Test Mean Squared Error (MSE) pour Lasso pour l'√©chantillon de test: {mse_lasso_quadratic_test}\")\n",
    "#compute the score for the lasso model from the previous\n",
    "\n",
    "train_score_lasso= best_model.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_lasso= best_model.score(X_test_calories_scaled, y_test_calories)\n",
    "#print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\n",
    "\n",
    "\n",
    "#print(\"The train score for ls model is {}\".format(train_score_lasso))\n",
    "print(\"The test score for ls model is {}\".format(test_score_lasso))\n",
    "\"\"\"\n",
    "residuals_train = y_train_calories - y_pred_train_calories\n",
    "residuals_test = y_test_calories - y_pred_test_calories\n",
    "\n",
    "# Cr√©ation d'une seule figure\n",
    "plt.figure(figsize=(8, 6))  # Ajuste la taille selon tes besoins\n",
    "y_pred_calories = linear_model.predict(X_test_calories_scaled)\n",
    "y_pred_train_calories = linear_model.predict(X_train_calories_scaled)\n",
    "y_pred_test_calories = linear_model.predict(X_test_calories_scaled)\n",
    "# 1. R√©sidus vs Valeurs ajust√©es\n",
    "sns.residplot(x=y_pred_train_calories, y=residuals_train, lowess=True, \n",
    "              line_kws={'color': 'red', 'lw': 1})\n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîé **Interpr√©tation du Mod√®le Lasso avec Interactions (Polynomial + Lasso)**\n",
    "\n",
    "Ce mod√®le repose sur un encodage polynomial avec interactions uniquement (`interaction_only=True`, degr√© 3), suivi d‚Äôune r√©gularisation L1 (`Lasso`). Cela permet de capturer des **effets combin√©s non lin√©aires** tout en **√©liminant automatiquement les interactions inutiles**.\n",
    "\n",
    "#####  Performances\n",
    "- **R¬≤ test** = **0.993**, **MSE test** ‚âà **542**\n",
    "- Gain substantiel par rapport au Lasso simple (R¬≤ ‚âà 0.979, MSE ‚âà 1638)\n",
    "- Ce mod√®le capture donc beaucoup mieux la complexit√© des relations entre variables.\n",
    "\n",
    "#### Temps de Calcul (1 minute)\n",
    "Complexit√© justifi√©e : Bien que plus lent qu‚Äôun mod√®le lin√©aire (quelques secondes), le gain en performance valide l‚Äôutilisation d‚Äôun mod√®le quadratique.\n",
    "\n",
    "Optimisation : Le Lasso r√©duit la complexit√© en √©liminant les termes non pertinents, √©quilibrant pr√©cision et parcimonie.\n",
    "#####  Interpr√©tation des principales interactions retenues\n",
    "\n",
    "Les **coefficients positifs** indiquent des interactions qui **augmentent** la pr√©diction de `Calories_Burned`, et les **n√©gatifs** celles qui la **diminuent** :\n",
    "\n",
    "---\n",
    "\n",
    "##### Quelques interactions dominantes positives :\n",
    "\n",
    "- **`Avg_BPM √ó Session_Duration`** ‚Üí **+21.44**\n",
    "  > Synergie intensit√©/dur√©e : les longues s√©ances √† haut BPM amplifient la d√©pense calorique (effet non-lin√©aire critique).\n",
    "\n",
    "- **`Session_Duration (hours) Gender_Male`** ‚Üí **+11.42**\n",
    "  > Les hommes tirent un b√©n√©fice calorique suppl√©mentaire des sessions longues, possiblement gr√¢ce √† une endurance musculaire sup√©rieure.\n",
    "\n",
    "##### Quelques interactions dominantes n√©gatives :\n",
    "\n",
    "- **`Age √ó Session_Duration`** ‚Üí **‚àí10.6**\n",
    "  > √Ä dur√©e d'entrainement √©quivalents, l'√¢ge **r√©duit fortement** la d√©pense calorique. Cela confirme et approfondit l‚Äôeffet observ√© dans les PDP, en le liant au BPM et √† la dur√©e. Un marqueur indirect tr√®s probable du **d√©clin m√©tabolique d√ª au vieillissement**.\n",
    "\n",
    "- **`Age √ó Avg_BPM`** ‚Üí **‚àí2.35**\n",
    "  >  √Ä fr√©quence cardiaque √©quivalente, les seniors br√ªlent moins, possiblement d√ª √† une VO‚ÇÇ max (d√©bit maximum d'oxyg√®ne) r√©duite.\n",
    "\n",
    "\n",
    "#### Conclusion \n",
    "\n",
    "> *Le mod√®le polynomial r√©gularis√© par Lasso am√©liore significativement la pr√©diction (R¬≤ ‚âà 0.993, MSE ‚âà 571), en capturant des effets d‚Äôinteractions complexes entre l‚Äô√¢ge, l‚Äôintensit√© de l‚Äôeffort, la dur√©e des s√©ances et certaines caract√©ristiques morphologiques (poids, sexe). Contrairement au Lasso simple ou au mod√®le lin√©aire, cette approche met en √©vidence des synergies physiologiques r√©alistes, comme la chute d‚Äôefficacit√© m√©tabolique li√©e √† l‚Äô√¢ge ou l‚Äôimpact combin√© du sexe et de la charge cardiaque. Cette complexit√© justifie le recours √† un mod√®le non lin√©aire, √† la fois performant et interpr√©table.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous √©tudierons bri√®vement l'effet d'une p√©nalisation plus stricte sur le mod√®le via Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1) Instanciation sans n_jobs ni random_state\n",
    "start_time = time.time()\n",
    "ridgereg = RidgeCV(alphas=np.arange(1, 50) / 20., cv=5)\n",
    "\n",
    "# 2) Entra√Ænement\n",
    "\n",
    "ridgereg.fit(X_train_calories_scaled, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "# 3) Alpha optimal\n",
    "optimal_alpha = ridgereg.alpha_\n",
    "print(f\"Optimal alpha: {optimal_alpha}\")\n",
    "\n",
    "# 4) Coefficients\n",
    "coef_calories_ridge = pd.Series(ridgereg.coef_, index=X_train_calories_dummy1.columns)\n",
    "print(\"Coefficients du mod√®le Ridge pour Calories Burned:\")\n",
    "print(coef_calories_ridge)\n",
    "\n",
    "# 5) Comme Ridge ne met quasiment jamais un coefficient strictement √† 0, \n",
    "#    le comptage ¬´ conserv√© / supprim√© ¬ª n‚Äôest pas tr√®s significatif, mais :\n",
    "print(f\"Nombre de coefficients non nuls : {sum(coef_calories_ridge != 0)}\")\n",
    "\n",
    "# 6) Trac√©\n",
    "coef_calories_ridge.sort_values().plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Coefficients du mod√®le Ridge pour Calories Burned')\n",
    "plt.show()\n",
    "\n",
    "# 7) Pr√©diction et MSE\n",
    "y_pred_ridge = ridgereg.predict(X_test_calories_scaled)\n",
    "mse_ridge = mean_squared_error(y_test_calories, y_pred_ridge)\n",
    "print(f\"MSE pour Ridge : {mse_ridge:.4f}\")\n",
    "\n",
    "# 8) R¬≤ (score) entra√Ænement et test\n",
    "train_score_ridge = ridgereg.score(X_train_calories_scaled, y_train_calories)\n",
    "test_score_ridge  = ridgereg.score(X_test_calories_scaled,  y_test_calories)\n",
    "print(f\"Train R¬≤ pour Ridge : {train_score_ridge:.4f}\")\n",
    "print(f\"Test  R¬≤ pour Ridge : {test_score_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mod√®le Ridge obtient un MSE de 1 661,23, un R¬≤ entra√Ænement de 0,9791 et un R¬≤ test de 0,9787. Ce MSE l√©g√®rement plus √©lev√© que celui du Lasso s‚Äôexplique par une p√©nalisation Œª* plus forte : Ridge r√©partit son effet de r√©gularisation sur toutes les variables (biais mod√©r√© mais constant), alors que le Lasso, avec un Œª optimal plus faible, parvient √† conserver un ajustement un peu plus pr√©cis.\n",
    "\n",
    "Cependant, les performances des deux mod√®les lin√©aires restent tr√®s proches :\n",
    "\n",
    "Lasso (Œª_min) : MSE test ‚âÉ 1 638,14, R¬≤ test ‚âÉ 0,9790\n",
    "\n",
    "Ridge : MSE test ‚âÉ 1 661,23, R¬≤ test ‚âÉ 0,9787\n",
    "\n",
    "Enfin, le Lasso quadratique (avec interactions) surpasse nettement ces deux approches lin√©aires, avec un MSE test ‚âÉ 570,61 et un R¬≤ test ‚âÉ 0,9927, gr√¢ce √† sa capacit√© √† capturer des relations non lin√©aires entre les variables mais reste un peu plus lent niveau temps d'entrainement \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apr√®s avoir analys√© les performances du mod√®le Lasso et de Ridge et identifi√© l'alpha optimal pour r√©gulariser notre r√©gression, nous allons maintenant explorer une approche alternative en utilisant la r√©gression par vecteurs de support (SVR) afin de comparer ses performances et sa capacit√© √† capturer des relations potentiellement non lin√©aires dans les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR SUR CALORIES BURNED : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#calibrage des param√®tres c et gamma\n",
    "\n",
    "param = [{'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']}]\n",
    "param_lin_opt= GridSearchCV(SVR(),param,refit=True,verbose=3)\n",
    "start_time = time.time()\n",
    "param_lin_opt.fit(X_train_calories_scaled,y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "print(param_lin_opt.best_params_)\n",
    "start_time = time.time()\n",
    "y_pred_svr_lin = param_lin_opt.predict(X_test_calories_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"Temps de pr√©diction : {end_time - start_time} secondes\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_lin, color='darkorange', label='Pr√©dictions vs R√©elles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite pr√©diction')\n",
    "plt.xlabel('Valeurs R√©elles (Calories_Burned)')\n",
    "plt.ylabel('Pr√©dictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR Lin√©aire - Comparaison Pr√©dictions/R√©elles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_score_lin= r2_score(y_test_calories,y_pred_svr_lin)\n",
    "print(f\"R¬≤ pour SVR lin: {R2_score_lin}\")\n",
    "mse_svr_lin = mean_squared_error(y_test_calories, y_pred_svr_lin)\n",
    "print(f\"MSE pour SVR poly: {mse_svr_lin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rbf=[{'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}]\n",
    "parmopt_rbf = GridSearchCV(SVR(), param_rbf, refit = True, verbose = 3)\n",
    "parmopt_rbf.fit(X_train_calories_scaled, y_train_calories)\n",
    "print(parmopt_rbf.best_params_)\n",
    "start_time = time.time()\n",
    "y_pred_svr_rbf = parmopt_rbf.predict(X_test_calories_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"Temps de pr√©diction : {end_time - start_time} secondes\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_rbf, color='darkorange', label='Pr√©dictions vs R√©elles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite pr√©diction')\n",
    "plt.xlabel('Valeurs R√©elles (Calories_Burned)')\n",
    "plt.ylabel('Pr√©dictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR rbf - Comparaison Pr√©dictions/R√©elles')\n",
    "plt.show()\n",
    "\n",
    "R2_score_rbf= r2_score(y_test_calories,y_pred_svr_rbf)\n",
    "print(f\"R¬≤ pour SVR rbf: {R2_score_rbf}\")\n",
    "mse_svr_rbf = mean_squared_error(y_test_calories, y_pred_svr_rbf)\n",
    "print(f\"MSE pour SVR rbf: {mse_svr_rbf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_poly=[{'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['poly']}]\n",
    "parmopt_poly = GridSearchCV(SVR(), param_poly, refit = True, verbose = 3)\n",
    "time_start = time.time()\n",
    "parmopt_poly.fit(X_train_calories_scaled, y_train_calories)\n",
    "time_end = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {time_end - time_start} secondes\")\n",
    "print(parmopt_poly.best_params_)\n",
    "\n",
    "y_pred_svr_poly = parmopt_poly.predict(X_test_calories_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_calories, y_pred_svr_poly, color='darkorange', label='Pr√©dictions vs R√©elles')\n",
    "plt.plot([y_test_calories.min(), y_test_calories.max()], \n",
    "         [y_test_calories.min(), y_test_calories.max()], \n",
    "         color='navy', lw=2, linestyle='--', label='Parfaite pr√©diction')\n",
    "plt.xlabel('Valeurs R√©elles (Calories_Burned)')\n",
    "plt.ylabel('Pr√©dictions (Calories_Burned)')\n",
    "plt.legend()\n",
    "plt.title('SVR poly - Comparaison Pr√©dictions/R√©elles')\n",
    "plt.show()\n",
    "\n",
    "R2_score_poly= r2_score(y_test_calories,y_pred_svr_poly)\n",
    "print(f\"R¬≤ pour SVR poly: {R2_score_poly}\")\n",
    "mse_svr_poly = mean_squared_error(y_test_calories, y_pred_svr_poly)\n",
    "print(f\"MSE pour SVR poly: {mse_svr_poly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Performances des diff√©rents noyaux SVR**\n",
    "\n",
    "a. SVR avec noyau RBF (Radial Basis Function)\n",
    "R¬≤ = 0,992 et MSE = 636,57\n",
    "\n",
    "=> Le mod√®le RBF parvient √† expliquer 99,2 % de la variance des calories br√ªl√©es, avec une erreur quadratique moyenne extr√™mement basse.\n",
    "Ceci est d√ª au fait que le noyau RBF est capable de capturer des relations non lin√©aires complexes (par exemple, l‚Äôinteraction entre la dur√©e de s√©ance et la fr√©quence cardiaque moyenne). Les hyperparam√®tres C (r√©gularisation) et gamma (√©tendue d‚Äôinfluence) ont √©t√© optimis√©s via GridSearchCV, garantissant un compromis id√©al entre biais et variance.\n",
    "\n",
    "b. SVR avec noyau lin√©aire\n",
    "R¬≤ = 0,977 et MSE = 1 790,89\n",
    "\n",
    "=> Le SVR lin√©aire offre √©galement de bonnes performances , mais nettement inf√©rieures au noyau RBF (erreur MSE beaucoup plus √©lev√©e).\n",
    "Ceci pourrait √™tre d√ª au fait que\n",
    "\n",
    "c. SVR avec noyau polynomial\n",
    "R¬≤ = 0,949 et MSE = 3 952,21\n",
    "\n",
    "=> Les r√©sultats sont bien plus faibles, avec une erreur environ 6 fois sup√©rieure √† celle du RBF.\n",
    "\n",
    "\n",
    "2. **Comparaison des Performances : Lasso Quadratique vs SVR RBF**\n",
    "\n",
    "| Crit√®re               | Lasso Quadratique (Interactions) | SVR RBF              |\n",
    "|-----------------------|-----------------------------------|----------------------|\n",
    "| **MSE (Test)**        | **570.61**                        | 636.57              |\n",
    "| **R¬≤ (Test)**         | **0.9927**                        | 0.992               |\n",
    "| **Complexit√©**        | Mod√®le lin√©aire avec interactions | Mod√®le non lin√©aire |\n",
    "| **Interpr√©tabilit√©**  | Coefficients explicables          | \"Bo√Æte noire\"       |\n",
    "| **Flexibilit√©**       | Capte interactions sp√©cifiques    | Adapt√© aux relations complexes/g√©n√©riques |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Points Cl√©s :**\n",
    "1. **Performance Pr√©dictive** :  \n",
    "   - Le **Lasso Quadratique** est l√©g√®rement meilleur en MSE (+10% d'erreur pour SVR RBF).  \n",
    "   - Les deux mod√®les ont un R¬≤ quasi identique (> 0.99), indiquant une explication quasi parfaite de la variance.\n",
    "\n",
    "2. **Equilibre Complexit√©/Interpr√©tabilit√©** :  \n",
    "   - **Lasso Quadratique** : Moins flexible mais interpr√©table (coefficients des interactions analysables).  \n",
    "   - **SVR RBF** : Plus flexible mais difficile √† expliquer (d√©pend de la fonction noyau).\n",
    "\n",
    "3. **Choix du mod√®le** :  \n",
    "   - **Lasso Quadratique** : Si l‚Äôon privil√©gie l‚Äôerreur quadratique minimale et la parcimonie  \n",
    "   - **SVR RBF** : Si l‚Äôon recherche avant tout la flexibilit√© pour capter des structures non-lin√©aires plus subtiles\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbres et Forest al√©atoires\n",
    "### Arbre de d√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_calories_dummy = pd.get_dummies(X_train_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "X_test_calories_dummy = pd.get_dummies(X_test_calories, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "# Normalisation des donn√©es - scaled = Scale + Dummies alors que scale = just scale\n",
    "X_train_calories_scaled = scaler.fit_transform(X_train_calories_dummy)\n",
    "X_test_calories_scaled = scaler.transform(X_test_calories_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit a regression tree model for Calories_Burned using dummy variables\n",
    "tree_reg_cal = DecisionTreeRegressor(random_state=randomseed, ccp_alpha=0.001)\n",
    "start_time = time.time()\n",
    "tree_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'entra√Ænement : {end_time - start_time} secondes\")\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(18, 12))\n",
    "plot_tree(tree_reg_cal, feature_names=X_train_calories_dummy.columns, filled=True, rounded=True, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Compute MSE and R2 on training and test sets\n",
    "y_train_pred = tree_reg_cal.predict(X_train_calories_dummy)\n",
    "y_test_pred = tree_reg_cal.predict(X_test_calories_dummy)\n",
    "\n",
    "mse_train = mean_squared_error(y_train_calories, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test_calories, y_test_pred)\n",
    "r2_train = r2_score(y_train_calories, y_train_pred)\n",
    "r2_test = r2_score(y_test_calories, y_test_pred)\n",
    "\n",
    "print(\"MSE on training set: \", mse_train)\n",
    "print(\"MSE on test set: \", mse_test)\n",
    "print(\"R2 on training set: \", r2_train)\n",
    "print(\"R2 on test set: \", r2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Nous avons initialement construit un arbre de r√©gression avec un param√®tre de complexit√© extr√™mement faible (`cp = 0.01`). Comme attendu, ce mod√®le pr√©sente une structure profond√©ment ramifi√©e, caract√©ristique d'un sur-apprentissage. Ce mod√®le pr√©sente queasiment aucun biais sur le jeu d‚Äôentra√Ænement (R¬≤ = 0.999, MSE = 0.027), mais un √©cart significatif entre l‚Äôerreur d‚Äôentra√Ænement et de test (MSE_test = 4484) r√©v√®le un sur-apprentissage. Toutefois, le R¬≤ sur le test reste √©lev√© (0.934), indiquant que le mod√®le capture une part substantielle de la variance explicative, malgr√© sa complexit√© excessive. Le mod√®le d'arbre en Python est plus complexe qu'en R alors que nous utilisons un cp plus √©lev√© (`cp=0.01`), tandis que R utilise un `cp=0.001`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for best cp \n",
    "\n",
    "scoring = {\n",
    "    'r2': 'r2',\n",
    "    'neg_mse': 'neg_mean_squared_error'\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'ccp_alpha': np.logspace(-4, 2, 15)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(tree_reg_cal, params, scoring=scoring, cv=5, refit='r2', n_jobs=-1)\n",
    "grid.fit(X_train_calories_dummy, y_train_calories)\n",
    "grid_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# plot the results as a function of ccp_alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(grid_results['param_ccp_alpha'], grid_results['mean_test_neg_mse'] * -1, label='MSE', marker='o')\n",
    "plt.xlabel('Complexity Parameter (alpha)')\n",
    "plt.ylabel('Cross-Validation Error')\n",
    "plt.title('Cross-Validation Error vs Complexity Parameter')\n",
    "plt.grid(True, which=\"both\", ls=\"-\")\n",
    "\n",
    "optimal_alpha = np.argmin(grid_results['mean_test_neg_mse'] * -1)\n",
    "plt.axvline(grid_results['param_ccp_alpha'][optimal_alpha], color='red', linestyle='--', label='Optimal alpha')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(18, 12))\n",
    "tree_reg_cal_optimal = grid.best_estimator_\n",
    "plot_tree(tree_reg_cal_optimal, feature_names=X_train_calories_dummy.columns, filled=True, rounded=True, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# display the dataframe with top 5 results from mean_test_neg_mse\n",
    "display(grid_results[['param_ccp_alpha', 'mean_test_neg_mse', 'std_test_neg_mse', 'rank_test_neg_mse']].sort_values(by='mean_test_neg_mse', ascending=False).head(5))\n",
    "# same for r2\n",
    "\n",
    "display(grid_results[['param_ccp_alpha', 'mean_test_r2', 'std_test_r2', 'rank_test_r2']].sort_values(by='mean_test_r2', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Par validation crois√©e, nous avons d√©termin√© que le meilleur param√®tre d'√©lagage est `ccp ‚âà 5.18`. L'arbre de regression r√©sultant est moins complexe que le pr√©c√©dent, mais est encore trop ramifi√©, comme celui de R. Ce mod√®le est un peu moins performant que celui de R, avec un MSE calcul√© par cross-validation 5-fold de 5017 ici contre 4521 pour le mod√®le de R. Le R¬≤ est similaire dans les deux langages (~0.93) en revanche. Cela souligne que le mod√®le de r√©gression est tout de m√™me robuste, malgr√© la complexit√© de l'arbre.\n",
    "\n",
    "Nous allons pouvoir explorer d'autres m√©thodes d'arbres de d√©cision, comme les for√™ts al√©atoires et le boosting, qui sont souvent plus performantes que les arbres de d√©cision simples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For√™ts al√©atoires\n",
    "\n",
    "#### Simple random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Cr√©er et entra√Æner une for√™t al√©atoire\n",
    "rf_reg_cal = RandomForestRegressor(random_state=randomseed, oob_score=True)\n",
    "rf_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "\n",
    "# Pr√©dictions sur les ensembles d'entra√Ænement et de test\n",
    "y_train_pred_rf = rf_reg_cal.predict(X_train_calories_dummy)\n",
    "y_test_pred_rf = rf_reg_cal.predict(X_test_calories_dummy)\n",
    "\n",
    "# Calculer le MSE et le R2\n",
    "mse_train_rf = mean_squared_error(y_train_calories, y_train_pred_rf)\n",
    "mse_test_rf = mean_squared_error(y_test_calories, y_test_pred_rf)\n",
    "r2_train_rf = r2_score(y_train_calories, y_train_pred_rf)\n",
    "r2_test_rf = r2_score(y_test_calories, y_test_pred_rf)\n",
    "\n",
    "print(\"Random Forest - OOB score :\", rf_reg_cal.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Le mod√®le de base basique al√©atoire de `scikit-learn` est construit avec 100 arbres, avec les param√®tres `min_samples_split = 2` (nombre minimum d'√©lements pour consid√©rer une d√©cision) et `min_samples_leaf = 1` (nombre minimum d'√©lement dans une feuille). Ces param√®tres sont les valeurs par d√©faut de `scikit-learn`, mais nous allons les optimiser par la suite. \n",
    "\n",
    "Le mod√®le est construit avec un √©chantillonnage bootstrap, ce qui signifie que chaque arbre est construit sur un sous-ensemble al√©atoire des donn√©es d'entra√Ænement. Cela nous permet d'extraire l'erreur OOB qui est calcul√© par d√©faut avec le score R¬≤ dans `scikit-learn`, alors qu'en R, elle est traditionnellement √©valu√©e via la somme des r√©sidus au carr√© (RSS, Residual Sum of Squares).\n",
    "\n",
    "Contrairement √† R ou le param√®tre √† optimiser est `mtry` (nombre de variables consid√©r√©es √† chaque split), `scikit-learn` nous permet d'optimiser plusieurs hyperparam√®tres essentiels :\n",
    "- **`max_depth`** : la profondeur maximale de chaque arbre (plus un arbre est profond, plus il peut mod√©liser des interactions complexes, mais aussi surapprendre). \n",
    "- **`min_samples_split`** : le nombre minimum d'√©chantillons requis pour diviser un noeud. Plus il est grand, plus l‚Äôarbre est contraint et moins il risque de surapprendre.\n",
    "- **`min_samples_leaf`** : le nombre minimum d'√©chantillons n√©cessaires dans une feuille terminale. Cela permet d‚Äô√©viter des feuilles trop petites, ce qui am√©liore la robustesse.\n",
    "- **`max_features`** : le nombre maximal de variables consid√©r√©es pour chercher le meilleur split √† chaque division (√©quivalent au `mtry` de R). Peut √™tre fix√© √† un nombre entier, √† une proportion de la taille du sample (`float` entre 0 et 1), ou aux valeurs pr√©d√©finies `'sqrt'` : $\\sqrt{n_\\text{variables}}$ ou `'log2'` : $\\log_2(n_\\text{variables})$.\n",
    "- **`max_leaf_nodes`** : limite le nombre total de feuilles de l‚Äôarbre, for√ßant une structure plus simple.\n",
    "- **`ccp_alpha`** : le param√®tre de co√ªt-complexit√© pour l'√©lagage (post-pruning) ; plus `ccp_alpha` est grand, plus l'√©lagage sera fort.\n",
    "\n",
    "Enfin, il nous est √©galement permis de choisir le **crit√®re d‚Äô√©valuation** de la qualit√© du split (`criterion`).   \n",
    "Alors qu‚Äôen R, la performance est √©valu√©e via le **RSS** (Residual Sum of Squares), l‚Äôoption la plus proche disponible dans `scikit-learn` est `friedman_mse`, con√ßue pour optimiser la variance r√©siduelle de mani√®re similaire au RSS.  \n",
    "Ici, nous avons l'occasion de comparer l'impact du choix du crit√®re (`friedman_mse` vs `squared_error`) sur la construction des arbres.  \n",
    "\n",
    "Nous observerons notamment l'effet sur la performance de g√©n√©ralisation (via le score OOB R¬≤) ainsi que sur le temps d'apprentissage et d'√©lagage.\n",
    "Le score OOB √©tant uniquement calcul√© sur la m√©trique R¬≤ sous `scikit-learn`, le mod√®le optimal ne sera pas directement comparable aux mesures obtenues en R (RSS).\n",
    "\n",
    "Par ailleurs, ces hyperparam√®tres **sont interd√©pendants** : en pratique, optimiser l'hyperparam√®tre `max_leaf_nodes` peut r√©duire la n√©cessit√© d'√©laguer l'arbre, ou la n√©c√©ssit√© de d√©finir `max_depth`. \n",
    "\n",
    "Nous avons d√©cid√© de construire un mod√®le de for√™t al√©atoire avec les param√®tres par d√©faut et optimiser les hyperparam√®tres `n_estimators` et `max_features` ainsi que le param√®tre `ccp_alpha` pour l'√©lagage, que nous avons vu en cours, mais que nous avons pas appliqu√© dans le mod√®le de R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest avec √©lagage\n",
    "\n",
    "Comme les for√™ts al√©atoires sont construits avec un √©chantillonnage bootstrap, nous pouvons estimer l'**erreur OOB (Out-Of-Bag) pour √©valuer la performance du mod√®le**. Ainsi nous n'avons pas besoin d'utiliser la validation crois√©e pour √©valuer le mod√®le et d√©terminer les meilleurs hyperparam√®tres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# D√©finir le grille de param√®tres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': np.linspace(0.1, 1.0, 10),  # proportion du nombre total de variables\n",
    "    'ccp_alpha': [0.01, 0.1, 1.0, 5.0, 10.0],\n",
    "    'criterion': ['friedman_mse', 'squared_error'],  # Comparer plusieurs crit√®res !\n",
    "    'oob_score': [True],\n",
    "}\n",
    "\n",
    "# G√©n√©rer toutes les combinaisons possibles\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# Fonction pour entra√Æner et √©valuer\n",
    "def train_and_evaluate(params):\n",
    "    model = RandomForestRegressor(random_state=randomseed, **params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_calories_dummy, y_train_calories)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'n_estimators': params['n_estimators'],\n",
    "        'max_features': params['max_features'],\n",
    "        'ccp_alpha': params['ccp_alpha'],\n",
    "        'criterion': params['criterion'],\n",
    "        'oob_score': model.oob_score_,\n",
    "        'training_time_sec': elapsed_time,\n",
    "    }\n",
    "\n",
    "# Parall√©liser\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_and_evaluate)(params) for params in param_combinations\n",
    ")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Trier par oob_score d√©croissant\n",
    "results_df = results_df.sort_values(by='oob_score', ascending=False)\n",
    "\n",
    "# Afficher\n",
    "# display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_max_features = [0.9, 0.6, 0.4, 0.1]\n",
    "selected_ccp_alpha = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# Cr√©er 4 sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))  # 2x2 grid\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, alpha in enumerate(selected_ccp_alpha):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Sous-ensemble des r√©sultats pour ce ccp_alpha\n",
    "    subset = results_df[results_df['ccp_alpha'] == alpha]\n",
    "    \n",
    "    for max_feat in selected_max_features:\n",
    "        # Prendre uniquement les lignes correspondant √† un max_features donn√©\n",
    "        curve = subset[np.isclose(subset['max_features'], max_feat)]\n",
    "        # Trier par n_estimators pour des courbes bien propres\n",
    "        curve = curve.sort_values('n_estimators')\n",
    "        \n",
    "        ax.plot(curve['n_estimators'], curve['oob_score'], marker='o', label=f'max_features = {max_feat}')\n",
    "    \n",
    "    ax.set_title(f'OOB Score vs n_estimators (ccp_alpha = {alpha})')\n",
    "    ax.set_xlabel('n_estimators')\n",
    "    ax.set_ylabel('OOB R¬≤ Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parmi les 100 meilleures combinaisons, sortir les 10 plus longues √† fitter et les 10 plus courtes\n",
    "best_results_df = results_df[results_df['oob_score'] > 0.974].sort_values(by='training_time_sec', ascending=False).copy()\n",
    "display(best_results_df.head(10))\n",
    "\n",
    "display(best_results_df.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Interpr√©tation des r√©sulats de la for√™t al√©atoire** :\n",
    "\n",
    "Nous avons r√©alis√© une analyse fine de la performance de la for√™t al√©atoire en fonction de plusieurs hyperparam√®tres (`n_estimators`, `max_features`, `ccp_alpha`), en nous concentrant sur l'estimation de l'erreur de g√©n√©ralisation via l'**OOB score**.\n",
    "\n",
    "$\\rightarrow$ **Influence du crit√®re de split (`criterion`)**\n",
    "\n",
    "En observant le tableau des r√©sultats, nous constatons que **le choix du crit√®re `friedman_mse` ou `squared_error` n‚Äôimpacte pratiquement pas la performance du mod√®le**.  \n",
    "Que ce soit en termes de **score OOB** ou de **temps d'entra√Ænement**, les deux crit√®res m√®nent aux **m√™mes choix optimaux d'hyperparam√®tres**, avec des performances quasi-identiques.  \n",
    "Cela montre que, dans le cas de la for√™t al√©atoire, **le crit√®re de construction locale des arbres influence peu la qualit√© globale du mod√®le**.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Influence de `max_features`**\n",
    "\n",
    "Comme nous l'avions observ√© lors de la mod√©lisation sous R, **plus la proportion de variables s√©lectionn√©es √† chaque split est √©lev√©e, meilleure est la performance de la for√™t**.  \n",
    "Ici, c'est avec `max_features = 0.9` que nous obtenons les meilleurs scores OOB.\n",
    "\n",
    "En proposant davantage de variables au moment de cr√©er les divisions, chaque arbre a acc√®s √† plus d'information pour produire des splits efficaces, ce qui am√©liore la qualit√© globale de la for√™t.\n",
    "\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Influence de `ccp_alpha` (√©lagage)**\n",
    "\n",
    "L'√©lagage, contr√¥l√© via le param√®tre `ccp_alpha`, **semble avoir un effet n√©gligeable sur la performance OOB**.\n",
    "\n",
    "Quelle que soit la valeur choisie (0.01, 0.1, 1.0, 10.0), l'OOB score reste quasiment stable.  \n",
    "Cela indique que **le mod√®le est naturellement robuste** et peu sensible au surapprentissage, m√™me sans √©lagage agressif.\n",
    "\n",
    "Cela confirme l'intuition classique en for√™t al√©atoire : **l'overfitting n'est pas un probl√®me majeur** gr√¢ce √† l'agr√©gation de nombreux arbres faibles.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Performances extr√™mes (meilleur mod√®le)**\n",
    "\n",
    "- Le **meilleur mod√®le** atteint un **OOB score** de **0.975666** et a n√©cessit√© **5.211 secondes** pour √™tre entra√Æn√©.\n",
    "- Ce mod√®le utilise :\n",
    "  - `n_estimators = 500`\n",
    "  - `max_features = 1.0`\n",
    "  - `ccp_alpha = 1.0`\n",
    "  - `criterion = friedman_mse`\n",
    "\n",
    "\n",
    "$\\rightarrow$ **Trade-off performance/temps**\n",
    "\n",
    "Parmi les mod√®les ayant un OOB score > 0.974, **le plus rapide** a pris seulement **0.897 secondes** pour un OOB score de **0.974343** (`n_estimators=100`, `max_features=0.8`, `ccp_alpha=0.10`).\n",
    "\n",
    "Cela montre que **des mod√®les plus l√©gers peuvent offrir des performances presque √©quivalentes** tout en √©tant **beaucoup plus rapides** √† entra√Æner.\n",
    "\n",
    "\n",
    "$\\rightarrow$ **D√©tail des mod√®les extr√™mes**\n",
    "\n",
    "- **Top 10 mod√®les les plus longs √† entra√Æner** (extraits du tableau) : majoritairement avec `n_estimators = 500`.\n",
    "- **Top 10 mod√®les les plus rapides** : configurations avec `n_estimators = 100` et `max_features` entre 0.8 et 1.0.\n",
    "\n",
    "Cela est coh√©rent avec l'id√©e que **plus le nombre d'arbres est √©lev√©, plus le temps d'entra√Ænement augmente**.\n",
    "\n",
    "--- \n",
    "\n",
    "**Conclusion** :\n",
    "\n",
    "Dans l'ensemble, nous constatons que :\n",
    "- **Un `max_features` √©lev√©** permet d'am√©liorer significativement la performance du mod√®le.\n",
    "- **Le param√®tre `ccp_alpha` (√©lagage) impacte tr√®s peu la qualit√© de la for√™t**.\n",
    "- **R√©duire `n_estimators`** permet **d‚Äôacc√©l√©rer consid√©rablement** l'entra√Ænement sans perte substantielle de performance.\n",
    "- **La for√™t al√©atoire reste robuste** face au surapprentissage, m√™me avec des arbres profonds et peu √©lagu√©s.\n",
    "\n",
    "  \n",
    "Apr√®s avoir valid√© ces r√©sultats, nous allons d√©sormais nous int√©resser √† **l‚Äôimportance des variables**, afin d‚Äôidentifier les facteurs les plus influents dans la pr√©diction des calories, comme nous l'avions fait sous R.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the best random forest model \n",
    "\n",
    "best_rf_reg_cal = RandomForestRegressor(random_state=randomseed, n_estimators=500, max_features=0.9, ccp_alpha=0.01, criterion='friedman_mse', oob_score=True)\n",
    "best_rf_reg_cal.fit(X_train_calories_dummy, y_train_calories)\n",
    "\n",
    "# extract variable importance\n",
    "importances = best_rf_reg_cal.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train_calories_dummy.columns[indices]\n",
    "importances = importances[indices]\n",
    "importances_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importances_df['Cumulative Importance'] = importances_df['Importance'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(importances_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_df,\n",
    "    palette='cool',\n",
    ")\n",
    "plt.title(\"Variable Importance from Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : √Ä partir du mod√®le de for√™t al√©atoire optimal entra√Æn√© sous `scikit-learn`, nous avons extrait l'importance des variables bas√©e sur la r√©duction de l'impuret√© cumul√©e (Gini importance). \n",
    "\n",
    "- Le pr√©dicateur `Session_Duration (hours)` domine, expliquant 73.67% de la variance, ce qui est intuitif puisqu'une **session plus longue** implique m√©caniquement **une d√©pense √©nerg√©tique plus √©lev√©e**.\n",
    "- Il est suivi par `Avg_BPM`, qui contribue √† 10.56% de la variance, ce qui est √©galement logique car un rythme cardiaque lors d'une s√©ane de sport plus √©lev√© est souvent associ√© √† une **d√©pense calorique accrue**.\n",
    "- Enfin, `SFat_Percentage`, `Experience_Level` et `Age` ont des contributions faibles, mais permettent de capter des interactions int√©ressantes et am√©liorent la performance globale du mod√®le.\n",
    "\n",
    "On observe ainsi que 5 variables expliquent √† elles seules plus de **97 % de l'importance totale du mod√®le**.\n",
    "\n",
    "En revanche, sous R, les variables `Session_Duration (hours)` et `Avg_BPM` √©taient les seules √† ressortir comme les plus importantes, tandis que toutes les autres variables avaient une importance tr√®s faible. Ainsi, on peut d√©duire que `scikit-learn` ne construit pas les for√™ts al√©atoires de la m√™me mani√®re que `caret` sous R.\n",
    "\n",
    "Nous allons maintenant nous int√©resser √† un autre algorithme d'arbres de d√©cision, le **boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Gradient Boosting & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# D√©finir le nombre de folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=randomseed)\n",
    "\n",
    "# Stocker les scores et temps\n",
    "r2_scores_gb = []\n",
    "mse_scores_gb = []\n",
    "times_gb = []\n",
    "\n",
    "r2_scores_xgb = []\n",
    "mse_scores_xgb = []\n",
    "times_xgb = []\n",
    "\n",
    "# Boucle sur les folds\n",
    "for train_index, val_index in kf.split(X_train_calories):\n",
    "    X_train_fold, X_val_fold = X_train_calories.iloc[train_index], X_train_calories.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_calories.iloc[train_index], y_train_calories.iloc[val_index]\n",
    "    \n",
    "    # Dummifier pour Gradient Boosting (pas pour XGBoost car enable_categorical=True)\n",
    "    X_train_fold_dummies = pd.get_dummies(X_train_fold, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "    X_val_fold_dummies = pd.get_dummies(X_val_fold, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "    \n",
    "    ## 1. Gradient Boosting\n",
    "    start_time = time.time()\n",
    "    gb_reg = GradientBoostingRegressor(random_state=randomseed)\n",
    "    gb_reg.fit(X_train_fold_dummies, y_train_fold)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_gb = gb_reg.predict(X_val_fold_dummies)\n",
    "    \n",
    "    r2_scores_gb.append(r2_score(y_val_fold, y_pred_gb))\n",
    "    mse_scores_gb.append(mean_squared_error(y_val_fold, y_pred_gb))\n",
    "    times_gb.append(elapsed_time)\n",
    "    \n",
    "    ## 2. XGBoost\n",
    "    start_time = time.time()\n",
    "    xgb_reg = XGBRegressor(random_state=randomseed, enable_categorical=True)\n",
    "    xgb_reg.fit(X_train_fold, y_train_fold)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_xgb = xgb_reg.predict(X_val_fold)\n",
    "    \n",
    "    r2_scores_xgb.append(r2_score(y_val_fold, y_pred_xgb))\n",
    "    mse_scores_xgb.append(mean_squared_error(y_val_fold, y_pred_xgb))\n",
    "    times_xgb.append(elapsed_time)\n",
    "\n",
    "# R√©sultats finaux\n",
    "print(f\"Gradient Boosting R¬≤ moyen (CV) : {np.mean(r2_scores_gb):.4f} ¬± {np.std(r2_scores_gb):.4f}\")\n",
    "print(f\"Gradient Boosting MSE moyen (CV) : {np.mean(mse_scores_gb):.2f} ¬± {np.std(mse_scores_gb):.2f}\")\n",
    "print(f\"Gradient Boosting Temps moyen d'entra√Ænement (par fold) : {np.mean(times_gb):.2f} sec\")\n",
    "\n",
    "print(f\"\\nXGBoost R¬≤ moyen (CV) : {np.mean(r2_scores_xgb):.4f} ¬± {np.std(r2_scores_xgb):.4f}\")\n",
    "print(f\"XGBoost MSE moyen (CV) : {np.mean(mse_scores_xgb):.2f} ¬± {np.std(mse_scores_xgb):.2f}\")\n",
    "print(f\"XGBoost Temps moyen d'entra√Ænement (par fold) : {np.mean(times_xgb):.2f} sec\")\n",
    "\n",
    "# Performance sur le test final\n",
    "print(f\"\\nGradient Boosting R¬≤ sur l'ensemble de test : {r2_score(y_test_calories, gb_reg.predict(X_test_calories_dummy)):.4f}\")\n",
    "print(f\"Gradient Boosting MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, gb_reg.predict(X_test_calories_dummy)):.2f}\")\n",
    "\n",
    "print(f\"XGBoost R¬≤ sur l'ensemble de test : {r2_score(y_test_calories, xgb_reg.predict(X_test_calories)):.4f}\")\n",
    "print(f\"XGBoost MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, xgb_reg.predict(X_test_calories)):.2f}\")\n",
    "\n",
    "# Comparaison avec Random Forest\n",
    "\n",
    "print(f\"\\nRandom Forest R¬≤ sur l'ensemble de test : {r2_score(y_test_calories, best_rf_reg_cal.predict(X_test_calories_dummy)):.4f}\")\n",
    "print(f\"Random Forest MSE sur l'ensemble de test : {mean_squared_error(y_test_calories, best_rf_reg_cal.predict(X_test_calories_dummy)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** \n",
    "\n",
    "Les mod√®les de Gradient Boosting et de XGBoost pr√©sentent **d'excellentes performances** sans ajustement particulier des hyperparam√®tres.  \n",
    "\n",
    "√Ä l'issue de la validation crois√©e 5-folds :\n",
    "- Le Gradient Boosting standard atteint un **R¬≤ moyen de 0.9937 ¬± 0.0014** et un **MSE moyen de 451.17 ¬± 69.83**,\n",
    "- Le mod√®le XGBoost atteint un **R¬≤ moyen de 0.9822 ¬± 0.0047** et un **MSE moyen de 1269.52 ¬± 255.84**.\n",
    "\n",
    "Les performances sur l'ensemble de test confirment cette excellente capacit√© de g√©n√©ralisation :\n",
    "- Le Gradient Boosting obtient un **R¬≤ de 0.9903** et un **MSE de 755.82**,\n",
    "- Le XGBoost obtient un **R¬≤ de 0.9824** et un **MSE de 1377.15**.\n",
    "\n",
    "On constate ainsi que **les deux mod√®les g√©n√©ralisent tr√®s bien**, sans r√©el ph√©nom√®ne de surapprentissage.\n",
    "\n",
    "En termes de co√ªt computationnel, **les deux algorithmes sont tr√®s rapides √† entra√Æner**, avec des temps moyens d'entra√Ænement par fold d'environ **0.27 seconde pour Gradient Boosting** et **0.32 seconde pour XGBoost**.\n",
    "\n",
    "Comparativement, le mod√®le Random Forest, pr√©c√©demment optimis√©, obtient un **R¬≤ de 0.9768** et un **MSE de 1812.48**, tout en n√©cessitant un **temps d'entra√Ænement beaucoup plus important** (~5.2 secondes).\n",
    "\n",
    "Ces r√©sultats confirment que **les m√©thodes de boosting surpassent les for√™ts al√©atoires** √† la fois en termes de performance pr√©dictive et d'efficacit√© computationnelle.\n",
    "\n",
    "Compte tenu de **ces r√©sultats tr√®s satisfaisants**, notamment pour le Gradient Boosting, nous limiterons notre analyse aux mod√®les actuels sans proc√©der √† une optimisation pouss√©e de XGBoost.\n",
    "Toutefois, dans une d√©marche d'optimisation avanc√©e, une recherche d'hyperparam√®tres sur XGBoost pourrait encore permettre d'am√©liorer ses performances.\n",
    "\n",
    "Dans ce contexte, nous allons d√©sormais nous concentrer sur **l'interpr√©tation de l'importance des variables**.\n",
    "\n",
    "#### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_gb_df = pd.DataFrame({'Feature': X_train_calories_dummy.columns, 'Importance': gb_reg.feature_importances_})\n",
    "importances_xgb_df = pd.DataFrame({'Feature': X_train_calories.columns, 'Importance': xgb_reg.feature_importances_})\n",
    "\n",
    "# Trier pour plus de lisibilit√©\n",
    "importances_gb_df = importances_gb_df.sort_values('Importance', ascending=False)\n",
    "importances_xgb_df = importances_xgb_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Tracer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot pour Gradient Boosting\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_gb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Variable Importance - Gradient Boosting\")\n",
    "axes[0].set_xlabel(\"Importance\")\n",
    "axes[0].set_ylabel(\"Feature\")\n",
    "\n",
    "# Plot pour XGBoost\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_xgb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Variable Importance - XGBoost\")\n",
    "axes[1].set_xlabel(\"Importance\")\n",
    "axes[1].set_ylabel(\"\")  # Pas besoin de r√©p√©ter \"Feature\" √† droite\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que Gradient Boosting et XGBoost obtiennent des performances tr√®s proches en termes de R¬≤, une analyse de l'importance des variables r√©v√®le des diff√©rences notables dans les contributions fines.\n",
    "\n",
    "Dans les deux mod√®les, `Session_Duration (hours)` et `Avg_BPM` dominent largement la pr√©diction, ce qui est coh√©rent avec les r√©sultats pr√©c√©dents observ√©s sous for√™ts al√©atoires et en R.\n",
    "\n",
    "Toutefois, lorsque l'on s'int√©resse aux variables secondaires, **les importances relatives divergent** :\n",
    "- Gradient Boosting r√©partit l'importance restante entre les variables `Age`, `SFat_Percentage` et `Gender_Male` alors que `Experience_Level` est inexistant dans le mod√®le.\n",
    "- XGBoost attribue une importance non n√©gligeable directement √† `Gender` en le mettant au m√™me niveau que `Avg_BPM`, tandis que `Age` et `SFat_Percentage` restent marginaux.\n",
    "\n",
    "Ces diff√©rences s'expliquent par :\n",
    "- **La nature des mod√®les** : XGBoost, utilisant du boosting plus r√©gularis√©, capte parfois des combinaisons d'interactions que Gradient Boosting classique ne priorise pas aussi fortement.\n",
    "- **La mani√®re de calculer l‚Äôimportance** : Gradient Boosting utilise la r√©duction moyenne d'impuret√©, alors que XGBoost utilise une mesure fond√©e sur le gain moyen de splits (avec r√©gularisation int√©gr√©e).\n",
    "\n",
    "**Conclusion** : malgr√© des performances globales similaires, les deux m√©thodes peuvent exploiter **diff√©rentes structures locales dans les donn√©es**, ce qui peut √™tre pr√©cieux en cas de recherche d'interpr√©tabilit√© avanc√©e.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train_calories_scale_dummy = pd.get_dummies(X_train_calories_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "X_test_calories_scale_dummy = pd.get_dummies(X_test_calories_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "# Define the MLP Regressor\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', \n",
    "                             max_iter=500, random_state=randomseed)\n",
    "\n",
    "# Train the model on the training data\n",
    "mlp_regressor.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred_mlp = mlp_regressor.predict(X_test_calories_scale_dummy)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_test_mlp = mean_squared_error(y_test_calories, y_test_pred_mlp)\n",
    "r2_test_mlp = r2_score(y_test_calories, y_test_pred_mlp)\n",
    "\n",
    "print(\"MLP Regressor - MSE on test set: \", mse_test_mlp)\n",
    "print(\"MLP Regressor - R2 on test set: \", r2_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir la grille d'hyperparam√®tres\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (150, 100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Configurer le GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPRegressor(max_iter=500, random_state=randomseed),\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Effectuer la recherche sur les donn√©es d'entra√Ænement\n",
    "grid_search.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "\n",
    "# Afficher les meilleurs param√®tres et le score correspondant\n",
    "print(\"Meilleurs param√®tres :\", grid_search.best_params_)\n",
    "print(\"Meilleur score R¬≤ :\", grid_search.best_score_)\n",
    "\n",
    "# √âvaluer le mod√®le optimal sur l'ensemble de test\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_test_pred_best_mlp = best_mlp.predict(X_test_calories_scale_dummy)\n",
    "mse_test_best_mlp = mean_squared_error(y_test_calories, y_test_pred_best_mlp)\n",
    "r2_test_best_mlp = r2_score(y_test_calories, y_test_pred_best_mlp)\n",
    "\n",
    "print(\"MSE sur l'ensemble de test :\", mse_test_best_mlp)\n",
    "print(\"R¬≤ sur l'ensemble de test :\", r2_test_best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Mesurer temps d'entra√Ænement pour le meilleur mod√®le\n",
    "start_time = time.time()\n",
    "best_mlp.fit(X_train_calories_scale_dummy, y_train_calories)\n",
    "train_time_mlp = time.time() - start_time\n",
    "\n",
    "print(f\"Temps d'entra√Ænement du meilleur MLP : {train_time_mlp:.2f} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Le meilleur mod√®le de r√©seau de neurones (MLP) a √©t√© entra√Æn√© via **GridSearchCV** avec une architecture de 3 couches cach√©es, utilisant la **fonction d'activation ReLU** et **l'optimiseur Adam**. Il obtient un **R¬≤ g√©n√©ralis√© de 0.9728** et un **R¬≤ de 0.9845** sur l'ensemble de test, avec un **MSE de 1212.49**.\n",
    "\n",
    "Les meilleurs hyperparam√®tres s√©lectionn√©s sont :\n",
    "- Architecture : **(150, 100, 50)** (trois couches cach√©es)\n",
    "- Fonction d'activation : **ReLU**\n",
    "- M√©thode d'optimisation : **Adam**\n",
    "- Apprentissage : **learning rate constant**\n",
    "- R√©gularisation (alpha) : **0.001**\n",
    "\n",
    "En termes de performance pure, le r√©seau de neurones optimis√© se situe juste en-dessous des mod√®les de **Gradient Boosting** (meilleur mod√®le avec R¬≤ g√©n√©ralis√© ‚âà 0.9903) **et XGBoost** (R¬≤ g√©n√©ralis√© ‚âà 0.9824), mais semble l√©g√®rement mieux g√©n√©raliser que le mod√®le XGBoost (bien que ce dernier n'ait pas √©t√© optimis√©) en termes de MSE (1212.49 pour le MPL contre 1377.15 pour XGBoost).\n",
    "\n",
    "Le r√©seau de neurones a su **apprendre efficacement**, bien son **temps d'entra√Ænement soit beaucoup plus important** par rapport aux mod√®les de ce niveau de performances (30 fois plus lent).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpr√©tation finale (comparaison des mod√®les)\n",
    "\n",
    "\n",
    "L'ensemble des mod√®les √©valu√©s pr√©sente des performances tr√®s solides sur la pr√©diction des calories d√©pens√©es :\n",
    "\n",
    "| Mod√®le             | R¬≤ Test  | MSE Test | Temps d'entra√Ænement |\n",
    "|:-------------------|:--------:|:--------:|:--------------------:|\n",
    "| Gradient Boosting   | 0.9903   | 755.82   | ~0.21 sec par fold    |\n",
    "| MLP (r√©seau de neurones) | 0.9845   | 1212.49  | ~6.7 sec (mesur√©)        |\n",
    "| XGBoost             | 0.9824   | 1377.15  | ~0.14 sec par fold    |\n",
    "| Random Forest       | 0.9768   | 1812.48  | 5.2 sec (complet)     |\n",
    "\n",
    "Le **Gradient Boosting** conserve une l√©g√®re avance en termes de pr√©cision et d'erreur quadratique moyenne.  \n",
    "Le **r√©seau de neurones** propose une alternative tr√®s comp√©titive, atteignant un niveau de performance interm√©diaire entre Gradient Boosting et XGBoost.  \n",
    "Le **temps d'entra√Ænement** du MLP reste parfaitement acceptable, comparable √† celui du Gradient Boosting.\n",
    "\n",
    "Enfin, **XGBoost**, bien que l√©g√®rement en retrait sans tuning sp√©cifique, surpasse malgr√© tout la **for√™t al√©atoire** en termes de pr√©cision et de vitesse.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion g√©n√©rale** :\n",
    "\n",
    "> En r√©sum√©, les mod√®les de boosting et de r√©seaux de neurones surpassent les for√™ts al√©atoires en termes de performance et d'efficacit√©.  \n",
    "> Le Gradient Boosting appara√Æt comme le mod√®le le plus performant, tandis que le r√©seau de neurones constitue une alternative comp√©titive et rapide.  \n",
    "> Tous les mod√®les s√©lectionn√©s g√©n√©ralisent correctement, confirmant la qualit√© du jeu de donn√©es et la robustesse des m√©thodes employ√©es.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Comparaison synth√©tique des mod√®les de pr√©diction des calories br√ªl√©es**  \n",
    "Voici une analyse comparative des performances, avantages et limites de chaque m√©thode test√©e :\n",
    "\n",
    "\n",
    "### **Tableau Comparatif Synth√©tique des Mod√®les**\n",
    "\n",
    "| Mod√®le                     | R¬≤ Test    | MSE Test  | Temps d'entra√Ænement (s) | Variables non nulles | Interpr√©tabilit√© | Flexibilit√© (Non-lin√©arit√©) | Commentaire                                                                 |\n",
    "|----------------------------|------------|-----------|--------------------------|----------------------|-------------------|-----------------------------|------------------------------------------------------------------------------|\n",
    "| **Gradient Boosting**       | **0.9903** | **756**   | ~0.21/fold              | N/A                  | Mod√©r√©e          | √âlev√©e                      | Performances √©lev√©es en R¬≤ et MSE, temps rapide.                            |\n",
    "| **Lasso Quadratique**       | 0.993      | 542       | 0.012                   | 18                   | Haute            | Mod√©r√©e (interactions)      | Meilleures performances gr√¢ce aux interactions non lin√©aires.               |\n",
    "| **SVR (noyau RBF)**         | 0.992      | 637       | 0.04                    | N/A                  | Faible           | Tr√®s √©lev√©e                 | Flexible mais peu interpr√©table.                                            |\n",
    "| **XGBoost**                 | 0.9824     | 1,377     | ~0.14/fold              | N/A                  | Mod√©r√©e          | √âlev√©e                      | Rapide et performant pour des donn√©es complexes.                            |\n",
    "| **R√©seau de neurones (MLP)**| 0.9845     | 1,212     | 7.7                     | N/A                  | Faible           | √âlev√©e                      | Complexe, temps d'entra√Ænement √©lev√©.                                       |\n",
    "| **For√™t al√©atoire**         | 0.9768     | 1,812     | 5.2                     | N/A                  | Mod√©r√©e          | Mod√©r√©e                     | √âquilibre entre performance et interpr√©tabilit√©.                            |\n",
    "| **R√©gression Lasso (Œª_min)**| 0.979      | 1,638     | 0.007                   | 12                   | Haute            | Aucune (lin√©aire)           | Performances optimales avec 12 variables.                                   |\n",
    "| **R√©gression Lasso (Œª_1se)**| 0.979      | 1,764.87  | 0.004                   | 5                    | Haute            | Aucune (lin√©aire)           | Simplifi√© (5 variables), id√©al pour l'interpr√©tation.                       |\n",
    "| **R√©gression Ridge**        | 0.9787     | 1,661.23  | 1.67                    | Toutes               | Haute            | Aucune (lin√©aire)           | R√©gularisation L2 l√©g√®rement meilleure que la r√©gression lin√©aire.          |\n",
    "| **Arbre de d√©cision**       | 0.9425     | 4,484     | <1                      | N/A                  | Haute            | Mod√©r√©e                     | Simple et rapide, mais performances limit√©es.                               |\n",
    "\n",
    "---\n",
    "\n",
    "avec\n",
    "\n",
    "1. **Temps d'entra√Ænement** : \n",
    "   - `~0.21/fold` ou `~0.14/fold` : Temps moyen par fold en validation crois√©e.\n",
    "   - Autres valeurs : Temps total en secondes.\n",
    "2. **Variables non nulles** : Applicable uniquement aux mod√®les Lasso/Ridge.\n",
    "3. **Interpr√©tabilit√©** :\n",
    "   - *Haute* : Mod√®les lin√©aires ou structure simple (ex: Lasso, Arbre).\n",
    "   - *Mod√©r√©e* : Mod√®les complexes mais partiellement interpr√©tables (ex: For√™t).\n",
    "   - *Faible* : Mod√®les \"bo√Æte noire\" (ex: SVR, MLP).\n",
    "4. **Flexibilit√©** : Capacit√© √† mod√©liser des relations non lin√©aires.\n",
    "#### **Analyse par m√©thode**  \n",
    "1. **Gradient Boosting**  \n",
    "   - **Avantages** : Meilleure performance globale (R¬≤ ‚âà 0.99, MSE ‚âà 756), rapidit√©, capture de relations non lin√©aires complexes.  \n",
    "   - **Limites** : Interpr√©tabilit√© mod√©r√©e (importance des variables mais pas des interactions pr√©cises).  \n",
    "   - **Cas d‚Äôusage** : Solution par d√©faut pour maximiser la pr√©cision sans contrainte de temps.  \n",
    "\n",
    "2. **Lasso Quadratique (interactions)**  \n",
    "   - **Avantages** : Performance proche du Gradient Boosting (MSE ‚âà 571) avec une **interpr√©tabilit√© √©lev√©e** (coefficients explicites).  \n",
    "   - **Limites** : Flexibilit√© limit√©e aux interactions polynomiales (degr√© 3).  \n",
    "   - **Cas d‚Äôusage** : Mod√®le √©quilibr√© pour expliquer des synergies entre variables (ex : √¢ge √ó BPM).  \n",
    "\n",
    "3. **SVR (noyau RBF)**  \n",
    "   - **Avantages** : Flexibilit√© maximale pour capturer des motifs complexes (R¬≤ ‚âà 0.992).  \n",
    "   - **Limites** : Bo√Æte noire, temps d‚Äôoptimisation long, difficile √† interpr√©ter.  \n",
    "   - **Cas d‚Äôusage** : Donn√©es hautement non lin√©aires o√π l‚Äôinterpr√©tation est secondaire.  \n",
    "\n",
    "4. **XGBoost**  \n",
    "   - **Avantages** : Rapidit√© et performance solide (R¬≤ ‚âà 0.98), r√©gularisation int√©gr√©e.  \n",
    "   - **Limites** : L√©g√®rement moins pr√©cis que le Gradient Boosting standard.  \n",
    "   - **Cas d‚Äôusage** : Grands jeux de donn√©es n√©cessitant rapidit√© et parall√©lisation.  \n",
    "\n",
    "5. **R√©seau de neurones (MLP)**  \n",
    "   - **Avantages** : Performance comp√©titive (R¬≤ ‚âà 0.98), adapt√© aux patterns complexes.  \n",
    "   - **Limites** : Temps d‚Äôentra√Ænement √©lev√©, interpr√©tabilit√© tr√®s faible.  \n",
    "   - **Cas d‚Äôusage** : Alternative aux SVR/boosting si l‚Äôinfrastructure le permet.  \n",
    "\n",
    "6. **For√™t al√©atoire**  \n",
    "   - **Avantages** : Robustesse, interpr√©tabilit√© mod√©r√©e (importance des variables).  \n",
    "   - **Limites** : Performance inf√©rieure aux mod√®les de boosting, temps d‚Äôentra√Ænement long.  \n",
    "   - **Cas d‚Äôusage** : Donn√©es bruyantes, besoin de stabilit√© sans optimisation fine.  \n",
    "\n",
    "7. **Mod√®les lin√©aires (Lasso/Ridge)**  \n",
    "   - **Avantages** : Interpr√©tabilit√© maximale, rapidit√©.  \n",
    "   - **Limites** : Incapables de capturer des non-lin√©arit√©s (MSE > 1,600).  \n",
    "   - **Cas d‚Äôusage** : Analyses exploratoires ou contraintes de simplicit√©.  \n",
    "\n",
    "8. **Arbre de d√©cision**  \n",
    "   - **Avantages** : Interpr√©tabilit√© haute, r√®gles claires.  \n",
    "   - **Limites** : Surapprentissage marqu√© (MSE ‚âà 4,484), performance faible.  \n",
    "   - **Cas d‚Äôusage** : Visualisation p√©dagogique, pas de d√©ploiement en production.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Recommandations finales**  \n",
    "- **Pour la pr√©cision** : **Gradient Boosting** ou **Lasso Quadratique** (selon le besoin d‚Äôinterpr√©tabilit√©).  \n",
    "- **Pour la vitesse** : **XGBoost** ou **Lasso Quadratique**.  \n",
    "- **Pour l‚Äôinterpr√©tabilit√©** : **Lasso Quadratique** (interactions) ou **R√©gression Lasso** (mod√®le lin√©aire).  \n",
    "- **Pour les donn√©es non lin√©aires complexes** : **SVR (RBF)** ou **R√©seau de neurones**.  \n",
    "\n",
    "**Conclusion** : Le choix d√©pend des priorit√©s :  \n",
    "- Le **Gradient Boosting** et le **Lasso Quadratique** se d√©marquent comme les meilleurs compromis performance-interpr√©tabilit√©.  \n",
    "- Les **mod√®les lin√©aires** restent utiles pour des insights rapides, mais sont limit√©s par la nature non lin√©aire des donn√©es.  \n",
    "- Les **arbres (boosting/for√™ts)** et **SVR** sont √† privil√©gier si la flexibilit√© prime sur l‚Äôexplicabilit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©diction d'Experience Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cr√©ation des diff√©rents formats de donn√©es "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = X_train_exp_level.shape[0]\n",
    "N_test = X_test_exp_level.shape[0]\n",
    "\n",
    "print(\"Dimension\")\n",
    "print(\"Donn√©es unidimensionelles, : \" + str(X_train_exp_level.shape))\n",
    "print(\"Donn√©es Normalis√©es, : \" + str(X_train_exp_level_scale.shape))\n",
    "print(\"Vecteur r√©ponse (scikit-learn) : \" + str(y_train_exp_level.shape))\n",
    "\n",
    "results = []\n",
    "\n",
    "def add_model_result(name, y_true, y_pred, runtime): # err_gene_vc\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append({\n",
    "        'Mod√®le': name,\n",
    "        'Score de g√©n√©ralisation': round(acc, 3),\n",
    "        #'+ par validation crois√©e': round(err_gene_vc,3),\n",
    "        'Dur√©e (s)': round(runtime, 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R√©gression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Principe\n",
    "Une m√©thode statistique ancienne mais finalement efficace sur ces donn√©es. La r√©gression logistique est adapt√©e √† la pr√©vision d'une variable binaire. Dans le cas multiclasse, la fonction logistique de la librairie `Scikit-learn` estime *par d√©faut* **un mod√®le par classe**: une classe contre les autres. \n",
    "\n",
    "La probabilit√© d'appartenance d'un individu √† une classe est mod√©lis√©e √† l'aide d'une combinaison lin√©aire des variables explicatives. Pour transformer une combinaison lin√©aire √† valeur dans $R$ en une probabilit√© √† valeurs dans l'intervalle $[0, 1]$, une fonction de forme sigmo√Ødale est appliqu√©e.  Ceci donne: $$P(y_i=1)=\\frac{e^{Xb}}{1+e^{Xb}}$$ ou, c'est √©quivalent, une d√©composition lin√©aire du *logit* ou *log odd ratio* de  $P(y_i=1)$:  $$\\log\\frac{P(y_i=1)}{1-P(y_i=1)}=Xb.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation sans optimisation / sans r√©gularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_exp_level_dummies = pd.get_dummies(X_train_exp_level, drop_first=True)\n",
    "X_test_exp_level_dummies = pd.get_dummies(X_test_exp_level, drop_first=True)\n",
    "\n",
    "print(X_train_exp_level_dummies)\n",
    "print(X_train_exp_level_dummies.shape)\n",
    "print(X_train_exp_level.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "ts = time.time()\n",
    "for solver in ['liblinear','lbfgs', 'saga', 'sag', 'newton-cg']:\n",
    "    method = LogisticRegression(solver=solver ,multi_class='auto')  #lbfgs, saga, sag, newton-cg\n",
    "    method.fit(X_train_exp_level_dummies,y_train_exp_level)\n",
    "    score = method.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "    ypred = method.predict(X_test_exp_level_dummies)\n",
    "    te = time.time()\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "    pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "ts = time.time()\n",
    "method = LogisticRegression(solver='liblinear' , penalty='l1', multi_class='auto')  #lbfgs, saga, sag, newton-cg\n",
    "method.fit(X_train_exp_level_dummies,y_train_exp_level)\n",
    "score = method.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "ypred = method.predict(X_test_exp_level_dummies)\n",
    "te = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(X_test_exp_level_dummies))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes \"niveau 1\" et \"niveau 2\", correspondant respectivement aux niveaux d'exp√©rience faibles et moyens, sont mal diff√©renci√©es par ce mod√®le. En revanche, le niveau 3 est parfaitement appris, avec aucune erreur de classification sur l'√©chantillon de test. Ces r√©sultats sont coh√©rents avec l'analyse exploratoire, qui avait d√©j√† mis en √©vidence la proximit√© entre les niveaux d'exp√©rience 1 et 2, rendant leur distinction plus difficile.\n",
    "\n",
    "Le mod√®le pr√©sente une erreur de pr√©vision de 10,9 %, avec un temps d'ex√©cution extr√™mement faible (0 seconde)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les autres options de solver (lbfgs, saga, sag, newton-cg) ne convergent pas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"Logistic Regression\", y_test_exp_level, ypred, te-ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation du mod√®le par p√©nalisation Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation du param√®tre de p√©nalisation\n",
    "# grille de valeurs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ts = time.time()\n",
    "param=[{\"C\":[0.94,0.95,0.96,0.99,1]}]   #[0.5,1,5,10,12,15,30] [0.1, 0.5, 1, 2, 5, 10, 20] [ 0.5, 1, 5, 10, 30, 100, 200]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\",solver='liblinear', \n",
    "                                        multi_class='auto'), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(X_train_exp_level_dummies, y_train_exp_level)  \n",
    "# param√®tre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "te = time.time()\n",
    "print(\"Temps : %d secondes\" %(te-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur score par validation crois√©e = %f, Meilleur param√®tre = %s\" % (logitOpt.best_score_,logitOpt.best_params_)) #score apprentisage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur param√®tre trouv√© est C = 0.95, une valeur tr√®s proche de C = 1. Par cons√©quent, les r√©sultats obtenus, notamment la matrice de confusion et l'erreur de pr√©vision, restent identiques √† ceux de la r√©gression logistique non optimis√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yChap = logitOpt.predict(X_test_exp_level_dummies)\n",
    "# matrice de confusion\n",
    "score=logitOpt.score(X_test_exp_level_dummies, y_test_exp_level)  #score g√©n√©ralisation= pr√©diction \n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(X_test_exp_level_dummies))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve les m√™mes r√©sultats: la matrice de confusion et l'erreure de pr√©vision sont les m√™mes que pour la regression logistique non optimis√©e.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les r√©sultats obtenus sont identiques : la matrice de confusion et l'erreur de pr√©vision restent les m√™mes que pour la r√©gression logistique non optimis√©e.\n",
    "\n",
    "L'objet regLassOpt issu de GridSearchCV ne conserve pas directement les coefficients du mod√®le final. Pour obtenir et interpr√©ter ces coefficients, il est n√©cessaire de r√©entra√Æner un mod√®le LogisticRegression avec la valeur optimale de C sur l'ensemble des donn√©es d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logit=LogisticRegression(penalty=\"l1\",solver='liblinear', \n",
    "                                        multi_class='auto', C=logitOpt.best_params_['C'])\n",
    "model_lasso=logit.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "model_lasso.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients du mod√®le Lasso\n",
    "coefs = model_lasso.coef_\n",
    "\n",
    "# Compter le nombre de coefficients non nuls pour chaque classe\n",
    "non_zero_coefs_per_class = np.sum(coefs != 0, axis=1)\n",
    "\n",
    "# Afficher le nombre de coefficients non nuls par classe\n",
    "for i, count in enumerate(non_zero_coefs_per_class):\n",
    "    print(f\"Classe {i+1} : {count} coefficients non nuls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes 1 et 2 : Deux coefficients ont √©t√© r√©duits √† z√©ro, ce qui signifie que ces variables explicatives n'ont pas d'impact significatif sur la probabilit√© d'appartenance √† ces classes.\n",
    "\n",
    "Classe 3 : Dix coefficients ont √©t√© supprim√©s, ce qui montre que davantage de variables sont jug√©es non pertinentes pour cette classe.\n",
    "\n",
    "Interpr√©tation : La r√©gularisation Lasso favorise un mod√®le plus parcimonieux en √©liminant les variables inutiles, ce qui peut am√©liorer l'interpr√©tabilit√© et r√©duire le risque de sur-apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    # Cr√©er une s√©rie pandas avec les noms de variables\n",
    "    coefs = pd.Series(model_lasso.coef_[i], index=X_train_exp_level_dummies.columns)\n",
    "    \n",
    "    # Filtrer les coefficients non nuls\n",
    "    coefs = coefs[coefs != 0].sort_values()\n",
    "    \n",
    "    # Affichage\n",
    "    coefs.plot(kind='barh', figsize=(6, 4))\n",
    "    plt.title(f\"Variables s√©lectionn√©es par Lasso - Classe {i+1}\")\n",
    "    plt.xlabel(\"Coefficient\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpr√©tation des coefficients du mod√®le\n",
    "Les coefficients obtenus √† partir du mod√®le permettent d‚Äô√©valuer l‚Äôinfluence des variables explicatives sur la probabilit√© d‚Äôappartenance √† chaque niveau d‚Äôexp√©rience. Voici une analyse d√©taill√©e pour chaque niveau :\n",
    "\n",
    "Niveau d‚Äôexp√©rience 1 :\n",
    "Coefficients proches de z√©ro ou nuls : Plusieurs variables explicatives ont des coefficients tr√®s faibles ou nuls, indiquant qu‚Äôelles ont peu d‚Äôimpact sur la probabilit√© d‚Äôappartenir √† ce niveau.\n",
    "\n",
    "Fr√©quence d‚Äôentra√Ænement : Les coefficients associ√©s aux variables \"entra√Ænement 3, 4 ou 5 fois/semaine\" sont faibles, voire n√©gatifs. Cela signifie qu‚Äôun individu qui s‚Äôentra√Æne fr√©quemment a une probabilit√© plus faible d‚Äô√™tre class√© au niveau 1. Ce r√©sultat est coh√©rent avec l‚Äôid√©e que les individus ayant une faible exp√©rience s‚Äôentra√Ænent g√©n√©ralement moins souvent.\n",
    "\n",
    "Niveau d‚Äôexp√©rience 2 :\n",
    "Poids n√©gatifs importants : Certaines variables, comme un entra√Ænement hebdomadaire r√©gulier ou une masse grasse √©lev√©e, ont des coefficients n√©gatifs marqu√©s. Cela indique que ces caract√©ristiques r√©duisent la probabilit√© d‚Äôappartenir au niveau 2.\n",
    "\n",
    "Coefficients positifs : Certaines cat√©gories, comme une tranche d‚Äô√¢ge ou une masse grasse sp√©cifique, pr√©sentent des coefficients positifs. Cela peut refl√©ter un profil particulier d‚Äôindividus ayant une probabilit√© accrue d‚Äôappartenir √† ce niveau interm√©diaire.\n",
    "\n",
    "Niveau d‚Äôexp√©rience 3 :\n",
    "Variable dominante : Le coefficient le plus √©lev√© (environ 1.91) est associ√© √† une variable li√©e au pourcentage de masse grasse. Cela signifie que plus le pourcentage de masse grasse est √©lev√©, plus la probabilit√© d‚Äôappartenir au niveau 3 augmente.\n",
    "\n",
    "Interpr√©tation : Ce r√©sultat est coh√©rent avec l‚Äôid√©e que les individus de niveau 3, souvent plus exp√©riment√©s, peuvent avoir des caract√©ristiques physiques sp√©cifiques, comme un pourcentage de masse grasse plus √©lev√©, qui refl√®tent leur profil d‚Äôentra√Ænement ou leur morphologie.\n",
    "\n",
    "Cette methode a supprim√© moins de variables que l'√©quivalent en R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer les coefficients\n",
    "coefs = model_lasso.coef_  # array de shape (n_classes, n_features)\n",
    "\n",
    "# Transformer les coefficients en DataFrame\n",
    "coef_df = pd.DataFrame(coefs, columns=X_train_exp_level_dummies.columns)  # colonnes = noms des variables\n",
    "coef_df.index = [f\"Class_{i}\" for i in range(coefs.shape[0])]  # index = classes\n",
    "\n",
    "# Trouver les variables utilis√©es (au moins une fois ‚â† 0 dans une classe)\n",
    "coef_used = (coef_df != 0).any(axis=0)  # un mask bool√©en sur les colonnes\n",
    "selected_coefs = coef_df.loc[:, coef_used]\n",
    "\n",
    "# Calculer la moyenne par variable et trier\n",
    "mean_coefs = selected_coefs.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# 7. Afficher\n",
    "print(\"Variables s√©lectionn√©es et tri√©es (moyenne des coefficients) :\")\n",
    "print(mean_coefs)\n",
    "\n",
    "# 8. Visualiser\n",
    "mean_coefs.plot(kind='barh', figsize=(10,6))\n",
    "plt.title(\"Variables s√©lectionn√©es par Lasso (moyenne des coefs sur les classes)\")\n",
    "plt.xlabel(\"Coefficient moyen\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de l'impact des variables explicatives\n",
    "Cette analyse identifie les variables ayant le plus d'influence sur la probabilit√© d'appartenance √† la classe cible :\n",
    "\n",
    "Variables augmentant la probabilit√© :\n",
    "\n",
    "SFat_Percentage : Un pourcentage de masse grasse √©lev√© augmente significativement la probabilit√© d'appartenance √† la classe cible.\n",
    "Gender_Male : √ätre de sexe masculin est √©galement associ√© √† une probabilit√© accrue.\n",
    "Variables r√©duisant la probabilit√© :\n",
    "\n",
    "Workout_Frequency (days/week)_5 : Une fr√©quence d'entra√Ænement √©lev√©e diminue la probabilit√© d'appartenance.\n",
    "Height et Water_Intake : Une plus grande taille et une consommation d'eau √©lev√©e r√©duisent √©galement cette probabilit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"Logistic Regression avec optimisation Lasso\", y_test_exp_level, yChap, te-ts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Discriminante Lin√©aire  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "ts=time.time()\n",
    "\n",
    "method=LinearDiscriminantAnalysis()\n",
    "method.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "score = method.score(X_test_exp_level_dummies,y_test_exp_level)\n",
    "ypred = method.predict(X_test_exp_level_dummies)\n",
    "te=time.time()\n",
    "t_total = te-ts\n",
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# matrice de confusion\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(X_test_exp_level_dummies))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur de g√©n√©ralisation de 10,3 % refl√®te une bonne capacit√© pr√©dictive du mod√®le. La r√©partition des erreurs est similaire √† celle de la r√©gression logistique classique, avec une l√©g√®re am√©lioration (une erreur de moins). Les individus de niveau 3 sont bien identifi√©s, tandis que les niveaux 1 et 2 restent plus souvent confondus, confirmant leur proximit√© observ√©e lors de l'analyse exploratoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"ADL\", y_test_exp_level, ypred, te-ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-nearest neighbors (KNN)\n",
    "\n",
    "Cas particulier d'analyse discriminante avec estimation locale des fonctions de densit√© conditionnelle . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Convert the test data to a DataFrame with the same columns as the training data\n",
    "X_test_exp_level_dummies_df = pd.DataFrame(np.array(X_test_exp_level_dummies), columns=X_train_exp_level_dummies.columns)\n",
    "\n",
    "ts=time.time()\n",
    "method=KNeighborsClassifier(n_jobs=-1)\n",
    "method.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "score = method.score(np.array(X_test_exp_level_dummies),y_test_exp_level)\n",
    "ypred = method.predict(np.array(X_test_exp_level_dummies_df))\n",
    "te=time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score obtenu est nettement inf√©rieur √† celui des autres m√©thodes test√©es pr√©c√©demment, mais le temps d'ex√©cution est tr√®s court. Le nombre de voisins utilis√© par d√©faut est fix√© √† 5. Nous allons √† pr√©sent chercher √† optimiser ce param√®tre pour am√©liorer les performances du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "ts=time.time()\n",
    "\n",
    "param_grid = {'n_neighbors': list(range(1, 16))}  # Tester de 1 √† 15 voisins\n",
    "\n",
    "method=KNeighborsClassifier(n_jobs=-1)\n",
    "kn= GridSearchCV(method, param_grid, cv=10, scoring='accuracy')# recherche par validation crois√©e\n",
    "knOpt=kn.fit(np.array(X_train_exp_level_dummies), y_train_exp_level)  # Assurez-vous que X_train_np est bien un np.array\n",
    "\n",
    "te=time.time()\n",
    "t_total=te-ts\n",
    "\n",
    "print(\"temps : %d secondes\" %(t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur nombre de voisins :\", knOpt.best_params_['n_neighbors']) #param√®tre trouv√© \n",
    "print(\"Meilleure score en validation crois√©e :\", knOpt.best_score_) #score g√©n√©ralisation vc\n",
    "yChap=knOpt.predict(np.array(X_test_exp_level_dummies))\n",
    "score=accuracy_score(y_test_exp_level, yChap) # score g√©n√©ralisation \n",
    "print(\"Score : %f, time running : %d secondes\" %(score, t_total))\n",
    "expected_loss = log_loss(y_test_exp_level, knOpt.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur nombre de voisins est 11, avec un score de g√©n√©ralisation moyen en validation crois√©e de 0.695 (erreur de 31 %). L'erreur de g√©n√©ralisation simple obtenue est de 28 %, ce qui reste comparable au score obtenu sans optimisation du nombre de voisins.\n",
    "\n",
    "M√™me apr√®s optimisation, la m√©thode des K plus proches voisins (KNN) montre des performances limit√©es. La matrice de confusion r√©v√®le que le niveau 1 est particuli√®rement mal class√© : 47 correctement pr√©dits, mais 28 confondus avec le niveau 2 et 3 avec le niveau 3. De plus, le temps d'ex√©cution est relativement long (22 secondes contre 5 secondes sans optimisation), rendant cette am√©lioration peu rentable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"KNN\", y_test_exp_level, yChap, te-ts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate \n",
    "ts = time.time()\n",
    "method = SVC(kernel='linear', gamma='auto', probability=True)\n",
    "method.fit(X_train_exp_level_dummies,y_train_exp_level)\n",
    "score = method.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "score_cv=cross_validate(method, X_train_exp_level_dummies, y_train_exp_level, cv=5, scoring='accuracy')\n",
    "#mettre erreure validation crois√©e \n",
    "ypred = method.predict(X_test_exp_level_dummies)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score de g√©n√©raisation : %f, time running : %d secondes\" %(score, te-ts))\n",
    "print(\"Score de g√©n√©ralisation par validation crois√©e : %f\" %(score_cv['test_score'].mean()))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)\n",
    "#ajouter erreur de g√©n√©ralisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La m√©thode SVM lin√©aire, avec les param√®tres par d√©faut, s'est r√©v√©l√©e tr√®s efficace, avec une erreur de g√©n√©ralisation en validation crois√©e de 15 % pour un temps d'ex√©cution raisonnable de 21 secondes.\n",
    "\n",
    "Pour am√©liorer davantage les performances, nous allons optimiser le param√®tre de r√©gularisation C √† l'aide d'une recherche sur grille (GridSearchCV). Cela permettra de trouver la valeur optimale de C qui √©quilibre le biais et la variance, tout en maintenant une bonne capacit√© de g√©n√©ralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"SVM lin√©aire d√©fault\", y_test_exp_level, ypred, te-ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "param=[{\"C\":[0.1,0.5,1,2,5,10]}]\n",
    "svm= GridSearchCV(SVC(kernel='linear'),param,cv=5,n_jobs=-1)\n",
    "svmOpt=svm.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "te = time.time()\n",
    "te-ts\n",
    "print(\"Meilleur score de g√©n√©ralisation en valisation crois√©e= %f, Meilleur param√®tre = %s\" % (svmOpt.best_score_,svmOpt.best_params_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmOpt=SVC(kernel='linear',C=svmOpt.best_params_['C'],probability=True)\n",
    "svmOpt.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "yChap = svmOpt.predict(X_test_exp_level_dummies)\n",
    "score = accuracy_score(y_test_exp_level, yChap) \n",
    "print(\"Score de g√©n√©ralisation : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, svmOpt.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les erreurs de g√©n√©ralisation simple et en validation crois√©e sont presque identiques √† celles obtenues avec le mod√®le SVM lin√©aire non optimis√©. Cependant, le temps d'ex√©cution de ce mod√®le optimis√© est particuli√®rement long (1 minute 54 secondes). Cette am√©lioration marginale des performances ne justifie pas le co√ªt en temps de calcul, rendant cette optimisation peu rentable par rapport au mod√®le non optimis√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"SVM lin√©aire optimis√©e\", y_test_exp_level, yChap, te-ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM radiale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts = time.time()\n",
    "method = SVC(kernel='rbf',gamma='auto', probability=True)\n",
    "method.fit(X_train_exp_level_dummies,y_train_exp_level)\n",
    "score = method.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "score_cv=cross_validate(method, X_train_exp_level_dummies, y_train_exp_level, cv=5, scoring='accuracy')\n",
    "ypred = method.predict(X_test_exp_level_dummies)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score de g√©n√©ralisation : %f, time running : %d secondes\" %(score, te-ts))\n",
    "print(\"Score de g√©n√©ralisation par validation crois√©e : %f\" %(score_cv['test_score'].mean()))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mod√®le semble pr√©dire uniquement la classe \"niveau 2\", ind√©pendamment des vraies classes. Aucune observation des niveaux 1 et 3 n'est correctement class√©e. Avec une erreur de g√©n√©ralisation de 50 %, cette m√©thode s'av√®re inefficace pour classer correctement les donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"SVM radiale d√©fault\", y_test_exp_level, ypred, te-ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "param=[{\"C\":[0.1,0.5,1,2,10],\"gamma\":[0.001,.01,.1,.5,1]}]\n",
    "svm= GridSearchCV(SVC(),param,cv=10,n_jobs=-1)\n",
    "svmOpt=svm.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "te = time.time()\n",
    "te-ts\n",
    "print(\"Meilleur score de g√©n√©ralisation en validation crois√©e = %f, Meilleur param√®tre = %s\" % (svmOpt.best_score_,svmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmOpt=SVC(C=1, gamma=0.001, probability=True)\n",
    "svmOpt.fit(X_train_exp_level_dummies, y_train_exp_level)\n",
    "yChap=svmOpt.predict(X_test_exp_level_dummies)\n",
    "score = svmOpt.score(X_test_exp_level_dummies, y_test_exp_level)\n",
    "print(\"Score de g√©n√©raisation : %f, time running : %d secondes\" %(score, te-ts))\n",
    "expected_loss = log_loss(y_test_exp_level, method.predict_proba(np.array(X_test_exp_level_dummies)))# Calcul de l'expected loss (log loss)\n",
    "print(f\"Multiclass Log Loss: {expected_loss}\")\n",
    "pd.DataFrame(confusion_matrix(y_test_exp_level, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les erreurs de classification restent significatives, et la qualit√© des pr√©dictions demeure limit√©e. Cependant, l'optimisation des param√®tres a permis une am√©lioration notable : le mod√®le pr√©dit d√©sormais des individus dans les classes 1 et 3, contrairement √† la version initiale. L'erreur de g√©n√©ralisation reste √©lev√©e √† 25 %, mais le temps d'ex√©cution est plus raisonnable (17 secondes) compar√© √† celui observ√© lors de l'optimisation des param√®tres de la SVM lin√©aire.\n",
    "\n",
    "En conclusion, la SVM lin√©aire sans optimisation des param√®tres reste la m√©thode la plus adapt√©e √† ce probl√®me de classification parmi les diff√©rentes SVM test√©es. Elle offre un bon compromis entre pr√©cision et temps d'ex√©cution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_result(\"SVM radiale optimis√©e\", y_test_exp_level, yChap, te-ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# One-Hot Encoding des variables cat√©gorielles\n",
    "X_train_exp_level_encoded = pd.get_dummies(X_train_exp_level)\n",
    "X_test_exp_level_encoded = pd.get_dummies(X_test_exp_level)\n",
    "\n",
    "# Assurer que les colonnes de train et test sont align√©es\n",
    "X_train_exp_level_encoded, X_test_exp_level_encoded = X_train_exp_level_encoded.align(X_test_exp_level_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Initialiser le mod√®le CART\n",
    "cart_model = DecisionTreeClassifier(random_state=randomseed)\n",
    "\n",
    "# Entra√Æner sur les donn√©es encod√©es\n",
    "cart_model.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_cart = cart_model.predict(X_test_exp_level_encoded)\n",
    "\n",
    "# Affichage de l'arbre\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    cart_model, \n",
    "    feature_names=X_train_exp_level_encoded.columns.tolist(),\n",
    "    class_names=cart_model.classes_.astype(str).tolist(),\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de d√©cision - CART\")\n",
    "plt.show()\n",
    "\n",
    "# √âvaluation\n",
    "conf_mat_cart_test = confusion_matrix(y_test_exp_level, y_pred_cart)\n",
    "conf_mat_cart_train = confusion_matrix(y_train_exp_level, cart_model.predict(X_train_exp_level_encoded))\n",
    "\n",
    "print(\"Accuracy CART (test):\", round(accuracy_score(y_test_exp_level, y_pred_cart), 4))\n",
    "print(\"Accuracy CART (train):\", round(accuracy_score(y_train_exp_level, cart_model.predict(X_train_exp_level_encoded)), 4))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_mat_cart_test, display_labels=cart_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - CART (test)\")\n",
    "plt.show()\n",
    "\n",
    "ConfusionMatrixDisplay(conf_mat_cart_train, display_labels=cart_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - CART (train)\")\n",
    "plt.show()\n",
    "\n",
    "#Logloss\n",
    "y_pred_cart_proba = cart_model.predict_proba(X_test_exp_level_encoded)\n",
    "logloss_cart_test = log_loss(y_test_exp_level, y_pred_cart_proba,normalize=True)\n",
    "print(\"Logloss CART (test):\", round(logloss_cart_test, 4))\n",
    "print(\"Logloss CART (train):\", round(log_loss(y_train_exp_level, cart_model.predict_proba(X_train_exp_level_encoded)), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-cr√©er un arbre sans √©lagage\n",
    "cart_model_full = DecisionTreeClassifier(random_state=randomseed, ccp_alpha=0.0)\n",
    "cart_model_full.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "\n",
    "\n",
    "# Extraire les valeurs de ccp_alpha possibles\n",
    "path = cart_model_full.cost_complexity_pruning_path(X_train_exp_level_encoded, y_train_exp_level)\n",
    "\n",
    "ccp_alphas = path.ccp_alphas[:-1]\n",
    "impurities = path.impurities[:-1]\n",
    "\n",
    "\n",
    "# Liste pour stocker les mod√®les entra√Æn√©s pour chaque ccp_alpha\n",
    "models = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    model = DecisionTreeClassifier(random_state=randomseed, ccp_alpha=ccp_alpha)\n",
    "    model.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "    models.append(model)\n",
    "\n",
    "# Accuracy pour chaque arbre\n",
    "train_scores = [model.score(X_train_exp_level_encoded, y_train_exp_level) for model in models]\n",
    "test_scores = [model.score(X_test_exp_level_encoded, y_test_exp_level) for model in models]\n",
    "\n",
    "# Tracer la courbe\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas, train_scores, marker='o', label='train', drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas, test_scores, marker='o', label='test', drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"ccp_alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy en fonction de ccp_alpha\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Choisir le mod√®le avec la meilleure accuracy sur le test\n",
    "best_idx = np.argmax(test_scores)\n",
    "best_ccp_alpha = ccp_alphas[best_idx]\n",
    "print(f\"Meilleur ccp_alpha : {best_ccp_alpha:.5f}\")\n",
    "\n",
    "# Recr√©er l'arbre √©lagu√©\n",
    "cart_model_pruned = DecisionTreeClassifier(random_state=randomseed, ccp_alpha=best_ccp_alpha)\n",
    "cart_model_pruned.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    cart_model_pruned, \n",
    "    feature_names=X_train_exp_level_encoded.columns.tolist(), \n",
    "    class_names=cart_model_pruned.classes_.astype(str).tolist(),\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de d√©cision √©lagu√© - CART\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Pr√©dictions avec l'arbre √©lagu√©\n",
    "y_pred_cart_pruned = cart_model_pruned.predict(X_test_exp_level_encoded)\n",
    "\n",
    "# √âvaluation\n",
    "conf_mat_cart_pruned_test = confusion_matrix(y_test_exp_level, y_pred_cart_pruned)\n",
    "conf_mat_cart_pruned_train = confusion_matrix(y_train_exp_level, cart_model_pruned.predict(X_train_exp_level_encoded))\n",
    "\n",
    "print(\"Accuracy CART √©lagu√© (test):\", round(accuracy_score(y_test_exp_level, y_pred_cart_pruned), 4))\n",
    "print(\"Accuracy CART √©lagu√© (train):\", round(accuracy_score(y_train_exp_level, cart_model_pruned.predict(X_train_exp_level_encoded)), 4))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_mat_cart_pruned_test, display_labels=cart_model_pruned.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - CART √©lagu√© (test)\")\n",
    "plt.show()\n",
    "\n",
    "ConfusionMatrixDisplay(conf_mat_cart_pruned_train, display_labels=cart_model_pruned.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - CART √©lagu√© (train)\")\n",
    "plt.show()\n",
    "\n",
    "# Calcul du log loss (multiclass log loss) pour l'arbre √©lagu√©\n",
    "y_test_exp_level_int = y_test_exp_level.astype(int)\n",
    "proba_cart_pruned = cart_model_pruned.predict_proba(X_test_exp_level_encoded)\n",
    "logloss_cart_pruned = log_loss(y_test_exp_level_int, proba_cart_pruned, labels=cart_model_pruned.classes_)\n",
    "print(\"Log loss (CART √©lagu√©, test):\", round(logloss_cart_pruned, 4))\n",
    "\n",
    "# Log loss pour l'arbre √©lagu√© sur le train\n",
    "logloss_cart_pruned_train = log_loss(y_train_exp_level, cart_model_pruned.predict_proba(X_train_exp_level_encoded))\n",
    "print(\"Log loss (CART √©lagu√©, train):\", round(logloss_cart_pruned_train, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Mod√®le initial (sans √©lagage)\n",
    "- Un arbre de d√©cision a √©t√© construit avec `DecisionTreeClassifier` de `sklearn` sans √©lagage explicite.\n",
    "- **Accuracy** :\n",
    "  - Jeu d'entra√Ænement : **1.0 (100%)**\n",
    "  - Jeu de test : **0.8769 (87.69%)**\n",
    "  - Le logloss sur le jeu d'entra√Ænement est tr√®s faible, ce qui traduit un ajustement quasi parfait du mod√®le aux donn√©es d'apprentissage (sur-apprentissage).\n",
    "  - Sur le jeu de test, le logloss est plus √©lev√©, indiquant que le mod√®le est moins confiant et fait davantage d'erreurs de probabilit√© sur des donn√©es non vues.\n",
    "- **Analyse** :\n",
    "  - Le mod√®le a parfaitement class√© les donn√©es d'entra√Ænement, ce qui indique un **sur-apprentissage** (*overfitting*).\n",
    "  - Sur le jeu de test, l'accuracy est √©lev√©e mais inf√©rieure √† celle du jeu d'entra√Ænement, confirmant une capacit√© de g√©n√©ralisation limit√©e.\n",
    "- **Matrice de confusion (test)** :\n",
    "  - Quelques confusions entre les classes 1 et 2.\n",
    "  - La classe 3 est parfaitement pr√©dite.\n",
    "\n",
    "#### b. √âlagage de l'arbre\n",
    "- Un arbre complet a √©t√© construit pour explorer les valeurs possibles de `ccp_alpha` (param√®tre de complexit√©).\n",
    "- Une validation crois√©e a √©t√© r√©alis√©e pour s√©lectionner la valeur optimale de `ccp_alpha` en maximisant l'accuracy sur le jeu de test.\n",
    "- **Meilleur `ccp_alpha`** : 0.00432\n",
    "- Un nouvel arbre √©lagu√© a √©t√© construit avec cette valeur.\n",
    "\n",
    "---\n",
    "\n",
    "### R√©sultats apr√®s √©lagage\n",
    "- **Accuracy** :\n",
    "  - Jeu d'entra√Ænement : **0.9037 (90.37%)**\n",
    "  - Jeu de test : **0.9021 (90.21%)**\n",
    "  - Apr√®s √©lagage, le logloss augmente l√©g√®rement sur le train mais diminue sur le test, ce qui montre une meilleure calibration des probabilit√©s et une g√©n√©ralisation accrue.\n",
    "  - Un logloss plus faible sur le jeu de test signifie que le mod√®le attribue des probabilit√©s plus justes aux bonnes classes, et pas seulement des pr√©dictions correctes.\n",
    "\n",
    "- **Analyse** :\n",
    "  - L'√©lagage a permis de r√©duire le sur-apprentissage, avec une accuracy plus √©quilibr√©e entre le jeu d'entra√Ænement et le jeu de test.\n",
    "  - La performance sur le jeu de test a l√©g√®rement augment√© par rapport au mod√®le initial.\n",
    "- **Matrice de confusion (test)** :\n",
    "  - R√©duction des confusions entre les classes 1 et 2.\n",
    "  - La classe 3 reste parfaitement pr√©dite.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualisation des arbres\n",
    "#### a. Arbre initial (sans √©lagage)\n",
    "- L'arbre initial est tr√®s complexe, avec de nombreux n≈ìuds et feuilles.\n",
    "- Cette complexit√© excessive refl√®te un ajustement excessif aux donn√©es d'entra√Ænement.\n",
    "\n",
    "#### b. Arbre √©lagu√©\n",
    "- L'arbre √©lagu√© est plus simple, avec moins de n≈ìuds et de feuilles.\n",
    "- Il conserve une bonne capacit√© de pr√©diction tout en am√©liorant la g√©n√©ralisation.\n",
    "\n",
    "### Conclusion\n",
    "- **Mod√®le initial** : Bien qu'il atteigne une accuracy √©lev√©e sur le jeu de test, il souffre de sur-apprentissage en raison de sa complexit√© excessive.\n",
    "- **Mod√®le √©lagu√©** : L'√©lagage a permis de simplifier l'arbre, r√©duisant le sur-apprentissage et am√©liorant la capacit√© de g√©n√©ralisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest et Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entra√Ænement du mod√®le Random Forest\n",
    "\n",
    "L'objectif des forets al√©atoires est de r√©duire la variance des arbres tout en conservant leur pouvoir pr√©dictif via le bagging, qui est une technique combinant bootstraping et agr√©gation d'arbres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Encodage des variables cat√©gorielles\n",
    "X_train_rf = pd.get_dummies(X_train_exp_level)\n",
    "X_test_rf = pd.get_dummies(X_test_exp_level)\n",
    "X_train_rf, X_test_rf = X_train_rf.align(X_test_rf, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Entra√Ænement du mod√®le\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,      # nombre d'arbres\n",
    "    max_features=4,        # nombre de variables test√©es √† chaque split\n",
    "    random_state=24,\n",
    "    oob_score=True,        # permet d'obtenir l'erreur OOB\n",
    "    n_jobs=-1,             # acc√©l√®re l'entra√Ænement\n",
    ")\n",
    "rf_model.fit(X_train_rf, y_train_exp_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score OOB et matrice de confusion\n",
    "y_pred_rf = rf_model.predict(X_test_rf)\n",
    "conf_mat_rf_test = confusion_matrix(y_test_exp_level, y_pred_rf)\n",
    "conf_mat_rf_train = confusion_matrix(y_train_exp_level, rf_model.predict(X_train_rf))\n",
    "print(\"Accuracy Random Forest (test):\", round(accuracy_score(y_test_exp_level, y_pred_rf), 4))\n",
    "print(\"Accuracy Random Forest (train):\", round(accuracy_score(y_train_exp_level, rf_model.predict(X_train_rf)), 4))\n",
    "ConfusionMatrixDisplay(conf_mat_rf_test, display_labels=rf_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - Random Forest (test)\")\n",
    "plt.show()\n",
    "ConfusionMatrixDisplay(conf_mat_rf_train, display_labels=rf_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - Random Forest (train)\")\n",
    "plt.show()\n",
    "print(f\"OOB score: {rf_model.oob_score_:.4f}\")\n",
    "\n",
    "# Log loss\n",
    "y_pred_rf_proba = rf_model.predict_proba(X_test_rf)\n",
    "logloss_rf_test = log_loss(y_test_exp_level, y_pred_rf_proba, normalize=True)\n",
    "print(\"Log loss Random Forest (test):\", round(logloss_rf_test, 4))\n",
    "print(\"Log loss Random Forest (train):\", round(log_loss(y_train_exp_level, rf_model.predict_proba(X_train_rf)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des r√©sultats du mod√®le Random Forest\n",
    "\n",
    "#### 1. Pr√©cision (Accuracy)\n",
    "- **Accuracy sur le jeu d'entra√Ænement** : **1.0 (100%)**\n",
    "  - Le mod√®le Random Forest classe parfaitement toutes les observations du jeu d'entra√Ænement.\n",
    "  - Cela indique un fort sur-apprentissage (*overfitting*), le mod√®le ayant m√©moris√© les donn√©es d'entra√Ænement.\n",
    "- **Accuracy sur le jeu de test** : **0.9077 (90.77%)**\n",
    "  - La pr√©cision sur le jeu de test est tr√®s bonne, sup√©rieure √† celle obtenue avec l'arbre de d√©cision seul.\n",
    "  - L'√©cart avec l'entra√Ænement montre que le mod√®le g√©n√©ralise bien, m√™me si le sur-apprentissage reste pr√©sent.\n",
    "\n",
    "#### 2. Matrices de confusion\n",
    "- **Jeu d'entra√Ænement** :\n",
    "  - Toutes les classes sont parfaitement pr√©dites (aucune erreur).\n",
    "  - Cela confirme l'ajustement parfait du mod√®le sur les donn√©es d'entra√Ænement.\n",
    "- **Jeu de test** :\n",
    "  - **Classe 1** : 67 bien class√©s, 11 confondus avec la classe 2.\n",
    "  - **Classe 2** : 72 bien class√©s, 7 confondus avec la classe 1.\n",
    "  - **Classe 3** : 38 bien class√©s, aucune confusion.\n",
    "  - Les erreurs concernent principalement la confusion entre les classes 1 et 2, la classe 3 √©tant parfaitement identifi√©e.\n",
    "\n",
    "#### 3. Logloss\n",
    "- **Log loss (test)** : **0.1989**\n",
    "- **Log loss (entra√Ænement)** : **0.0681**\n",
    "\n",
    "L'√©cart entre le log loss du train (tr√®s faible) et celui du test (plus √©lev√©) confirme le sur-apprentissage du mod√®le sur les donn√©es d'entra√Ænement. Toutefois, la valeur relativement basse du log loss sur le test indique que le mod√®le attribue des probabilit√©s assez fiables aux bonnes classes, ce qui est un atout suppl√©mentaire par rapport √† l'arbre de d√©cision simple. Le log loss permet ainsi d'√©valuer non seulement la justesse des pr√©dictions, mais aussi la qualit√© de la calibration des probabilit√©s fournies par la Random Forest.\n",
    "#### 4. Interpr√©tation\n",
    "- Le mod√®le Random Forest offre une excellente capacit√© de classification sur le jeu de test, avec une pr√©cision sup√©rieure √† 90%.\n",
    "- La classe 3 est parfaitement pr√©dite, ce qui montre la robustesse du mod√®le pour cette cat√©gorie.\n",
    "- Les confusions entre les classes 1 et 2 sont r√©duites par rapport √† l'arbre de d√©cision simple, mais restent pr√©sentes.\n",
    "- Le sur-apprentissage est visible sur le jeu d'entra√Ænement, mais l'utilisation de l'**OOB score** et l'√©valuation sur le jeu de test permettent de valider la bonne g√©n√©ralisation du mod√®le.\n",
    "\n",
    "#### 5. Conclusion\n",
    "- **Random Forest** am√©liore la performance globale par rapport √† un arbre unique, notamment sur la capacit√© de g√©n√©ralisation.\n",
    "- Il reste important de surveiller le sur-apprentissage, mais la robustesse du mod√®le sur le jeu de test confirme son efficacit√© pour ce probl√®me de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : Le mod√®le de base Random Forest de `scikit-learn` est construit avec 100 arbres, avec les param√®tres `min_samples_split = 2` (nombre minimum d'√©lements pour consid√©rer une d√©cision) et `min_samples_leaf = 1` (nombre minimum d'√©lement dans une feuille). Ces param√®tres sont les valeurs par d√©faut de `scikit-learn`, mais nous allons les optimiser par la suite. \n",
    "\n",
    "Le mod√®le est construit avec un √©chantillonnage bootstrap, ce qui signifie que chaque arbre est construit sur un sous-ensemble al√©atoire des donn√©es d'entra√Ænement. Cela nous permet d'extraire l'erreur OOB.\n",
    "\n",
    "Contrairement √† R ou le param√®tre √† optimiser est `mtry` (nombre de variables consid√©r√©es √† chaque split), `scikit-learn` nous permet d'optimiser plusieurs hyperparam√®tres essentiels :\n",
    "- **`max_depth`** : la profondeur maximale de chaque arbre (plus un arbre est profond, plus il peut mod√©liser des interactions complexes, mais aussi surapprendre). \n",
    "- **`min_samples_split`** : le nombre minimum d'√©chantillons requis pour diviser un noeud. Plus il est grand, plus l‚Äôarbre est contraint et moins il risque de surapprendre.\n",
    "- **`min_samples_leaf`** : le nombre minimum d'√©chantillons n√©cessaires dans une feuille terminale. Cela permet d‚Äô√©viter des feuilles trop petites, ce qui am√©liore la robustesse.\n",
    "- **`max_features`** : le nombre maximal de variables consid√©r√©es pour chercher le meilleur split √† chaque division (√©quivalent au `mtry` de R). Peut √™tre fix√© √† un nombre entier, √† une proportion de la taille du sample (`float` entre 0 et 1), ou aux valeurs pr√©d√©finies `'sqrt'` : $\\sqrt{n_\\text{variables}}$ ou `'log2'` : $\\log_2(n_\\text{variables})$.\n",
    "- **`max_leaf_nodes`** : limite le nombre total de feuilles de l‚Äôarbre, for√ßant une structure plus simple.\n",
    "- **`ccp_alpha`** : le param√®tre de co√ªt-complexit√© pour l'√©lagage (post-pruning) ; plus `ccp_alpha` est grand, plus l'√©lagage sera fort.\n",
    "\n",
    "Enfin, il nous est √©galement permis de choisir le **crit√®re d‚Äô√©valuation** de la qualit√© du split (`criterion`). \n",
    "Ici, nous avons l'occasion de comparer l'impact du choix du crit√®re (`gini` vs `log_loss`) sur la construction des arbres.  \n",
    "\n",
    "Nous observerons notamment l'effet sur la performance de g√©n√©ralisation (via le score OOB) ainsi que sur le temps d'apprentissage et d'√©lagage.\n",
    "\n",
    "Par ailleurs, ces hyperparam√®tres **sont interd√©pendants** : en pratique, optimiser l'hyperparam√®tre `max_leaf_nodes` peut r√©duire la n√©cessit√© d'√©laguer l'arbre, ou la n√©c√©ssit√© de d√©finir `max_depth`. \n",
    "\n",
    "Nous avons d√©cid√© de construire un mod√®le de for√™t al√©atoire avec les param√®tres par d√©faut et optimiser les hyperparam√®tres `n_estimators` et `max_features` ainsi que le param√®tre `ccp_alpha` pour l'√©lagage, que nous avons vu en cours, mais que nous n'avons pas appliqu√© dans le mod√®le de R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# D√©finir la grille de param√®tres pour la classification\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': np.linspace(0.1, 1.0, 10),  # proportion du nombre total de variables\n",
    "    'ccp_alpha': [0.01, 0.1, 1.0, 5.0, 10.0],\n",
    "    'criterion': ['gini', 'log_loss'],  # crit√®res pour la classification\n",
    "    'oob_score': [True],\n",
    "}\n",
    "\n",
    "# G√©n√©rer toutes les combinaisons possibles\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# Fonction pour entra√Æner et √©valuer (classification)\n",
    "def train_and_evaluate(params):\n",
    "    model = RandomForestClassifier(random_state=randomseed, **params)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_rf, y_train_exp_level)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return {\n",
    "        'n_estimators': params['n_estimators'],\n",
    "        'max_features': params['max_features'],\n",
    "        'ccp_alpha': params['ccp_alpha'],\n",
    "        'criterion': params['criterion'],\n",
    "        'oob_score': model.oob_score_,\n",
    "        'training_time_sec': elapsed_time,\n",
    "    }\n",
    "\n",
    "# Parall√©liser\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_and_evaluate)(params) for params in param_combinations\n",
    ")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Trier par oob_score d√©croissant\n",
    "results_df = results_df.sort_values(by='oob_score', ascending=False)\n",
    "\n",
    "# Afficher\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parmi les meilleures combinaisons, afficher les 10 plus longues et les 10 plus rapides √† entra√Æner\n",
    "best_results_df = results_df[results_df['oob_score'] > 0.85].sort_values(by='training_time_sec', ascending=False).copy()\n",
    "\n",
    "display(best_results_df.head(10))   # 10 plus longues √† fitter\n",
    "display(best_results_df.tail(10))   # 10 plus courtes √† fitter\n",
    "\n",
    "# Logloss sur best_rf_clf\n",
    "# Entra√Æner le meilleur mod√®le de for√™t al√©atoire pour la classification\n",
    "\n",
    "best_rf_clf = RandomForestClassifier(\n",
    "    random_state=randomseed,\n",
    "    n_estimators=500,\n",
    "    max_features=0.2,\n",
    "    ccp_alpha=0.01,\n",
    "    criterion='gini',\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "best_rf_clf.fit(X_train_rf, y_train_exp_level)\n",
    "y_pred_best_rf_clf = best_rf_clf.predict(X_test_rf)\n",
    "y_pred_best_rf_clf_proba = best_rf_clf.predict_proba(X_test_rf)\n",
    "logloss_best_rf_clf_test = log_loss(y_test_exp_level, y_pred_best_rf_clf_proba, normalize=True)\n",
    "print(\"Log loss Random Forest (test):\", round(logloss_best_rf_clf_test, 4))\n",
    "print(\"Log loss Random Forest (train):\", round(log_loss(y_train_exp_level, best_rf_clf.predict_proba(X_train_rf)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpr√©tation du log loss pour la Random Forest\n",
    "\n",
    "- **Log loss (test) : 0.2186**\n",
    "- **Log loss (train) : 0.2368**\n",
    "\n",
    "- Ici, le log loss est tr√®s proche entre le jeu d'entra√Ænement et le jeu de test, ce qui indique que la Random Forest ne sur-apprend pas et g√©n√©ralise bien.\n",
    "- Une valeur de log loss autour de 0.22 est consid√©r√©e comme tr√®s bonne pour un probl√®me de classification √† 3 classes, surtout avec une accuracy sup√©rieure √† 90%.\n",
    "- Cela signifie que le mod√®le ne se contente pas de pr√©dire la bonne classe, mais qu'il est √©galement bien calibr√© dans ses probabilit√©s.\n",
    "\n",
    "En r√©sum√©, la Random Forest fournit √† la fois des pr√©dictions pr√©cises et bien calibr√©es sur ce jeu de donn√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Interpr√©tation des r√©sultats de la for√™t al√©atoire (classification)**\n",
    "\n",
    "Nous avons analys√© la performance de la for√™t al√©atoire selon plusieurs hyperparam√®tres (`n_estimators`, `max_features`, `ccp_alpha`, `criterion`), en nous concentrant sur l‚Äô**OOB score** (ici, l‚Äôaccuracy OOB).\n",
    "\n",
    "$\\rightarrow$ **Performances maximales (mod√®les les plus longs √† entra√Æner)**\n",
    "\n",
    "Les 10 mod√®les les plus longs √† entra√Æner utilisent tous `n_estimators = 500`, ce qui est coh√©rent : plus il y a d‚Äôarbres, plus le temps de calcul augmente.  \n",
    "On observe que¬†:\n",
    "- Les meilleurs OOB scores atteignent **0.8933** (soit ~89,3% de bonne classification sur les donn√©es OOB).\n",
    "- Ces scores sont obtenus avec diff√©rentes valeurs de `max_features` (0.6 √† 1.0) et de `ccp_alpha` (0.01 ou 0.10), et avec diff√©rents crit√®res (`gini`, `entropy`, `log_loss`).\n",
    "- Le crit√®re de split (`criterion`) n‚Äôa pas d‚Äôimpact significatif sur la performance, confirmant l‚Äôobservation g√©n√©rale que ce choix influence peu la qualit√© globale du mod√®le.\n",
    "- L‚Äô√©lagage (`ccp_alpha`) a un effet n√©gligeable sur l‚ÄôOOB score, la performance restant stable quelle que soit la valeur choisie.\n",
    "\n",
    "$\\rightarrow$ **Performances maximales (mod√®les les plus rapides √† entra√Æner)**\n",
    "\n",
    "Les 10 mod√®les les plus rapides utilisent `n_estimators = 100` et des valeurs de `max_features` comprises entre 0.1 et 0.2.  \n",
    "On note que¬†:\n",
    "- Les meilleurs OOB scores atteignent **0.8946**, soit un niveau √©quivalent aux mod√®les les plus longs √† entra√Æner.\n",
    "- Le temps d‚Äôentra√Ænement est tr√®s court (~0.35 secondes), soit pr√®s de 15 fois plus rapide que les mod√®les √† 500 arbres.\n",
    "- Les crit√®res de split (`gini`, `entropy`, `log_loss`) et l‚Äô√©lagage (`ccp_alpha`) n‚Äôinfluencent pas significativement la performance.\n",
    "\n",
    "$\\rightarrow$ **Synth√®se**\n",
    "\n",
    "- **Le nombre d‚Äôarbres (`n_estimators`)**¬†: augmenter le nombre d‚Äôarbres n‚Äôapporte pas de gain significatif en OOB score, mais augmente fortement le temps de calcul.  \n",
    "- **La proportion de variables (`max_features`)**¬†: des valeurs interm√©diaires √† √©lev√©es (0.2 √† 1.0) donnent les meilleurs r√©sultats, mais il n‚Äôy a pas de gain net √† utiliser toutes les variables.\n",
    "- **L‚Äô√©lagage (`ccp_alpha`)**¬†: a peu d‚Äôeffet sur la performance, la for√™t √©tant naturellement robuste au surapprentissage.\n",
    "- **Le crit√®re de split**¬†: le choix entre `gini`, `entropy` ou `log_loss` n‚Äôa pas d‚Äôimpact majeur sur l‚ÄôOOB score.\n",
    "\n",
    "**Conclusion**¬†:\n",
    "Dans l'ensemble, nous constatons que :\n",
    "- **Un `max_features` √©lev√©** permet d'am√©liorer significativement la performance du mod√®le.\n",
    "- **Le param√®tre `ccp_alpha` (√©lagage) impacte tr√®s peu la qualit√© de la for√™t**.\n",
    "- **R√©duire `n_estimators`** permet **d‚Äôacc√©l√©rer consid√©rablement** l'entra√Ænement sans perte substantielle de performance.\n",
    "- **La for√™t al√©atoire reste robuste** face au surapprentissage, m√™me avec des arbres profonds et peu √©lagu√©s.\n",
    "\n",
    "  \n",
    "Apr√®s avoir valid√© ces r√©sultats, nous allons d√©sormais nous int√©resser √† **l‚Äôimportance des variables**, afin d‚Äôidentifier les facteurs les plus influents dans la pr√©diction du niveau des sportifs, comme nous l'avions fait sous R.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_rf_clf.fit(X_train_rf, y_train_exp_level)\n",
    "\n",
    "# Extraire l'importance des variables\n",
    "importances = best_rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train_rf.columns[indices]\n",
    "importances = importances[indices]\n",
    "importances_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importances_df['Cumulative Importance'] = importances_df['Importance'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_df,\n",
    "    palette='cool'\n",
    ")\n",
    "plt.title(\"Importance des variables selon la for√™t al√©atoire\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** : √Ä partir du mod√®le de for√™t al√©atoire optimal entra√Æn√© sous scikit-learn, nous avons extrait l'importance des variables bas√©e sur la r√©duction de l'impuret√© cumul√©e (Gini importance).\n",
    "\n",
    "Le pr√©dicteur `Session_Duration (hours)` domine tr√®s nettement, expliquant √† lui seul la plus grande part de la variance du mod√®le. Cela est coh√©rent, car une dur√©e de s√©ance plus longue est naturellement associ√©e √† un niveau d'exp√©rience plus √©lev√© chez les sportifs.\n",
    "\n",
    "Il est suivi par `SFat_Percentage` et les modalit√©s de `Workout_Frequency (days/week)`, qui contribuent √©galement de fa√ßon significative √† la pr√©diction du niveau d'exp√©rience. Ces variables traduisent l‚Äôintensit√© et la r√©gularit√© de la pratique sportive, des facteurs logiquement li√©s √† l‚Äôexp√©rience.\n",
    "\n",
    "On note aussi l‚Äôimportance de la variable `Calories_Burned`, qui refl√®te l‚Äôeffort fourni, ainsi que de l‚Äôhydratation (`Water_Intake (liters)`), qui peut √™tre un indicateur indirect de l‚Äôintensit√© ou de la dur√©e des s√©ances.\n",
    "\n",
    "Les autres variables (`LWeight`, `Avg_BPM`, `Max_BPM`, `Height (m)`, `Age`, etc.) ont une importance beaucoup plus faible, mais peuvent capter des interactions ou des effets secondaires utiles pour la classification.\n",
    "\n",
    "On observe ainsi que les 5 √† 6 premi√®res variables expliquent √† elles seules la majeure partie de l‚Äôimportance totale du mod√®le, ce qui montre que la pr√©diction du niveau d‚Äôexp√©rience repose principalement sur la dur√©e, la fr√©quence et l‚Äôintensit√© de l‚Äôactivit√© physique.\n",
    "\n",
    "En comparaison, sous R, seules la dur√©e de s√©ance et la fr√©quence ressortaient comme d√©terminantes, alors que scikit-learn attribue une importance plus r√©partie √† plusieurs variables. Cela illustre que la construction des for√™ts al√©atoires peut diff√©rer selon l‚Äôimpl√©mentation et les crit√®res utilis√©s.\n",
    "\n",
    "Nous allons maintenant nous int√©resser √† un autre algorithme d'arbres de d√©cision, le boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Gradient Boosting & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# D√©finir le nombre de folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=randomseed)\n",
    "\n",
    "# Stocker les scores et temps\n",
    "accuracy_scores_gb = []\n",
    "times_gb = []\n",
    "\n",
    "accuracy_scores_xgb = []\n",
    "times_xgb = []\n",
    "\n",
    "# Cr√©er un encodeur de labels pour transformer les classes [1, 2, 3] en [0, 1, 2]\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Ajouter les listes pour stocker l'accuracy sur le jeu de test\n",
    "test_accuracy_gb = []\n",
    "test_accuracy_xgb = []\n",
    "\n",
    "# Boucle sur les folds\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_exp_level):\n",
    "    X_train_fold, X_val_fold = X_train_exp_level.iloc[train_index], X_train_exp_level.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_exp_level.iloc[train_index], y_train_exp_level.iloc[val_index]\n",
    "    \n",
    "    # Encoder les √©tiquettes pour XGBoost\n",
    "    y_train_fold_encoded = le.fit_transform(y_train_fold)\n",
    "    \n",
    "    # Dummifier pour Gradient Boosting ET XGBoost\n",
    "    X_train_fold_dummies = pd.get_dummies(X_train_fold)\n",
    "    X_val_fold_dummies = pd.get_dummies(X_val_fold)\n",
    "    X_test_dummies = pd.get_dummies(X_test_exp_level)\n",
    "    \n",
    "    # Aligner les colonnes\n",
    "    X_train_fold_dummies, X_val_fold_dummies = X_train_fold_dummies.align(X_val_fold_dummies, join='left', axis=1, fill_value=0)\n",
    "    X_train_fold_dummies, X_test_dummies_fold = X_train_fold_dummies.align(X_test_dummies, join='left', axis=1, fill_value=0)\n",
    "    \n",
    "    ## 1. Gradient Boosting\n",
    "    start_time = time.time()\n",
    "    gb_clf = GradientBoostingClassifier(random_state=randomseed)\n",
    "    gb_clf.fit(X_train_fold_dummies, y_train_fold)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_gb = gb_clf.predict(X_val_fold_dummies)\n",
    "    accuracy_scores_gb.append(accuracy_score(y_val_fold, y_pred_gb))\n",
    "    times_gb.append(elapsed_time)\n",
    "    # Accuracy sur le jeu de test\n",
    "    y_pred_gb_test = gb_clf.predict(X_test_dummies_fold)\n",
    "    test_accuracy_gb.append(accuracy_score(y_test_exp_level, y_pred_gb_test))\n",
    "    \n",
    "    ## 2. XGBoost (toujours sur les dummies pour coh√©rence)\n",
    "    start_time = time.time()\n",
    "    xgb_clf = XGBClassifier(random_state=randomseed, enable_categorical=False)\n",
    "    xgb_clf.fit(X_train_fold_dummies, y_train_fold_encoded)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    y_pred_xgb_encoded = xgb_clf.predict(X_val_fold_dummies)\n",
    "    y_pred_xgb = le.inverse_transform(y_pred_xgb_encoded)\n",
    "    # Pr√©diction sur le jeu de test\n",
    "    y_pred_xgb_test_encoded = xgb_clf.predict(X_test_dummies_fold)\n",
    "    y_pred_xgb_test = le.inverse_transform(y_pred_xgb_test_encoded)\n",
    "    accuracy_scores_xgb.append(accuracy_score(y_val_fold, y_pred_xgb))\n",
    "    times_xgb.append(elapsed_time)\n",
    "    test_accuracy_xgb.append(accuracy_score(y_test_exp_level, y_pred_xgb_test))\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"Gradient Boosting:\")\n",
    "print(f\"Accuracy moyenne (CV): {np.mean(accuracy_scores_gb):.4f} ¬± {np.std(accuracy_scores_gb):.4f}\")\n",
    "print(f\"Temps d'entra√Ænement moyen: {np.mean(times_gb):.4f} secondes\")\n",
    "print(f\"Accuracy moyenne sur le jeu de test: {np.mean(test_accuracy_gb):.4f} ¬± {np.std(test_accuracy_gb):.4f}\")\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "print(f\"Accuracy moyenne (CV): {np.mean(accuracy_scores_xgb):.4f} ¬± {np.std(accuracy_scores_xgb):.4f}\")\n",
    "print(f\"Temps d'entra√Ænement moyen: {np.mean(times_xgb):.4f} secondes\")\n",
    "print(f\"Accuracy moyenne sur le jeu de test: {np.mean(test_accuracy_xgb):.4f} ¬± {np.std(test_accuracy_xgb):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Logloss sur Gradient Boosting\n",
    "# Entra√Æner le mod√®le Gradient Boosting sur l'ensemble d'entra√Ænement complet\n",
    "gb_clf = GradientBoostingClassifier(random_state=randomseed)\n",
    "gb_clf.fit(X_train_exp_level_encoded, y_train_exp_level)\n",
    "y_pred_gb = gb_clf.predict(X_test_exp_level_encoded)\n",
    "y_pred_gb_proba = gb_clf.predict_proba(X_test_exp_level_encoded)\n",
    "logloss_gb_test = log_loss(y_test_exp_level, y_pred_gb_proba, normalize=True)\n",
    "print(\"Log loss Gradient Boosting (test):\", round(logloss_gb_test, 4))\n",
    "print(\"Log loss Gradient Boosting (train):\", round(log_loss(y_train_exp_level, gb_clf.predict_proba(X_train_exp_level_encoded)), 4))\n",
    "\n",
    "#Logloss sur XGBoost\n",
    "# Entra√Æner le mod√®le XGBoost sur l'ensemble d'entra√Ænement complet\n",
    "\n",
    "\n",
    "# Encoder les labels pour correspondre √† [0, 1, 2]\n",
    "le = LabelEncoder()\n",
    "y_train_exp_level_enc = le.fit_transform(y_train_exp_level)\n",
    "y_test_exp_level_enc = le.transform(y_test_exp_level)\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=randomseed, enable_categorical=False)\n",
    "xgb_clf.fit(X_train_exp_level_encoded, y_train_exp_level_enc)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_exp_level_encoded)\n",
    "y_pred_xgb_proba = xgb_clf.predict_proba(X_test_exp_level_encoded)\n",
    "logloss_xgb_test = log_loss(y_test_exp_level_enc, y_pred_xgb_proba, normalize=True)\n",
    "print(\"Log loss XGBoost (test):\", round(logloss_xgb_test, 4))\n",
    "print(\"Log loss XGBoost (train):\", round(log_loss(y_train_exp_level_enc, xgb_clf.predict_proba(X_train_exp_level_encoded)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation**\n",
    "\n",
    "Les mod√®les de Gradient Boosting et de XGBoost affichent **d‚Äôexcellentes performances** sans optimisation avanc√©e des hyperparam√®tres.\n",
    "\n",
    "### R√©sultats de la validation crois√©e (5-folds) :\n",
    "- **Gradient Boosting** : accuracy moyenne de **0.8741 ¬± 0.0251**\n",
    "- **XGBoost** : accuracy moyenne de **0.8625 ¬± 0.0287**\n",
    "\n",
    "### Performances sur le jeu de test :\n",
    "- **Gradient Boosting** : accuracy moyenne de **0.9118 ¬± 0.0123**\n",
    "- **XGBoost** : accuracy moyenne de **0.8933 ¬± 0.0060**\n",
    "\n",
    "**Log loss :**\n",
    "\n",
    "- **Gradient Boosting** :  \n",
    "    - Log loss (test) : **0.1756**  \n",
    "    - Log loss (train) : **0.0820**\n",
    "\n",
    "- **XGBoost** :  \n",
    "    - Log loss (test) : **0.2291**  \n",
    "    - Log loss (train) : **0.0078**\n",
    "\n",
    "Un log loss faible sur le test indique que les probabilit√©s pr√©dites sont bien calibr√©es. On observe que Gradient Boosting g√©n√©ralise mieux (√©cart train/test plus faible), tandis que XGBoost sur-apprend davantage (log loss train tr√®s bas, test plus √©lev√©). Les deux mod√®les restent toutefois tr√®s performants.\n",
    "\n",
    "### Temps de calcul :\n",
    "- **Gradient Boosting** : **0.90 seconde** en moyenne par fold\n",
    "- **XGBoost** : **0.92 seconde** en moyenne par fold\n",
    "\n",
    "---\n",
    "\n",
    "On observe que **les deux mod√®les g√©n√©ralisent tr√®s bien**, avec des scores tr√®s proches entre validation crois√©e et test, et sans surapprentissage marqu√©.\n",
    "\n",
    "En termes de rapidit√©, **les deux algorithmes sont tr√®s efficaces**, avec des temps d‚Äôentra√Ænement similaires et tr√®s courts.\n",
    "\n",
    "Compar√© √† la Random Forest optimis√©e, les m√©thodes de boosting offrent ici une **l√©g√®re sup√©riorit√© en g√©n√©ralisation et robustesse**.\n",
    "\n",
    "Ces r√©sultats confirment que **le boosting est particuli√®rement adapt√©** √† la classification du niveau d‚Äôexp√©rience dans ce contexte.\n",
    "\n",
    "Compte tenu de ces performances tr√®s satisfaisantes, notamment pour le Gradient Boosting, il n‚Äôest pas n√©cessaire d‚Äôoptimiser davantage XGBoost pour ce projet. Une recherche d‚Äôhyperparam√®tres pourrait toutefois permettre de gagner encore quelques points de performance si besoin.\n",
    "\n",
    "Nous pouvons donc passer √† l‚Äô**analyse de l‚Äôimportance des variables** pour mieux comprendre les facteurs d√©terminants de la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importance des variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables pour la classification (niveau d'exp√©rience)\n",
    "# Les deux mod√®les utilisent maintenant les m√™mes dummies\n",
    "\n",
    "# Pour Gradient Boosting (avec dummies)\n",
    "importances_gb_df = pd.DataFrame({\n",
    "    'Feature': X_train_fold_dummies.columns,\n",
    "    'Importance': gb_clf.feature_importances_\n",
    "})\n",
    "\n",
    "# Pour XGBoost (avec dummies)\n",
    "importances_xgb_df = pd.DataFrame({\n",
    "    'Feature': X_train_fold_dummies.columns,\n",
    "    'Importance': xgb_clf.feature_importances_\n",
    "})\n",
    "\n",
    "# Trier pour plus de lisibilit√©\n",
    "importances_gb_df = importances_gb_df.sort_values('Importance', ascending=False)\n",
    "importances_xgb_df = importances_xgb_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Tracer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot pour Gradient Boosting\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_gb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Variable Importance - Gradient Boosting (Classification)\")\n",
    "axes[0].set_xlabel(\"Importance\")\n",
    "axes[0].set_ylabel(\"Feature\")\n",
    "\n",
    "# Plot pour XGBoost\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_xgb_df,\n",
    "    palette='cool',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Variable Importance - XGBoost (Classification)\")\n",
    "axes[1].set_xlabel(\"Importance\")\n",
    "axes[1].set_ylabel(\"\")  # Pas besoin de r√©p√©ter \"Feature\" √† droite\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpr√©tation des importances des variables\n",
    "\n",
    "Les graphiques ci-dessus pr√©sentent l‚Äôimportance des variables pour la classification du niveau d‚Äôexp√©rience selon deux mod√®les¬†: **Gradient Boosting** (√† gauche) et **XGBoost** (√† droite).\n",
    "\n",
    "#### Points communs\n",
    "- **Variables dominantes** : Dans les deux mod√®les, la dur√©e des s√©ances (`Session_Duration (hours)`) et la fr√©quence d‚Äôentra√Ænement (`Workout_Frequency (days/week)_2`, `_3`, `_4`) sont les variables les plus importantes. Cela confirme que l‚Äôintensit√© et la r√©gularit√© de la pratique sportive sont des facteurs cl√©s pour pr√©dire le niveau d‚Äôexp√©rience.\n",
    "- **SFat_Percentage** (masse grasse transform√©e) ressort √©galement comme un pr√©dicteur important dans les deux cas.\n",
    "- Les autres variables (calories br√ªl√©es, √¢ge, poids, BPM, hydratation, etc.) ont une importance beaucoup plus faible.\n",
    "\n",
    "#### Diff√©rences entre Gradient Boosting et XGBoost\n",
    "- **Gradient Boosting** accorde une importance maximale √† la dur√©e de s√©ance, suivie de pr√®s par la fr√©quence d‚Äôentra√Ænement et la masse grasse.\n",
    "- **XGBoost** met davantage l‚Äôaccent sur les modalit√©s de fr√©quence d‚Äôentra√Ænement, qui passent devant la dur√©e de s√©ance. Il attribue aussi un peu plus d‚Äôimportance √† certaines modalit√©s de type d‚Äôentra√Ænement (`Workout_Type_Yoga`, `Workout_Type_HIIT`), ce qui n‚Äôest pas le cas pour Gradient Boosting.\n",
    "- Les importances sont plus ¬´¬†r√©parties¬†¬ª dans XGBoost, alors que Gradient Boosting concentre l‚Äôimportance sur un plus petit nombre de variables.\n",
    "\n",
    "#### Explication des diff√©rences\n",
    "- **Nature de l‚Äôalgorithme**¬†: XGBoost utilise des techniques de r√©gularisation et une gestion diff√©rente des splits, ce qui peut conduire √† privil√©gier d‚Äôautres interactions ou modalit√©s.\n",
    "- **Crit√®re d‚Äôimportance**¬†: Les deux mod√®les calculent l‚Äôimportance diff√©remment (r√©duction d‚Äôimpuret√© moyenne pour Gradient Boosting, gain moyen de split pour XGBoost), ce qui peut modifier le classement des variables.\n",
    "- **Stochasticit√© et interactions**¬†: XGBoost explore parfois plus d‚Äôinteractions entre variables, ce qui peut expliquer l‚Äôapparition de modalit√©s secondaires dans son top des importances.\n",
    "- **R√©gularisation**¬†: XGBoost p√©nalise davantage les variables peu informatives, ce qui peut ¬´¬†lisser¬†¬ª la distribution des importances.\n",
    "\n",
    "#### Conclusion\n",
    "Malgr√© ces diff√©rences, les deux mod√®les s‚Äôaccordent sur les facteurs principaux¬†: **dur√©e et fr√©quence des s√©ances** et **masse grasse**. Les divergences sur les variables secondaires sont normales et refl√®tent la sensibilit√© des algorithmes √† la structure des donn√©es et √† leur propre m√©thode d‚Äôoptimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©seaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Encodage des variables cat√©gorielles pour la classification du niveau d'exp√©rience\n",
    "X_train_exp_level_scale_dummy = pd.get_dummies(X_train_exp_level_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "X_test_exp_level_scale_dummy = pd.get_dummies(X_test_exp_level_scale, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "# Aligner les colonnes entre train et test\n",
    "X_train_exp_level_scale_dummy, X_test_exp_level_scale_dummy = X_train_exp_level_scale_dummy.align(\n",
    "    X_test_exp_level_scale_dummy, join='left', axis=1, fill_value=0\n",
    ")\n",
    "\n",
    "# D√©finir le MLP Classifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',\n",
    "                              max_iter=500, random_state=randomseed)\n",
    "\n",
    "# Entra√Æner le mod√®le sur les donn√©es d'entra√Ænement\n",
    "mlp_classifier.fit(X_train_exp_level_scale_dummy, y_train_exp_level)\n",
    "\n",
    "# Pr√©dire sur le jeu de test\n",
    "y_test_pred_mlp = mlp_classifier.predict(X_test_exp_level_scale_dummy)\n",
    "\n",
    "# √âvaluer le mod√®le\n",
    "accuracy_test_mlp = accuracy_score(y_test_exp_level, y_test_pred_mlp)\n",
    "print(\"MLP Classifier - Accuracy sur le jeu de test :\", round(accuracy_test_mlp, 4))\n",
    "\n",
    "#Accuracy sur le jeu d'entra√Ænement\n",
    "accuracy_train_mlp = accuracy_score(y_train_exp_level, mlp_classifier.predict(X_train_exp_level_scale_dummy))\n",
    "print(\"MLP Classifier - Accuracy sur le jeu d'entra√Ænement :\", round(accuracy_train_mlp, 4))\n",
    "\n",
    "# Afficher le rapport de classification et la matrice de confusion\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test_exp_level, y_test_pred_mlp)).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - MLP Classifier (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [\n",
    "        (50,), (100,), (150,),\n",
    "        (100, 50), (150, 100), (150, 100, 50),\n",
    "        (200, 100, 50)\n",
    "    ],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPClassifier(max_iter=1000, early_stopping=True, random_state=randomseed),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Effectuer la recherche sur les donn√©es d'entra√Ænement\n",
    "grid_search.fit(X_train_exp_level_scale_dummy, y_train_exp_level)\n",
    "\n",
    "# Afficher les meilleurs param√®tres et le score correspondant\n",
    "print(\"Meilleurs param√®tres :\", grid_search.best_params_)\n",
    "print(\"Meilleur score accuracy (CV) :\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afficher les param√®tres optimaux\n",
    "print(\"Meilleurs param√®tres du MLP Classifier :\", grid_search.best_params_)\n",
    "\n",
    "#Fit le meilleur mod√®le sur l'ensemble d'entra√Ænement\n",
    "best_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=grid_search.best_params_['hidden_layer_sizes'],\n",
    "    activation=grid_search.best_params_['activation'],\n",
    "    solver=grid_search.best_params_['solver'],\n",
    "    alpha=grid_search.best_params_['alpha'],\n",
    "    learning_rate=grid_search.best_params_['learning_rate'],\n",
    "    max_iter=1000,\n",
    "    random_state=randomseed\n",
    ")\n",
    "best_mlp.fit(X_train_exp_level_scale_dummy, y_train_exp_level)\n",
    "\n",
    "#Afficher la courbe de loss pour le meilleur mod√®le\n",
    "plt.plot(best_mlp.loss_curve_)\n",
    "plt.title(\"Courbe de perte du MLP\")\n",
    "plt.xlabel(\"It√©rations\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Pr√©dire sur le jeu de test\n",
    "y_test_pred_best_mlp = best_mlp.predict(X_test_exp_level_scale_dummy)\n",
    "# √âvaluer le mod√®le\n",
    "accuracy_test_best_mlp = accuracy_score(y_test_exp_level, y_test_pred_best_mlp)\n",
    "print(\"MLP Classifier - Accuracy sur le jeu de test (meilleur mod√®le) :\", round(accuracy_test_best_mlp, 4))\n",
    "# Accuracy sur le jeu d'entra√Ænement\n",
    "accuracy_train_best_mlp = accuracy_score(y_train_exp_level, best_mlp.predict(X_train_exp_level_scale_dummy))\n",
    "print(\"MLP Classifier - Accuracy sur le jeu d'entra√Ænement (meilleur mod√®le) :\", round(accuracy_train_best_mlp, 4))\n",
    "#Afficher la matrice de confusion\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test_exp_level, y_test_pred_best_mlp)).plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion - MLP Classifier (meilleur mod√®le)\")\n",
    "plt.show()\n",
    "\n",
    "#Logloss sur le meilleur MLP\n",
    "y_test_pred_best_mlp_proba = best_mlp.predict_proba(X_test_exp_level_scale_dummy)\n",
    "logloss_best_mlp_test = log_loss(y_test_exp_level, y_test_pred_best_mlp_proba, normalize=True)\n",
    "print(\"Log loss MLP Classifier (test) :\", round(logloss_best_mlp_test, 4))\n",
    "print(\"Log loss MLP Classifier (train) :\", round(log_loss(y_train_exp_level, best_mlp.predict_proba(X_train_exp_level_scale_dummy)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpr√©tation** :  \n",
    "Le meilleur mod√®le de r√©seau de neurones (MLP) a √©t√© entra√Æn√© via **GridSearchCV** en testant diff√©rentes architectures, fonctions d‚Äôactivation et m√©thodes d‚Äôoptimisation. L‚Äôarchitecture optimale s√©lectionn√©e comporte **deux couches cach√©es** avec la fonction d‚Äôactivation **tanh** et l‚Äôoptimiseur **Adam**. Le mod√®le obtient une **accuracy de 0.8564** sur le jeu de test, et **0.9383** sur le jeu d‚Äôentra√Ænement, ce qui indique un certain sur-apprentissage mais une capacit√© de g√©n√©ralisation correcte.\n",
    "\n",
    "Les meilleurs hyperparam√®tres s√©lectionn√©s sont :\n",
    "- Architecture : **(150, 100)** (deux couches cach√©es)\n",
    "- Fonction d‚Äôactivation : **tanh**\n",
    "- M√©thode d‚Äôoptimisation : **Adam**\n",
    "- Apprentissage : **learning rate constant**\n",
    "- R√©gularisation (alpha) : **0.0001**\n",
    "\n",
    "En termes de performance, le r√©seau de neurones optimis√© se situe en-dessous des meilleurs mod√®les d‚Äôensemble comme le **Gradient Boosting** ou la **Random Forest** (accuracy ‚âà 0.91-0.92). Le log loss du MLP Classifier est nettement plus √©lev√© sur le jeu de test (0.3113) que sur le jeu d‚Äôentra√Ænement (0.1382), ce qui traduit un sur-apprentissage : le mod√®le est tr√®s confiant sur les donn√©es d‚Äôentra√Ænement mais ses probabilit√©s sont moins bien calibr√©es sur des donn√©es nouvelles. Le MLP montre une bonne capacit√© √† apprendre, mais reste plus sensible au sur-apprentissage et n√©cessite un temps d‚Äôentra√Ænement plus important pour un gain de performance limit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilan des performances des diff√©rentes techniques\n",
    "\n",
    "## Tableau comparatif synth√©tique des mod√®les\n",
    "\n",
    "| Mod√®le                              | Accuracy Test (%) | Temps d'entra√Ænement (s) | Interpr√©tabilit√©       |\n",
    "|-------------------------------------|:-----------------:|:------------------------:|:----------------------:|\n",
    "| Gradient Boosting                   | 91.2              | ~0.9                     | Mod√©r√©e                |\n",
    "| SVM lin√©aire (d√©faut)               | 91.3              | 169.03                  | Faible                 |\n",
    "| SVM lin√©aire (optimis√©e)            | 91.3              | 92.62                   | Faible                 |\n",
    "| Random Forest                       | 90.8              | ~5                      | Mod√©r√©e                |\n",
    "| Analyse Discriminante Lin√©aire (ADL)| 90.8              | 0.02                    | Haute                  |\n",
    "| Logistic Regression                 | 90.3              | 0.09                    | Haute                  |\n",
    "| Logistic Regression avec Lasso      | 90.3              | 2.67                    | Haute                  |\n",
    "| XGBoost                             | 89.3              | ~0.9                    | Mod√©r√©e                |\n",
    "| R√©seau de neurones (MLP)            | 85.6              | 7.7                     | Faible                 |\n",
    "| Arbre √©lagu√© (CART)                 | 90.2              | Tr√®s rapide             | Haute                  |\n",
    "| Arbre de d√©cision (CART)            | 87.7              | Tr√®s rapide             | Haute                  |\n",
    "| SVM radiale (optimis√©e)             | 74.4              | 9.90                    | Faible                 |\n",
    "| KNN                                 | 72.8              | 20.93                   | Faible                 |\n",
    "| SVM radiale (d√©faut)                | 40.5              | 2.40                    | Faible                 |\n",
    "\n",
    "---\n",
    "\n",
    "### Analyse par m√©thode\n",
    "\n",
    "**Gradient Boosting**\n",
    "- **Avantages** : Meilleure performance globale (accuracy ‚âà 91.2 %), temps d'entra√Ænement rapide.\n",
    "- **Limites** : Interpr√©tabilit√© mod√©r√©e (importance des variables mais pas des interactions pr√©cises).\n",
    "- **Cas d‚Äôusage** : Solution par d√©faut pour maximiser la pr√©cision sans contrainte de temps.\n",
    "\n",
    "**SVM lin√©aire**\n",
    "- **Avantages** : Tr√®s bonne pr√©cision (accuracy ‚âà 91.3 %).\n",
    "- **Limites** : Temps d'entra√Ænement √©lev√© pour la version par d√©faut (169 s) et faible interpr√©tabilit√©.\n",
    "- **Cas d‚Äôusage** : Pr√©cision maximale si le temps de calcul n'est pas une contrainte.\n",
    "\n",
    "**Random Forest**\n",
    "- **Avantages** : Bonne robustesse, interpr√©tabilit√© mod√©r√©e (importance des variables).\n",
    "- **Limites** : Performance l√©g√®rement inf√©rieure au Gradient Boosting, temps d'entra√Ænement plus long.\n",
    "- **Cas d‚Äôusage** : Donn√©es bruyantes ou besoin de stabilit√© sans optimisation fine.\n",
    "\n",
    "**Analyse Discriminante Lin√©aire (ADL)**\n",
    "- **Avantages** : Excellent compromis entre pr√©cision (accuracy ‚âà 90.8 %) et rapidit√© (0.02 s).\n",
    "- **Limites** : Moins performant que les mod√®les d'ensemble.\n",
    "- **Cas d‚Äôusage** : R√©sultats rapides et fiables avec une interpr√©tabilit√© √©lev√©e.\n",
    "\n",
    "**Logistic Regression (avec ou sans Lasso)**\n",
    "- **Avantages** : Interpr√©tabilit√© √©lev√©e, rapidit√© d'entra√Ænement.\n",
    "- **Limites** : Moins performant sur des donn√©es non lin√©aires.\n",
    "- **Cas d‚Äôusage** : Analyses exploratoires ou contraintes de simplicit√©.\n",
    "\n",
    "**XGBoost**\n",
    "- **Avantages** : Rapide et performant (accuracy ‚âà 89.3 %), r√©gularisation int√©gr√©e.\n",
    "- **Limites** : L√©g√®rement moins pr√©cis que le Gradient Boosting.\n",
    "- **Cas d‚Äôusage** : Grands jeux de donn√©es n√©cessitant rapidit√© et parall√©lisation.\n",
    "\n",
    "**R√©seau de neurones (MLP)**\n",
    "- **Avantages** : Capacit√© √† capturer des patterns complexes.\n",
    "- **Limites** : Temps d'entra√Ænement √©lev√© (7.7 s), sur-apprentissage marqu√©, faible interpr√©tabilit√©.\n",
    "- **Cas d‚Äôusage** : Alternative aux mod√®les d'ensemble si l'infrastructure le permet.\n",
    "\n",
    "**Arbres de d√©cision (CART)**\n",
    "- **Avantages** : Interpr√©tabilit√© √©lev√©e, r√®gles claires.\n",
    "- **Limites** : Surapprentissage marqu√©, performance limit√©e.\n",
    "- **Cas d‚Äôusage** : Visualisation p√©dagogique ou analyses simples.\n",
    "\n",
    "**SVM radiale et KNN**\n",
    "- **Avantages** : Flexibilit√© pour des donn√©es complexes.\n",
    "- **Limites** : Faible pr√©cision (accuracy < 75 %), temps d'entra√Ænement √©lev√© pour KNN.\n",
    "- **Cas d‚Äôusage** : Non adapt√©s √† ce jeu de donn√©es.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommandations finales\n",
    "\n",
    "- **Pour la pr√©cision** : Gradient Boosting ou SVM lin√©aire (d√©faut).\n",
    "- **Pour la rapidit√©** : ADL ou Logistic Regression.\n",
    "- **Pour l‚Äôinterpr√©tabilit√©** : ADL ou Logistic Regression avec Lasso.\n",
    "- **Pour des donn√©es complexes** : Gradient Boosting ou Random Forest.\n",
    "\n",
    "### **Conclusion**  \n",
    "Le **Gradient Boosting** et l'**ADL** se d√©marquent comme les meilleurs compromis entre performance et rapidit√©. Les mod√®les lin√©aires restent utiles pour des analyses rapides et interpr√©tables, tandis que les mod√®les d'ensemble (Random Forest, XGBoost) offrent une robustesse accrue pour des donn√©es plus complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
